{
  
    
        "post0": {
            "title": "What's the difference between a metric and a loss?",
            "content": "In machine learning, there are usually two values used to evaluate our model: a metric and a loss. For instance, if we are doing a binary classification task, our metric may be the accuracy and our loss would be the cross-entroy. They both show how good our model performs. However, why do we need both rather than just use one of them? Furthermore, what&#39;s the difference between them? . The short answer is that the metric is for human while the loss is for your model. . Based on the metric, machine learning paritioners such as data scientis and researchers assess a machine learning model. On the assessment, ML paritioners make decisions to address their problems or achieve their business goals. For example , say a data scientist aims to build a spam classifier to distinguish normal email from spam with 95% accuracy. First, the data scientist bulid a logistic regression model with 90% accruay. Apparently, this result doesn&#39;t meet his business goal, so he tries to build another model. After training a random forest model, he get a classifier with 97% accuracy, which goes beyond his goal. Since the goal is met, the data scientist decides to integrate this model into his data product. ML partitioners use the metric to tell whether their model is good enough. . On the other hand, a loss indicates in what direction your model should improve. The difference between machine learning and traditinal programming is how they get the ability to solve a problem. Traditional programs solve problems by following exact instructions given by prgrammers. On the contrary, a machine learning models learn that by taking into some examples (data) and discovery the underlying patterns. How does a machine learning model do that? Most ML models learn from a gradient-based method. Here&#39;s how a gradient-based method (be specifically, a gradient descent method in supervised learning context) works: . A model takes into data and makes predictions. | Compute a loss based on the predictions and the true data. | Compute the gradients of the loss with respect to parameters of the model. | Updating these parameters based on these gradients. | The gradient of the loss helps our model to get better and better. The reason why we need a loss is that it is sensitive enough to small changes so our model can improve. More precisely, the gradient of the loss should vary if our parameters change slighly. In our spam classification example, accuracy is obviously not suitable for being a loss since it only changes when some examples are classified differently. The cross-entrpy is relatively smoother and so it is a good candidate for a loss. However, a metric do not have to be different from a loss. A metric can be a loss as long as it is sensitive enough. For instance, in a regression setting, MSE (mean squared error) can be both a metric and a loss. . In summary, a metric helps ML partitioners to evaluate their models and a loss faciliates the learning process of a ML model. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/machine%20learning/2020/11/29/What's-the-difference-between-a-metric-and-a-loss.html",
            "relUrl": "/learning/machine%20learning/2020/11/29/What's-the-difference-between-a-metric-and-a-loss.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Interactive Visualization Female College Majors with Altair",
            "content": "import pandas as pd women_degrees = pd.read_csv(&#39;percent-bachelors-degrees-women-usa.csv&#39;) women_degrees.iloc[:, :8].head() . Year Agriculture Architecture Art and Performance Biology Business Communications and Journalism Computer Science . 0 1970 | 4.229798 | 11.921005 | 59.7 | 29.088363 | 9.064439 | 35.3 | 13.6 | . 1 1971 | 5.452797 | 12.003106 | 59.9 | 29.394403 | 9.503187 | 35.5 | 13.6 | . 2 1972 | 7.420710 | 13.214594 | 60.4 | 29.810221 | 10.558962 | 36.6 | 14.9 | . 3 1973 | 9.653602 | 14.791613 | 60.2 | 31.147915 | 12.804602 | 38.4 | 16.4 | . 4 1974 | 14.074623 | 17.444688 | 61.9 | 32.996183 | 16.204850 | 40.5 | 18.9 | . women_degrees.columns . Index([&#39;Year&#39;, &#39;Agriculture&#39;, &#39;Architecture&#39;, &#39;Art and Performance&#39;, &#39;Biology&#39;, &#39;Business&#39;, &#39;Communications and Journalism&#39;, &#39;Computer Science&#39;, &#39;Education&#39;, &#39;Engineering&#39;, &#39;English&#39;, &#39;Foreign Languages&#39;, &#39;Health Professions&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Psychology&#39;, &#39;Public Administration&#39;, &#39;Social Sciences and History&#39;], dtype=&#39;object&#39;) . Preprocessing . Tidying the data . tidy_data = women_degrees.melt(id_vars=&quot;Year&quot;, value_vars=women_degrees.columns[1:], var_name=&quot;Discipline&quot;, value_name=&quot;Female proportion&quot;) tidy_data.head(10) . Year Discipline Female proportion . 0 1970 | Agriculture | 4.229798 | . 1 1971 | Agriculture | 5.452797 | . 2 1972 | Agriculture | 7.420710 | . 3 1973 | Agriculture | 9.653602 | . 4 1974 | Agriculture | 14.074623 | . 5 1975 | Agriculture | 18.333162 | . 6 1976 | Agriculture | 22.252760 | . 7 1977 | Agriculture | 24.640177 | . 8 1978 | Agriculture | 27.146192 | . 9 1979 | Agriculture | 29.633365 | . Divide disciplines into groups . stem_cats = [&#39;Psychology&#39;, &#39;Biology&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Computer Science&#39;, &#39;Engineering&#39;] lib_arts_cats = [&#39;Foreign Languages&#39;, &#39;English&#39;, &#39;Communications and Journalism&#39;, &#39;Art and Performance&#39;, &#39;Social Sciences and History&#39;] other_cats = [&#39;Health Professions&#39;, &#39;Public Administration&#39;, &#39;Education&#39;, &#39;Agriculture&#39;, &#39;Business&#39;, &#39;Architecture&#39;] def which_type(subject): if subject in stem_cats: return &quot;STEM&quot; elif subject in lib_arts_cats: return &quot;Liberal Arts&quot; else : return &quot;Others&quot; tidy_data[&quot;Type&quot;] = tidy_data[&quot;Discipline&quot;].apply(which_type) tidy_data.head() . Year Discipline Female proportion Type . 0 1970 | Agriculture | 4.229798 | Others | . 1 1971 | Agriculture | 5.452797 | Others | . 2 1972 | Agriculture | 7.420710 | Others | . 3 1973 | Agriculture | 9.653602 | Others | . 4 1974 | Agriculture | 14.074623 | Others | . Plotting . import altair as alt selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=tidy_data).mark_point(filled=True).encode( x=&quot;Year:N&quot;, y=&quot;median(Female proportion)&quot;, color=&quot;Type&quot;, tooltip=[&quot;Type&quot;, &quot;Year&quot;, &quot;median(Female proportion)&quot;] ).properties( width=700, height=300 ) line = point.mark_line() (point + line).encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . STEM . stem = tidy_data[tidy_data.Type == &quot;STEM&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=stem).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in STEM major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . Liberal Arts . liberal = tidy_data[tidy_data.Type == &quot;Liberal Arts&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=liberal).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in Liberal Arts major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . Others . other = tidy_data[tidy_data.Type == &quot;Others&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=other).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in other majors&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . Summary .",
            "url": "https://peiyihung.github.io/mywebsite/project/2020/09/24/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "relUrl": "/project/2020/09/24/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "A Spam Classifier",
            "content": "Introduction . In this project, I built a spam classifer by implementing machine learning models. Models were trained by the datasets from Apache SpamAssassin website. . Get the data . Download emails and load them into my program . import os import urllib import tarfile import urllib.request download_root = &quot;https://spamassassin.apache.org/old/publiccorpus/&quot; file_names = [&quot;20030228_easy_ham.tar.bz2&quot;, &quot;20030228_easy_ham_2.tar.bz2&quot;, &quot;20030228_hard_ham.tar.bz2&quot;, &quot;20030228_spam.tar.bz2&quot;, &quot;20030228_spam_2.tar.bz2&quot;] store_path = os.path.join(&quot;data&quot;) def fetch_data(root_url=download_root, file_names=file_names, store_path=store_path): # make directory storing emails os.makedirs(store_path, exist_ok=True) # download files for file in file_names: file_url = os.path.join(download_root, file) path = os.path.join(store_path, file) urllib.request.urlretrieve(file_url, path) # extract emails for file in file_names: path = os.path.join(store_path, file) with tarfile.open(path, &#39;r&#39;) as f: f.extractall(path=store_path) #fetch_data() # get file names of emails email_folders = [&quot;hard_ham&quot;, &quot;easy_ham&quot;, &quot;easy_ham_2&quot;, &quot;spam&quot;, &quot;spam_2&quot;] ham_names = {} for ham in email_folders[:3]: ham_path = os.path.join(store_path, ham) names = [name for name in sorted(os.listdir(ham_path)) if len(name) &gt; 20] ham_names[ham] = names spam_names = {} for spam in email_folders[3:]: spam_path = os.path.join(store_path, spam) names = [name for name in sorted(os.listdir(spam_path)) if len(name) &gt; 20] spam_names[spam] = names # parse emails import email import email.policy def load_email(directory, filename, spam_path=store_path): path = os.path.join(spam_path, directory) with open(os.path.join(path, filename), &quot;rb&quot;) as f: return email.parser.BytesParser(policy=email.policy.default).parse(f) hams = [] for ham in email_folders[:3]: emails = [load_email(ham, filename=name) for name in ham_names[ham]] hams.extend(emails) spams = [] for spam in email_folders[3:]: emails = [load_email(spam, filename=name) for name in spam_names[spam]] spams.extend(emails) . . explain how to download the emails and load them in my notebook . len(hams), len(spams), len(spams) / (len(hams) + len(spams)) . (4150, 1897, 0.31370927732760046) . Accuracy of random guess is 70%, so we must do better than that. . Take a look at the emails . headers . hams[1].items() . [(&#39;Return-Path&#39;, &#39;&lt;malcolm-sweeps@mrichi.com&gt;&#39;), (&#39;Delivered-To&#39;, &#39;rod@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16821 invoked by uid 505); 7 May 2002 14:37:01 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 500 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.260914 secs); 07 May 2002 14:37:01 -0000&#39;), (&#39;Delivered-To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16811 invoked by uid 505); 7 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 502 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.250416 secs); 07 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from bocelli.siteprotect.com (64.41.120.21) by h0090272a42db.ne.client2.attbi.com with SMTP; 7 May 2002 14:36:59 -0000&#39;), (&#39;Received&#39;, &#39;from mail.mrichi.com ([208.33.95.187]) tby bocelli.siteprotect.com (8.9.3/8.9.3) with SMTP id JAA14328; tTue, 7 May 2002 09:37:01 -0500&#39;), (&#39;From&#39;, &#39;malcolm-sweeps@mrichi.com&#39;), (&#39;Message-Id&#39;, &#39;&lt;200205071437.JAA14328@bocelli.siteprotect.com&gt;&#39;), (&#39;To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Subject&#39;, &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39;), (&#39;Date&#39;, &#39;Tue, 07 May 2002 09:38:27 -0600&#39;), (&#39;X-Mailer&#39;, &#39;sendEmail-v1.33&#39;)] . hams[1][&quot;Subject&quot;] . &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39; . Contents . print(hams[1].get_content()[:600]) . May 7, 2002 Dear rod-3ds@arsecandle.org: Congratulations! On behalf of Frito-Lay, Inc., we are pleased to advise you that you&#39;ve won Fourth Prize in the 3D&#39;s(R) Malcolm in the Middle(TM) Sweepstakes. Fourth Prize consists of 1 manufacturer&#39;s coupon redeemable at participating retailers for 1 free bag of 3D&#39;s(R) brand snacks (up to 7 oz. size), with an approximate retail value of $2.59 and an expiration date of 12/31/02. Follow these instructions to claim your prize: 1. Print out this email message. 2. Complete ALL of the information requested. Print clearly and legibly. Sign . Get email structure . There are some emails that have multiple parts. . from collections import Counter def get_email_structure(email): if isinstance(email, str): return email payload = email.get_payload() if isinstance(payload, list): return &quot;multipart({})&quot;.format(&quot;, &quot;.join([ get_email_structure(sub_email) for sub_email in payload ])) else: return email.get_content_type() def structure_counter(emails): structures = [get_email_structure(email) for email in emails] return Counter(structures) . structure_counter(hams).most_common() . [(&#39;text/plain&#39;, 3832), (&#39;text/html&#39;, 120), (&#39;multipart(text/plain, application/pgp-signature)&#39;, 101), (&#39;multipart(text/plain, text/html)&#39;, 63), (&#39;multipart(text/plain, text/plain)&#39;, 5), (&#39;multipart(text/plain)&#39;, 3), (&#39;multipart(text/plain, application/x-pkcs7-signature)&#39;, 2), (&#39;multipart(text/html)&#39;, 2), (&#39;multipart(text/plain, application/ms-tnef, text/plain)&#39;, 2), (&#39;multipart(text/plain, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)&#39;, 2), (&#39;multipart(text/plain, image/bmp)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html))&#39;, 1), (&#39;multipart(text/plain, image/png, image/png)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, text/enriched)&#39;, 1), (&#39;multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(text/plain, video/mng)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))&#39;, 1), (&#39;multipart(text/plain, application/x-java-applet)&#39;, 1), (&#39;multipart(text/plain, application/x-patch)&#39;, 1), (&#39;multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, application/ms-tnef)&#39;, 1), (&#39;multipart(text/plain, text/plain, text/plain)&#39;, 1)] . structure_counter(spams).most_common() . [(&#39;text/plain&#39;, 816), (&#39;text/html&#39;, 772), (&#39;multipart(text/plain, text/html)&#39;, 159), (&#39;multipart(text/html)&#39;, 49), (&#39;multipart(text/plain)&#39;, 44), (&#39;multipart(multipart(text/html))&#39;, 23), (&#39;multipart(multipart(text/plain, text/html))&#39;, 5), (&#39;multipart(text/plain, application/octet-stream)&#39;, 3), (&#39;multipart(text/html, text/plain)&#39;, 3), (&#39;multipart(text/plain, image/jpeg)&#39;, 3), (&#39;multipart(text/plain, application/octet-stream, text/plain)&#39;, 3), (&#39;multipart(text/html, application/octet-stream)&#39;, 2), (&#39;multipart/alternative&#39;, 2), (&#39;multipart(text/html, image/jpeg)&#39;, 2), (&#39;multipart(multipart(text/plain), application/octet-stream)&#39;, 2), (&#39;multipart(multipart(text/html), application/octet-stream, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)&#39;, 1), (&#39;text/plain charset=us-ascii&#39;, 1), (&#39;multipart(multipart(text/html), image/gif)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/jpeg)&#39;, 1)] . It seems that most hams are plain text, while spams are more often html. What we need to do next? . Preprocessing emails . write helper funtions and make pipeline . Split emails into train and test set . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split X = np.array(hams+spams) y = np.array([0] * len(hams) + [1] * len(spams)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, stratify=y) X_train.shape, X_test.shape . ((4837,), (1210,)) . Email to text . Parse HTML . from bs4 import BeautifulSoup def html_to_plain_text(html): soup = BeautifulSoup(html, &quot;lxml&quot;) strings = &quot;&quot; for i in soup.find_all(): if i.string: strings += i.string + &quot; n&quot; return strings . Turn email to plain text . def email_to_text(email): html = None for part in email.walk(): ctype = part.get_content_type() if not ctype in (&quot;text/plain&quot;, &quot;text/html&quot;): continue try: content = part.get_content() except: # in case of encoding issues content = str(part.get_payload()) if ctype == &quot;text/plain&quot;: return content else: html = content if html: return html_to_plain_text(html) . example_spam = email_to_text(spams[10]) print(example_spam) . Cellular Phone Accessories All At Below Wholesale Prices! http://202.101.163.34:81/sites/merchant/sales/ Hands Free Ear Buds 1.99! Phone Holsters 1.98! Booster Antennas Only $0.99 Phone Cases 1.98! Car Chargers 1.98! Face Plates As Low As 0.99! Lithium Ion Batteries As Low As 6.94! http://202.101.163.34:81/sites/merchant/sales/ Click Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, SAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below WHOLESALE PRICES! http://202.101.163.34:81/sites/merchant/sales/ ***If You Need Assistance Please Call Us (732) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be removed from future mailings please send your remove request to: removemenow68994@btamail.net.cn Thank You and have a super day :) . Replace url with &quot;URL&quot; . import re url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; example_spam = re.sub(url_pattern, &quot;URL&quot;, example_spam) example_spam . &#39;Cellular Phone Accessories All At Below Wholesale Prices! n nURL n nHands Free Ear Buds 1.99! nPhone Holsters 1.98! nBooster Antennas Only $0.99 nPhone Cases 1.98! nCar Chargers 1.98! nFace Plates As Low As 0.99! nLithium Ion Batteries As Low As 6.94! n nURL n nClick Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, nSAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below nWHOLESALE PRICES! n nURL n n***If You Need Assistance Please Call Us (732) 751-1457*** n n n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nTo be removed from future mailings please send your remove nrequest to: removemenow68994@btamail.net.cn nThank You and have a super day :) n n&#39; . Tokenize . import nltk from nltk.tokenize import word_tokenize nltk.download(&#39;punkt&#39;) example_spam_tokenized = word_tokenize(example_spam) example_spam_tokenized[:10] . [nltk_data] Downloading package punkt to /Users/hongpeiyi/nltk_data... [nltk_data] Package punkt is already up-to-date! . [&#39;Cellular&#39;, &#39;Phone&#39;, &#39;Accessories&#39;, &#39;All&#39;, &#39;At&#39;, &#39;Below&#39;, &#39;Wholesale&#39;, &#39;Prices&#39;, &#39;!&#39;, &#39;URL&#39;] . Stemming . def stemming_email(tokenized_email): stemmer = nltk.PorterStemmer() stemmed_words = [stemmer.stem(word) for word in tokenized_email] return &quot; &quot;.join(stemmed_words) stemmed_eamil = stemming_email(example_spam_tokenized) stemmed_eamil . &#39;cellular phone accessori all At below wholesal price ! url hand free ear bud 1.99 ! phone holster 1.98 ! booster antenna onli $ 0.99 phone case 1.98 ! car charger 1.98 ! face plate As low As 0.99 ! lithium ion batteri As low As 6.94 ! url click below for accessori On all nokia , motorola LG , nextel , samsung , qualcomm , ericsson , audiovox phone At below wholesal price ! url ***if you need assist pleas call Us ( 732 ) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be remov from futur mail pleas send your remov request to : removemenow68994 @ btamail.net.cn thank you and have a super day : )&#39; . Write a sklearn estimator to transform our email . from sklearn.base import BaseEstimator, TransformerMixin class EmailToTokenizedStemmed(BaseEstimator, TransformerMixin): def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True): self.strip_headers = strip_headers self.lower_case = lower_case self.remove_punctuation = remove_punctuation self.replace_urls = replace_urls self.replace_numbers = replace_numbers self.stemming = stemming def fit(self, X, y=None): return self def transform(self, X, y=None): X_transformed = [] for email in X: text = email_to_text(email) or &quot;&quot; if self.lower_case: text = text.lower() if self.replace_urls: url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; text = re.sub(url_pattern, &quot;URL&quot;, text) if self.replace_numbers: text = re.sub(r&#39; d+(?: . d*(?:[eE] d+))?&#39;, &#39;NUMBER&#39;, text) if self.remove_punctuation: text = re.sub(r&#39;[^a-zA-Z0-9]+&#39;, &#39; &#39;, text, flags=re.M) text = word_tokenize(text) text = stemming_email(text) X_transformed.append(text) return np.array(X_transformed) . Vectorizing . from sklearn.feature_extraction.text import TfidfVectorizer . Make Pipeline . from sklearn.pipeline import Pipeline email_pipeline = Pipeline([ (&quot;Tokenizing and Stemming&quot;, EmailToTokenizedStemmed()), (&quot;tf-idf Vectorizing&quot;, TfidfVectorizer()), (&quot;passthrough&quot;, None) ]) . The processed datasets . X_train_processed = email_pipeline.fit_transform(X_train) X_test_processed = email_pipeline.transform(X_test) . . Modeling . from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV from sklearn.metrics import confusion_matrix, classification_report from sklearn.naive_bayes import MultinomialNB from sklearn.linear_model import LogisticRegressionCV from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier # plotting import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # others from scipy.stats import uniform, randint, loguniform import joblib # storing models . Functions from evaluating and comparing models . models = {} # storing trained models models_names = [] # storing models names # add models and its name to dict def add_model(name, model, models_list=models, name_list=models_names): name_list.append(name) models_list[name] = model . def get_classification_report(model, X_test=X_test_processed, y_test=y_test): y_pred = model.predict(X_test) print(classification_report(y_test, y_pred, target_names=[&quot;not spam&quot;, &quot;spam&quot;], digits=4)) . Building models and tuning them . how I trained and tuned the models? what&#39;s the process? . Naive Bayes (baseline model) . nb = MultinomialNB().fit(X_train_processed, y_train) . add_model(&quot;Naive Bayes&quot;, nb) . Logistic regression . logitCV = LogisticRegressionCV(max_iter=1000, Cs=20, cv=10, scoring=&quot;accuracy&quot;) logitCV.fit(X_train_processed, y_train) . LogisticRegressionCV(Cs=20, class_weight=None, cv=10, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=1000, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, refit=True, scoring=&#39;accuracy&#39;, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0) . add_model(&quot;Logistic regression&quot;, logitCV) . SVM . svc = SVC() svc_params = {&#39;C&#39;: loguniform(1e0, 1e3), &#39;gamma&#39;: loguniform(1e-4, 1e-3), &#39;kernel&#39;: [&#39;rbf&#39;], &#39;class_weight&#39;:[&#39;balanced&#39;, None]} svc_grid = RandomizedSearchCV(svc, svc_params, n_jobs=-1, cv=10, n_iter=15, scoring=&quot;accuracy&quot;) svc_grid.fit(X_train_processed, y_train) svc_best = svc_grid.best_estimator_ #svc = joblib.load(&quot;tmp/svc.pkl&quot;) . svc.get_params() . {&#39;C&#39;: 280.3887191550727, &#39;break_ties&#39;: False, &#39;cache_size&#39;: 200, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;coef0&#39;: 0.0, &#39;decision_function_shape&#39;: &#39;ovr&#39;, &#39;degree&#39;: 3, &#39;gamma&#39;: 0.000984422644629166, &#39;kernel&#39;: &#39;rbf&#39;, &#39;max_iter&#39;: -1, &#39;probability&#39;: False, &#39;random_state&#39;: None, &#39;shrinking&#39;: True, &#39;tol&#39;: 0.001, &#39;verbose&#39;: False} . add_model(&quot;SVM&quot;, svc) . Random Forest . max_depths = [10, 50, 100, 150] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1500, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 10, oob accuracy: 0.8594 Max Depth: 50, oob accuracy: 0.9692 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 150, oob accuracy: 0.9694 . max_depths = [90, 100, 110, 120, 130] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 90, oob accuracy: 0.9700 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 110, oob accuracy: 0.9704 Max Depth: 120, oob accuracy: 0.9708 Max Depth: 130, oob accuracy: 0.9698 . rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=100) rf.fit(X_train_processed, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=100, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1, oob_score=True, random_state=44, verbose=0, warm_start=False) . add_model(&quot;Random forest&quot;, rf) . Evaluate on test set . for name in models_names: print(name) get_classification_report(models[name]) print(&quot;--&quot;) print() . Naive Bayes precision recall f1-score support not spam 0.8494 0.9988 0.9181 830 spam 0.9957 0.6132 0.7590 380 accuracy 0.8777 1210 macro avg 0.9226 0.8060 0.8385 1210 weighted avg 0.8953 0.8777 0.8681 1210 -- Logistic regression precision recall f1-score support not spam 0.9927 0.9880 0.9903 830 spam 0.9740 0.9842 0.9791 380 accuracy 0.9868 1210 macro avg 0.9833 0.9861 0.9847 1210 weighted avg 0.9868 0.9868 0.9868 1210 -- SVM precision recall f1-score support not spam 0.9891 0.9880 0.9885 830 spam 0.9738 0.9763 0.9750 380 accuracy 0.9843 1210 macro avg 0.9814 0.9821 0.9818 1210 weighted avg 0.9843 0.9843 0.9843 1210 -- Random forest precision recall f1-score support not spam 0.9776 0.9976 0.9875 830 spam 0.9945 0.9500 0.9717 380 accuracy 0.9826 1210 macro avg 0.9860 0.9738 0.9796 1210 weighted avg 0.9829 0.9826 0.9825 1210 -- . Comparing performance of models using ROC curve and AUC . from sklearn.metrics import roc_curve, roc_auc_score def plot_roc_curve(models_names=models_names, models=models): plt.figure(dpi=120) for name in models_names: if name == &quot;SVM&quot;: y_score = models[name].decision_function(X_test_processed) fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) else: y_score = models[name].predict_proba(X_test_processed)[:,1] fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) plt.plot([0, 1], [0,1], &quot;b--&quot;) plt.xlim(-0.01, 1.02) plt.ylim(-0.01, 1.02) plt.legend(title=&quot;Model (AUC score)&quot;,loc=(1.01, 0.4)) . plot_roc_curve() . Conclusion .",
            "url": "https://peiyihung.github.io/mywebsite/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "relUrl": "/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://peiyihung.github.io/mywebsite/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://peiyihung.github.io/mywebsite/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Who am I ? . My name is Peiyi Hong (洪培翊). . Contect me . Email | Github | Linkedin | .",
          "url": "https://peiyihung.github.io/mywebsite/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Learning",
          "content": "My Learning Notes . What&#39;s the difference between a metric and a loss? . . Nov 29, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/learning/",
          "relUrl": "/learning/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "My Projects . Interactive Visualization Female College Majors with Altair . . Sep 24, 2020 . | Analyzing Sharing Bikes Usage Trends . A exploratory data analysis project answering questions about bike shiring. . Sep 16, 2020 . | A Spam Classifier . This project builds a spam classifier using Apache SpamAssassin&#39;s public datasets. . Sep 12, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://peiyihung.github.io/mywebsite/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}