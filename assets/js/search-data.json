{
  
    
        "post0": {
            "title": "Recognize insincere questions on Quora with BERT",
            "content": "Introduction . This project is originally a Kaggle competition in which Kagglers tried to identify and flag insincere questions on Quora. Quora is a platform where people learn from each other by asking and answering questions. Insincere questions, explained on the description section of the competition, refer to those &quot;founded upon false premises, or intend to make a statement rather than look for helpful answers.&quot; Identifying these kind of questions will make Quora a better place to shiring and gaining knowledge. . The mode I used is BERT (Bidirectional Encoder Representations from Transformers) which was pretrained on a large amount of texts and provides contextual representations of words. BERT boosts the perforamnces of many downstreaming tasks like question answering and sentiemnt analysis. In this project, each sentence would be fed into BERT and then passed into a dense neural net as classifier. I implement this model using Hugging Face&#39;s Transformer library. . Since this dataset is highly imbalanced (only 6% of questions are insincere), the f1 score was considered as evaluation metric. My model reaches 0.71148 f1 score which is as good as 5th place of the private leaderboard. The whole Implementation can be found on this Kaggle notebook. . Import the data . import numpy as np import pandas as pd from transformers import AutoTokenizer, AutoModelForSequenceClassification from fastai.text.all import * from sklearn.model_selection import train_test_split from torch.utils.data import Dataset . path = &quot;/kaggle/input/quora-insincere-questions-classification/&quot; train_df = pd.read_csv(path + &quot;train.csv&quot;) test_df = pd.read_csv(path + &quot;test.csv&quot;) . train_df.shape, test_df.shape . ((1306122, 3), (375806, 2)) . train_df[&quot;target&quot;].value_counts()/train_df.shape[0] . 0 0.93813 1 0.06187 Name: target, dtype: float64 . From the codes above, we can know that . There are 1306122 questions in the training set and 375806 questions for testing. | 94% of questions are normal ones and 6% are insincere ones. | . By these observations, intead of using k-fold cross validation, I decide to split a holdout set as a validation set due to the large size of data. The imbalanced labels suggest that our training process requires some modifications to deal with these situation. . Data preprocessing . Before we dive into modeling, we must transform our data into the format that our model can used. First, we tokenize each word with BERT&#39;s tokenizer. . tokenizer = AutoTokenizer.from_pretrained(&#39;bert-base-cased&#39;) . Next, we define a dataset class so as to form dataloaders. . class QuestionDataset(Dataset): def __init__(self, X, y, tokenizer): self.text = X.reset_index(drop=True) self.targets = y.reset_index(drop=True) self.tok = tokenizer def __len__(self): return len(self.text) def __getitem__(self, idx): text = self.text[idx] targ = self.targets[idx] return self.tok(text, padding=&#39;max_length&#39;, truncation=True, max_length=30, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;][0], tensor(targ) . Finally, I split the dataset in a stratified way to deal with imbalanced labels and convert our datasets into two dataloaders, one for training and the other for validation. . df = train_df X_train, X_valid, y_train, y_valid = train_test_split(df[&quot;question_text&quot;], df[&quot;target&quot;], stratify=df[&quot;target&quot;], test_size=0.01) train_ds = QuestionDataset(X_train, y_train, tokenizer) valid_ds = QuestionDataset(X_valid, y_valid, tokenizer) train_dl = DataLoader(train_ds, bs=256) valid_dl = DataLoader(valid_ds, bs=512) dls = DataLoaders(train_dl, valid_dl).to(&quot;cuda&quot;) . Model . Now that the data is prepared, we can build our model. . bert = AutoModelForSequenceClassification.from_pretrained(&#39;bert-base-cased&#39;).train() classifier = nn.Sequential( nn.Linear(768, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 2) ) bert.classifier = classifier class BertClassifier(Module): def __init__(self, bert): self.bert = bert def forward(self, x): x = self.bert(x) return x.logits model = BertClassifier(bert).to(&quot;cuda&quot;) . I first download the BERT using Transformers and make a new classifier to do the classification work. Since the only output I want is the &quot;logit&quot;, I make a Module class that only return the &quot;logit&quot; part and then initialize it. . Important: Unlike training a pretrained CNN, pretrained transformer do not need to be freezed first. . Because our labels are imbalanced, let&#39;s put some weights loss function to emphasize the importance of these insincere question. Accuracy and f1 score are our evaluation metrics. All floats are converted to 16-bit floats by to_fp16 for memory saving. . n_0 = (train_df[&quot;target&quot;] == 0).sum() n_1 = (train_df[&quot;target&quot;] == 1).sum() n = n_0 + n_1 class_weights = tensor([n / (n+n_0), n / (n+n_1)]).to(&#39;cuda&#39;) learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(weight=class_weights), metrics=[accuracy, F1Score()]).to_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.00014454397605732084, lr_steep=9.120108734350652e-05) . learn.fit_one_cycle(2, lr_max=5e-5) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.126460 | 0.127293 | 0.953300 | 0.688776 | 1:06:13 | . 1 | 0.084351 | 0.122348 | 0.962027 | 0.715270 | 1:06:21 | . After 2 hours of training, the model yeilds 96.2% accuracy and 0.715 f1 score on validation set. . Find the optimal threshold for f1 score . The threshold to make prediction may influence the f1 score, so I try to find a threshold that can maximize our f1 score using the validation set. . from sklearn.metrics import f1_score . preds, targs = learn.get_preds() . thresholds = np.linspace(0.3, 0.7, 50) for threshold in thresholds: f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]&gt;threshold) print(f&quot;threshold:{threshold:.4f} - f1:{f1:.4f}&quot;) . threshold:0.3000 - f1:0.7047 threshold:0.3082 - f1:0.7056 threshold:0.3163 - f1:0.7050 threshold:0.3245 - f1:0.7055 threshold:0.3327 - f1:0.7070 threshold:0.3408 - f1:0.7068 threshold:0.3490 - f1:0.7084 threshold:0.3571 - f1:0.7107 threshold:0.3653 - f1:0.7105 threshold:0.3735 - f1:0.7093 threshold:0.3816 - f1:0.7108 threshold:0.3898 - f1:0.7107 threshold:0.3980 - f1:0.7116 threshold:0.4061 - f1:0.7119 threshold:0.4143 - f1:0.7129 threshold:0.4224 - f1:0.7134 threshold:0.4306 - f1:0.7125 threshold:0.4388 - f1:0.7124 threshold:0.4469 - f1:0.7114 threshold:0.4551 - f1:0.7127 threshold:0.4633 - f1:0.7126 threshold:0.4714 - f1:0.7139 threshold:0.4796 - f1:0.7164 threshold:0.4878 - f1:0.7151 threshold:0.4959 - f1:0.7160 threshold:0.5041 - f1:0.7145 threshold:0.5122 - f1:0.7125 threshold:0.5204 - f1:0.7111 threshold:0.5286 - f1:0.7112 threshold:0.5367 - f1:0.7138 threshold:0.5449 - f1:0.7139 threshold:0.5531 - f1:0.7137 threshold:0.5612 - f1:0.7159 threshold:0.5694 - f1:0.7172 threshold:0.5776 - f1:0.7158 threshold:0.5857 - f1:0.7128 threshold:0.5939 - f1:0.7151 threshold:0.6020 - f1:0.7158 threshold:0.6102 - f1:0.7158 threshold:0.6184 - f1:0.7148 threshold:0.6265 - f1:0.7115 threshold:0.6347 - f1:0.7105 threshold:0.6429 - f1:0.7086 threshold:0.6510 - f1:0.7075 threshold:0.6592 - f1:0.7082 threshold:0.6673 - f1:0.7104 threshold:0.6755 - f1:0.7094 threshold:0.6837 - f1:0.7093 threshold:0.6918 - f1:0.7078 threshold:0.7000 - f1:0.7072 . It seems that a threshold of 0.4796 can yield the highest f1 score. As a result, I would this threhsold while making predictions on test set. . Make prediction on test set . The following codes show how I make prediction and submit to the Kaggle competition. This process follow the same step to convert data and use the optimal threshold we find to make predictions. . test_tensor = tokenizer(list(test_df[&quot;question_text&quot;]), padding=&quot;max_length&quot;, truncation=True, max_length=30, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;] . class TestDS: def __init__(self, tensors): self.tensors = tensors def __len__(self): return len(self.tensors) def __getitem__(self, idx): t = self.tensors[idx] return t, tensor(0) test_dl = DataLoader(TestDS(test_tensor), bs=128) . test_preds = learn.get_preds(dl=test_dl) . prediction = (F.softmax(test_preds[0], dim=1)[:, 1]&gt;0.48).int() sub = pd.read_csv(path + &quot;sample_submission.csv&quot;) sub[&quot;prediction&quot;] = prediction sub.to_csv(&quot;submission.csv&quot;, index=False) . The f1 scores on the test set are: . . This result is better than fifth place of the competition: . .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/2021/05/22/training-a-model-using-pretrained-bert.html",
            "relUrl": "/category/project/2021/05/22/training-a-model-using-pretrained-bert.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Identifying dog breeds using multiple CNNs ",
            "content": "Introduction . In this project I bulid a model for identifying dog breed from a image. This task is originally a Kaggle competition held three years age. The competition provided 10222 dog images for training and 10357 for evaluating model performance. There are 120 various dog breed to classify in the dataset. I tackle this task by concatenating outputs of convolutional layers of two CNNs that were both pretrained on the ImageNet dataset. This model yields 92.5% accuracy with only 5 epochs. The complete work can be found on this Kaggle notebook. . Prepare the data for modeling . Let&#39;s first import the data: . from fastai.vision.all import * labels = pd.read_csv(&quot;../input/dog-breed-identification/labels.csv&quot;) labels . id breed . 0 000bec180eb18c7604dcecc8fe0dba07 | boston_bull | . 1 001513dfcb2ffafc82cccf4d8bbaba97 | dingo | . 2 001cdf01b096e06d78e9e5112d419397 | pekinese | . 3 00214f311d5d2247d5dfe4fe24b2303d | bluetick | . 4 0021f9ceb3235effd7fcde7f7538ed62 | golden_retriever | . ... ... | ... | . 10217 ffd25009d635cfd16e793503ac5edef0 | borzoi | . 10218 ffd3f636f7f379c51ba3648a9ff8254f | dandie_dinmont | . 10219 ffe2ca6c940cddfee68fa3cc6c63213f | airedale | . 10220 ffe5f6d8e2bff356e9482a80a6e29aac | miniature_pinscher | . 10221 fff43b07992508bc822f33d8ffd902ae | chesapeake_bay_retriever | . 10222 rows × 2 columns . Each training image has a unique id and a dog breed label. By simple exploration, you can find that the labels are imbalanced, so I split the dataset into training and valid sets in a stratified manner. . from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) train_ids, valid_ids = next(split.split(labels, labels[&quot;breed&quot;])) labels[&quot;is_valid&quot;] = [i in valid_ids for i in range(len(labels))] labels[&quot;id&quot;] = labels[&quot;id&quot;].apply(lambda x: x + &quot;.jpg&quot;) . In order to feed these images into convolutional neural network, I implement these preprocessing steps: . Reszie all images | Add data augmentation | Normalize the images with statistics from ImageNet dataset | Package these images into dataloaders | . All these steps can be easily done by fastai&#39;s ImageDateLoaders: . path = &quot;../input/dog-breed-identification/train&quot; dls = ImageDataLoaders.from_df(labels, path, item_tfms=Resize(460, method=&quot;squeeze&quot;), batch_tfms=[*aug_transforms(size=300), Normalize.from_stats(*imagenet_stats)], bs=32, valid_col=&quot;is_valid&quot;) dls.show_batch() . Extracting feature using various pretrained CNN . Once the data is ready, we can start to build our model architecture. . Step 1: Download two pretrained CNNs, chop off the classifiers and set them to evalutaion mode by .eval(). . from torchvision.models import inception_v3 inception = inception_v3(pretrained=True, aux_logits=False) inception = nn.Sequential(*list(inception.children())[:-2], nn.Flatten()).eval() . resnet = nn.Sequential(*list(resnet50(pretrained=True).children())[:-1], nn.Flatten()).eval() . I download pretrained InceptionV3 and resnet50 from the internet. In order to use these CNNs as feature extractors, I get rid of the classifiers part and freeze them. . Step 2: Concatenate the output of CNNs as features and Make a new classifier. . class NeuralNet(Module): def __init__(self, extractors, hidden_size, vocab_size, device): self.extractors = extractors for conv in self.extractors: conv.to(device) self.classifier = nn.Sequential( nn.Dropout(0.25), nn.Linear(hidden_size, 512), nn.ReLU(), nn.BatchNorm1d(512), nn.Dropout(0.5), nn.Linear(512, vocab_size) ) def forward(self, x): features = torch.cat([conv(x) for conv in self.extractors], dim=1) return self.classifier(features) . The outputs of convolutional layers would be used as features. Based on these features, a entirely new classifier would be trained. Here&#39;s what the model attributes indicate: . extractors: list of pretrained CNNs | hidden_size: size of concatenated outputs of CNNs | vocab_size: number of distinct dog breeds | device: the device we would like to train our model on, which can be &quot;cpu&quot; or &quot;cuda&quot; | . In the classifier, dropout is used to prevent overfitting and batch normalization for stable training process. . Step 3: Initializing our model . extractors = [inception, resnet] hidden_size = 2048 + 2048 device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; model = NeuralNet(extractors, hidden_size, len(dls.vocab), device) . Why do I use this architecture? . I have tried a single and deeper CNN (resnet152) to address this problem, but it took a longer time to train and did not reach the same level of accuracy. After several experiments on using a single model, I start to find what others did on Kaggle&#39;s Discussions section. Many people used similar approaches as these to get good preformance, so I decide to use this architecture. . Train our model . Here&#39;s how I trained this model: . use cross entropy as loss function and accuracy to evaluate the model performance | use Adam as optimization method | fit_one_cycle: learning rate starts at a small value and first increase and then decrease. | . learn = Learner(dls, model, metrics=accuracy, path=&quot;.&quot;).to_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.009120108559727669) . learn.fit_one_cycle(5, 1e-3, wd=0.3) . epoch train_loss valid_loss accuracy time . 0 | 0.883909 | 0.433452 | 0.892421 | 02:59 | . 1 | 0.466446 | 0.338898 | 0.901711 | 03:00 | . 2 | 0.370352 | 0.289116 | 0.908068 | 03:00 | . 3 | 0.260682 | 0.237924 | 0.925183 | 03:00 | . 4 | 0.196238 | 0.223895 | 0.925183 | 03:00 | . Summary . This model can reach a fairly good accuracy with only 5 epochs (roughly 15 minutes), which is quite efficient. It outperforms a single and deeper model by higher accuracy and less training time. The reason why it can do this might be the variety of features. That is, these pretrained CNNs learn different features from the image, so our classifier can learn better with these abundant features. What I&#39;ve learned from this project is that a &quot;wider&quot; model may do better than a &quot;deeper&quot; model in some cases, so getting your model deeper and deeper is not the only choice to make you model more powerful. .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/2021/05/12/using-multiple-pretrained-cnns-with-fastai.html",
            "relUrl": "/category/project/2021/05/12/using-multiple-pretrained-cnns-with-fastai.html",
            "date": " • May 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Experiments with activation functions and weights initialization",
            "content": "Introduction . Training a deep neural network is hard. One of the reasons that makes it so challenging is vanishing/exploding gradient problem. This problem can be partly resolved by carefully initializing your weights. But, the questions bothering me are: How does these weight initialization techniques affect the learning process? and What would happen if we use inappropriate weights initalization? Therefore, in this article, I run an experiment with different weight initialization approaches for different activation functions and observe how the activations change while passing through a deep neural network. . The experiment is simple. I will make a fully-connnected network with six hidden layers, pass random values into the network, and plot and observe the distributions of activations. In the experiment, two commonly used activation functions are under consideration: tanh and ReLU. For each function, I initialize the weights which are normally distributed with zero mean and different variances. Weights initialized with small variances would cause the acticvations vanishing, while weights with large may cause activations exploding or activation function satuartes. So, to successfully train our neural network, we should use appropriate weight initialization. The proper initialization methods are Xavier initialization for tanh (Golorot and Bengio, 2010) and He initialzation for ReLU (He et al., 2015). By observing the distributions of activations, we can see how these two initialization method help us to trian neural networks. Also, I will briefly show why these two methods works before experimenting with these two methods. . Here&#39;s the code for the experiment: . import numpy as np import matplotlib.pyplot as plt def experiment(weights_multipler, act_func, lim, ticks): &quot;&quot;&quot; Make a neural network with activation function &#39;act_func&#39; and weight initialization &#39;weights_multipler&#39;, and plot the distributions of activations in each layer. &quot;&quot;&quot; # model n_layers = 7 hidden_size = 4096 batch_size = 16 dims = [hidden_size] * n_layers x = np.random.randn(batch_size, dims[0]) activations = [] for Din, Dout in zip(dims[:-1], dims[1:]): W = weights_multipler * np.random.randn(Din, Dout) x = act_func(x.dot(W)) activations.append(x) # plot fig, axes = plt.subplots(ncols=n_layers-1, figsize=(25, 4)) for i, ax in enumerate(axes): acts = activations[i].reshape(-1) ax.hist(acts, bins=80) ax.set_xlim(*lim) ax.set_xticks(ticks) title = f&quot;&quot;&quot; Layer {i+1} mean={acts.mean():.2f}, std={acts.std():.2f} &quot;&quot;&quot; ax.set_title(title) plt.tight_layout() . This article is inpsired by the lecture 10 of the course, Deep Learning for Computer Vision, taught by Justin Johnson. . Tanh . Let&#39;s take a look at tanh. . Formula: $$ text{tanh}(x) = frac{e^x - e^{-x}}{e^x + e^{-x}}$$ . Derivative of Tanh(x): $$ text{tanh}&#39;(x) = 1 - text{tanh}^2(x)$$ . x = np.linspace(-5, 5, 200) y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) derivative = 1 - y**2 fig, axes = plt.subplots(ncols=2, figsize=(15, 4)) axes[0].plot(x, y) axes[0].set_title(&quot;Tanh&quot;) axes[1].plot(x, derivative) axes[1].set_title(&quot;Derivatives of Tanh&quot;); . . Tanh function is a S-shaped function bounded within -1 and 1. Most derivative values are around 0 and become close to zero when it gets larger or smaller. . We first try a small variance. . experiment(0.008, np.tanh, (-1,1), (-1, 0, 1)) . We can see that the distributions of activations shrink as the data pass through the network and most of activations concentrate around zero in the last hidden layer. What would happen if most of our actications are zero? Assuming that we want to compute the gradient of weights in layer $l$, we can decompose the derivative into two parts: $$ frac{ partial{L}}{ partial{w_l}} = frac{ partial{L}}{ partial{z_l}} frac{ partial{z_l}}{ partial{w_l}}$$ where $L$ is the loss function, $z_l$ is the activation of layer $l$, and $w_l$ is the weight of layer $l$. By applying simple derivative rules, we get $ frac{ partial{z_l}}{ partial{w_l}} = z_{l-1}$. If $z_{l-1}$ is zero, then $ frac{ partial{L}}{ partial{w_l}}$ will also be zero. Zero gradient means the weights do not update and our network are not getting better. . Next, we try a big variance. . experiment(0.05, np.tanh, (-1,1), (-1, 0, 1)) . Most of activations are around -1 or 1. Within these areas, the slope of tanh are flat and the gradients are close to zero. This is usually referred to as &quot;saturation&quot;. Since most gradients are zero, our network is not learning. . The right initialization method for tanh is Xavier initialization. . Core idea: variance of input = variance of output . We have . $$y=Wx, quad y_i = displaystyle sum_{j=1}^{Din}x_jw_{ij}$$ . where $y_i$ is the ith element of $y$, $x_j$ is the jth element of $x$, and $w_{ij}$ is the element in ith row, jth col of $W$. . Assuming that $x$ and $W$ are iid and zero-mean, then we got: $$ text{Var}(y_i) = D_{in} times text{Var}(x_jw_{ij}) = D_{in} times text{Var}(x_j) times text{Var}(w_{ij}) $$ . If $ text{Var}(w_{ij}) = frac{1}{D_{in}}$, then $ text{Var}(y_i) = text{Var}(x_j)$. We set $ text{std}(w) = frac{1}{ sqrt{D_{in}}}$ . How do we get $ text{Var}(x_jw_{ij}) = text{Var}(x_j) text{Var}(w_{ij})$? We know that $ text{Var}(x_jw_{ij}) = text{E}(x_j^{2}w_{ij}^2) - text{E}(x_j) text{E}(w_{ij})$. Since $x$ and $w$ are both zero-mean and independent, we have $ text{Var}(x_jw_{ij}) = text{E}(x_j^{2}) text{E}(w_{ij}^2) = text{Var}(x_j) text{Var}(w_{ij})$. . Let&#39;s try this method: . experiment(1/np.sqrt(4096), np.tanh, (-1,1), (-1, 0, 1)) . By using Xavier initialization, we can get stable activation distributions in contrast to previous two cases. . ReLU . The next activation function we are going to discuss is rectified linear units (ReLU). Here&#39;s the formula of ReLU and its derivatives, implementation of ReLU in numpy, and plots of ReLU formula and derivatives. . Formula: $$f(x) = text{max}(0, x)$$ . Derivatives: $$f&#39;(x) = begin{cases} x, ; text{if } x &gt; 0 0, ; text{if } x leq 0 end{cases} $$ . def relu(x): return np.clip(x, 0, None) . x = np.linspace(-5, 5, 200) y = relu(x) derivative = y &gt; 0 fig, axes = plt.subplots(ncols=2, figsize=(15, 4)) axes[0].plot(x, y) axes[0].set_title(&quot;ReLU&quot;) axes[1].plot(x, derivative) axes[1].set_title(&quot;Derivatives of ReLU&quot;); . . As in the tanh section, we first try a normal distribution with a small variance to initialize weights. . experiment(0.01, relu, (-0.1, 1), (0, 1)) . Almost all activations are zero. This is problematic as explained in previous section. We next try a distribution with large variance. . experiment(0.05, relu, (-0.1, 1), (0, 1)) . The activations become larger as the data pass throught the network. Because of the charateristic of ReLU, the gradients will also get larger. An extremely large gradient causes numerical overflow which is not good for optimization. . We should use Kaiming initialization / He initialization for ReLU. . Kaiming initialization follows the same core idea as Xavier initialization but considers the case of ReLU being the activation function. . For a single convolutional layer $l$, we have $$ x_l = f(y_{l-1}) y_l = W_lx_l+b_l $$ where $x_l$ is the input of this layer, $y_l$ is the output before activation, $f$ is ReLU function, and $W_l$ and $b_l$ are weights and bias of this layer. . Following a similar setting with Xavier initialization, we have $$ text{Var}(y_l) = D_{l} times text{Var}(x_lw_{l}) $$ . But now we cannot decompose $ text{Var}(x_lw_{l})$ into $ text{Var}(x_l) text{Var}(w_{l})$ because ReLU is not centered at zero as tanh. Instead, we can only have $ text{Var}(y_l) = D_{l} times text{Var}(w_{l}) text{E}(x_l^2)$. . If we let $w_{l}$ have a zero-mean and symmetric distribution and set $b_l$ to zero, distribution of $y_l$ will also be zero-mean and symmetric. When we pass $y_l$ into ReLU, the negative half of distribution will be eliminate. This lead to $ text{E}(x_l^2) = frac{1}{2} text{Var}(y_{l-1})$. Then we have $ text{Var}(y_l) = D_{l} times text{Var}(w_{l}) times frac{1}{2} text{Var}(y_{l-1})$. . Consider the $L$th layer, variance of $y_L$ would be $$ text{Var}(y_L) = text{Var}(y_{1}) big( prod_{l=2}^{L} frac{1}{2}D_l text{Var}(w_{l}) big) $$ We want to avoid the variance to grow or reduce exponetially, so we can have $$ frac{1}{2}D_l text{Var}(w_{l})=1, forall l ; rightarrow text{std}(w_l) = sqrt{2/D_{l}} $$ . Here&#39;s how it works in our experiment: . experiment(np.sqrt(2/4096), relu, (-0.1, 1), (0, 1)) . As shown in plot above, Kaiming initializtion successfully avoids extreme cases in previous two plots and get stable distributions. . Summary . If we don&#39;t carefully specify initialization methods, we may get gradients that are extremely big or small, in both of which case our network fails to learn properly. We should use Xavier initialization for tanh function and Kaiming initialization for ReLU function. Both these two methods share the same core idea: Input variance should be equal to output variance. This idea helps us to avoid cases of getting extremely big or extremely small activations and gradients. .",
            "url": "https://peiyihung.github.io/mywebsite/category/learning/2021/04/02/Experiments-with-activation-functions-and-weights-initialization.html",
            "relUrl": "/category/learning/2021/04/02/Experiments-with-activation-functions-and-weights-initialization.html",
            "date": " • Apr 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Classifiying Fish Spieces with AlexNet",
            "content": "Introduction . In this project, I build a AlexNet model (Krizhevsky et al., 2012) using pytorh and train it from scratch on fish spieces classification task using fastai. On the other hand, I adopt transfer learning technique. That is, I download an AlexNet pretrained on the ImageNet dataset and fine-tune it. . The data I use in this project is obtained from Kaggle and originates from Ulucan et al. 2020. . Species Classification . Prepare datasets for training . First, I collect the locations of all the fish images and split the whole dataset into training and test set. . from fastai.vision.all import * # all the categories fish_types = [&quot;Gilt-Head Bream&quot;, &quot;Red Sea Bream&quot;, &quot;Sea Bass&quot;, &quot;Red Mullet&quot;, &quot;Hourse Mackerel&quot;, &quot;Black Sea Sprat&quot;, &quot;Striped Red Mullet&quot;, &quot;Trout&quot;, &quot;Shrimp&quot;] # path to folder of each type fish_data_path = [ Path(f&quot;Fish_Dataset/Fish_Dataset/{fish}/{fish}&quot;) for fish in fish_types ] # path to all image files fish_files = [] for fish in fish_data_path: fish_files += get_image_files(fish) # split train and test set train_idx, test_idx = RandomSplitter()(fish_files) . Next, I split the training set again into two datasets and transform them into two dataloaders, training dataloader and validation dataloader. Moreover, I resize all the images to the size of 227$ times$227, adopt data augmentations, and normalize the tensors. . dblocks = DataBlock(blocks=(ImageBlock, CategoryBlock), get_y=parent_label, splitter=RandomSplitter(), item_tfms=Resize(460), batch_tfms=[*aug_transforms(mult=2, size=227, min_scale=0.5), Normalize]) dls = dblocks.dataloaders(fish_files[train_idx]) . Let&#39;s take a look at our dataset: . dls.show_batch() . For test set, I resize the images and normalize the tensor in the same way as the training set, and form the dataset into a dataloader. . test_dl = dls.test_dl(fish_files[test_idx], with_labels=True) . Model . I make an AlexNet as follows: . Alexnet = nn.Sequential( ### conv1 nn.Conv2d(3, 96, kernel_size=11, stride=4), # 55x55 nn.ReLU(), nn.BatchNorm2d(96), nn.MaxPool2d(kernel_size=3, stride=2), # 27x27 ### conv2 nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2), # 27x27 nn.ReLU(), nn.BatchNorm2d(256), nn.MaxPool2d(kernel_size=3, stride=2), # 13x13 ### conv3 nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), # 13x13 nn.ReLU(), ### conv4 nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), # 13x13 nn.ReLU(), ### conv5 nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), # 13x13 nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=0), # 6x6 ### fully connected nn.Flatten(), # 6x6x256=9216 nn.Linear(9216, 4096), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.Dropout(p=0.5), nn.Linear(4096, 10) # 10 classes ) . This network is slightly different from the original AlexNet. I have once tried to train an original AlexNet and got bad performance, so I add batch normalization layers after each ReLU nonlinearity. . Train from scratch . I train the model using Adam, use cross entropy as loss function, and take accuracy as evaluation metric. . learn = Learner(dls, Alexnet, metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=6.918309954926372e-05, lr_steep=0.0002754228771664202) . learn.fit_one_cycle(10, lr_max=1e-4) . epoch train_loss valid_loss accuracy time . 0 | 2.028091 | 1.573765 | 0.449306 | 01:22 | . 1 | 1.417891 | 1.016496 | 0.599306 | 01:22 | . 2 | 0.966973 | 0.540824 | 0.808333 | 01:22 | . 3 | 0.702682 | 0.464403 | 0.822222 | 01:22 | . 4 | 0.492881 | 0.218500 | 0.914583 | 01:22 | . 5 | 0.351371 | 0.123457 | 0.954167 | 01:22 | . 6 | 0.283956 | 0.100527 | 0.966667 | 01:22 | . 7 | 0.209780 | 0.065298 | 0.979167 | 01:22 | . 8 | 0.160508 | 0.054405 | 0.984722 | 01:22 | . 9 | 0.141618 | 0.035071 | 0.993750 | 01:22 | . After 10 epochs of training, we get 99.4% accuracy on validation set. To get a more robust assessment of our model, let&#39;s see how it works on the test set. . Evaluate the model on the test set . preds, targ = learn.get_preds(dl=test_dl) . accuracy(preds, targ) . TensorBase(0.9844) . Our model gets an overall 98.4% accuracy on the test set. We can further observe how our model works on each fish spieces by plotting a confusion matrix. . interpret = ClassificationInterpretation.from_learner(learn, dl=test_dl) interpret.plot_confusion_matrix(figsize=(10, 10), dpi=60) . Red sea bream and gilt-head bream seems a little hard to distinguish for our model because it misclassifies 12 red sea bream as gilt-head bream. Overall, the model gives a good performance. . Transfer Learning . In this section, I download a pretrained AlexNet model from pytorch and train it to tackle this task. I freeze all the convolution layers as feature extractor and add new fully connected network to train for this task. . transfer_learn = cnn_learner(dls, alexnet, metrics=accuracy) transfer_learn.lr_find() . Downloading: &#34;https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth&#34; to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth . . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.004365158267319202) . transfer_learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.681534 | 0.105453 | 0.963889 | 01:20 | . 1 | 0.478349 | 0.080955 | 0.970139 | 01:20 | . 2 | 0.386160 | 0.035312 | 0.987500 | 01:19 | . 3 | 0.291277 | 0.021411 | 0.991667 | 01:20 | . 4 | 0.204846 | 0.020786 | 0.993750 | 01:20 | . It takes only 5 epochs to achieve the same level of accuracy as the model in previous section. Let&#39;s see how it does on test set. . interp_trans = ClassificationInterpretation.from_learner(transfer_learn,dl=test_dl) . accuracy(interp_trans.preds, interp_trans.targs) . TensorBase(0.9950) . interp_trans.plot_confusion_matrix(figsize=(10, 10), dpi=60) . This model gets almost all the images correctly classified. . Summary . This is a relatively easy task. It only takes 10 epochs to get a 98.4% accuray from a model that is trained from scratch. An AlexNet without batch normalization is hard to train. It is essential to add batch normalization to train a deep nural network. By applying transfer learning technique, we can get a 99.5% accuracy after only 5 epochs of traning. .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/2021/03/15/Classifying-Fish-Species-with-AlexNet.html",
            "relUrl": "/category/project/2021/03/15/Classifying-Fish-Species-with-AlexNet.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Building a Language Model for Chinese Numbers using LSTM and GRU",
            "content": "Introduction . In order to better understand what are a language model and recurrent neural networks (RNN), I create a simple dataset on Chinese numbers and train various RNNs to build a language model of Chinese numbers in this project. The dataset contains Traditional Chinese characters of numbers from 0 to 9999 and I train models to predict the next Chinese number in the dataset. Most models are built by common modules in Pytorch instead of off-the-shelf Pytorch RNN moduels. For example, I make LSTM cells using only nn.Linear rather than nn.LSTM. Only the last model are created using Pytorch&#39;s nn.LSTM and, by adding regularization techniques, it yields an accuracy of 95.5%. This project is inspired by the chapter 12 of the fastai book in which the author creates a dataset of English numbers and trains language models. . Generate data . Here&#39;s how I make this simple model of Chinese numbers: . from fastai.text.all import * # generate numbers from 0 to 9 zero_to_nine = [&quot;零&quot;, &quot;一&quot;, &quot;二&quot;, &quot;三&quot;, &quot;四&quot;, &quot;五&quot;, &quot;六&quot;, &quot;七&quot;, &quot;八&quot;, &quot;九&quot;] num = zero_to_nine.copy() # 10 to 19 ten_to_nineteen = [] for i in zero_to_nine: if i == &quot;零&quot;: add = &quot;&quot; else: add = i ten_to_nineteen.append(&quot;十&quot; + add) num += ten_to_nineteen # 20 to 99 ten_to_hundred = [i+j for i in zero_to_nine[2:] for j in ten_to_nineteen] num += ten_to_hundred # 100 to 999 hundred_to_thousand = [] for i in zero_to_nine[1:]: hundred_to_thousand.append(i + &quot;百&quot;) for g in zero_to_nine[1:]: hundred_to_thousand.append(i + &quot;百&quot; + &quot;零&quot; + g) for j in ten_to_nineteen: hundred_to_thousand.append(i + &quot;百&quot; + &quot;一&quot; + j) for k in ten_to_hundred: hundred_to_thousand.append(i + &quot;百&quot; + k) num += hundred_to_thousand # 999 to 9999 thousand_tenthousand = [] for i in zero_to_nine[1:]: thousand_tenthousand.append(i + &quot;千&quot;) for g in zero_to_nine[1:]: thousand_tenthousand.append(i + &quot;千&quot; + &quot;零&quot; + g) for j in ten_to_nineteen: thousand_tenthousand.append(i + &quot;千&quot; + &quot;零&quot;+ &quot;一&quot; + j) for k in ten_to_hundred: thousand_tenthousand.append(i + &quot;千&quot; + &quot;零&quot; + k) for o in hundred_to_thousand: thousand_tenthousand.append(i + &quot;千&quot; + o) num += thousand_tenthousand . Let&#39;s take a look at the dataset: . len(num) . 10000 . num[100:111] . [&#39;一百&#39;, &#39;一百零一&#39;, &#39;一百零二&#39;, &#39;一百零三&#39;, &#39;一百零四&#39;, &#39;一百零五&#39;, &#39;一百零六&#39;, &#39;一百零七&#39;, &#39;一百零八&#39;, &#39;一百零九&#39;, &#39;一百一十&#39;] . The numbers in the dataset are from 0 to 9999, so there are 10000 data points in it. If you want to save this dataset, you can uncomment and execute the following codes: . # with open(&quot;chinese_numbers.txt&quot;, &quot;w&quot;) as text_file: # text_file.write(&quot; n&quot;.join(num)) . Tokenization and Numericalization . Before we delve into modeling, we should first make our dataset understandable by the model. That is, we should tokenize the data and numericalize the tokens. . Tokenization is to transfer each Chinese character into a token. . tokens = L(n for n in &quot;.&quot;.join(num) if n) tokens . (#74691) [&#39;零&#39;,&#39;.&#39;,&#39;一&#39;,&#39;.&#39;,&#39;二&#39;,&#39;.&#39;,&#39;三&#39;,&#39;.&#39;,&#39;四&#39;,&#39;.&#39;...] . vocab is the collection of unique tokens. . vocab = L(*tokens).unique() . Numericalization is to map a token to a non-negative number so as to be taken by the nn.Embedding layer of RNNs. . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#74691) [0,1,2,1,3,1,4,1,5,1...] . Dataloaders . Next, we rearrange the data into batches and form the dataloaders . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds . sl = 16 # sequence length bs = 32 # batch size seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . Let&#39;s see how the data is formed in a batch: . xs, y = dls.one_batch() . L(vocab[i] for i in xs[0]) . (#16) [&#39;零&#39;,&#39;.&#39;,&#39;一&#39;,&#39;.&#39;,&#39;二&#39;,&#39;.&#39;,&#39;三&#39;,&#39;.&#39;,&#39;四&#39;,&#39;.&#39;...] . L(vocab[i] for i in y[0]) . (#16) [&#39;.&#39;,&#39;一&#39;,&#39;.&#39;,&#39;二&#39;,&#39;.&#39;,&#39;三&#39;,&#39;.&#39;,&#39;四&#39;,&#39;.&#39;,&#39;五&#39;...] . For each time step, the model is predicting the next word given previous word, so the independent variable is a sequence of tokens and the target would be the next word. . Models . In this section, I create RNN models for the language model task. Here&#39;s a list of models: . Vanilla RNN | Two-layered RNN | Long Short-Term Memory (LSTM) | Two-layered LSTM | Gated Recurrent Unit (GRU) | LSTM + regularization | . The last one uses Pytorch&#39;s nn.LSTM, while the rest only use nn.Linear and nn.Embedding modules. All the models would be trained using cross entropy as loss function, Adam as optimization method and accuracy as evaluation metric. . Vanilla RNN . The first model is a RNN with only plain linear layers in it. . class VanillaRNN(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] # walking through each time step for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . This model is composed of: . self.i_h: an embedding layer (nn.Embedding) taking into a index of a token and return the embedding vector of a token. | self.h_h: a plain linear layer taking the vector of word and performing linear transformation | self.h_o: the output layer generating the probability of being each word in the vocab | self.h: the hidden state of the RNN | . Let&#39;s train this model: . learn1 = Learner(dls, VanillaRNN(len(vocab), 128), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn1.lr_find() . SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.009120108559727669) . learn1.fit_one_cycle(10, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.479430 | 1.525608 | 0.455280 | 00:01 | . 1 | 1.121994 | 1.544240 | 0.526670 | 00:01 | . 2 | 0.921751 | 1.768980 | 0.579270 | 00:01 | . 3 | 0.737316 | 1.990620 | 0.618400 | 00:01 | . 4 | 0.612017 | 2.001600 | 0.637998 | 00:01 | . 5 | 0.520435 | 2.110346 | 0.662985 | 00:01 | . 6 | 0.432043 | 2.188616 | 0.696862 | 00:01 | . 7 | 0.357842 | 2.515358 | 0.727910 | 00:01 | . 8 | 0.302256 | 2.760709 | 0.741110 | 00:01 | . 9 | 0.252840 | 2.879565 | 0.750606 | 00:01 | . We get 75% accuracy after 10 epochs. . Two-layered RNN . You can stack multiple linear layers in a RNN to make you model more powerful. Here I make a two-layered one. . class TwoLayersRNN(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h1 = nn.Linear(n_hidden, n_hidden) self.h_h2 = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h1 = 0 self.h2 = 0 def forward(self, x): outs = [] for i in range(sl): self.h1 = self.h1 + self.i_h(x[:,i]) self.h1 = F.relu(self.h_h1(self.h1)) self.h2 = self.h2 + self.h1 self.h2 = F.relu(self.h_h2(self.h2)) outs.append(self.h_o(self.h2)) self.h1 = self.h1.detach() self.h2 = self.h2.detach() return torch.stack(outs, dim=1) def reset(self): self.h1 = 0 self.h2 = 0 . This model is also a RNN with plain linear layers but it has two linear layers. You can find the two linear layers in the codes:self.h_h1 and self.h_h2. Let&#39;s see if it can outperform the single-layered one. . learn2 = Learner(dls, TwoLayersRNN(len(vocab), 128), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn2.lr_find() . SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.005248074419796467) . learn2.fit_one_cycle(10, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.432051 | 1.517559 | 0.454337 | 00:01 | . 1 | 1.138805 | 1.898586 | 0.539601 | 00:01 | . 2 | 0.823014 | 2.276638 | 0.555900 | 00:01 | . 3 | 0.629227 | 1.774288 | 0.618871 | 00:01 | . 4 | 0.507961 | 1.674384 | 0.629782 | 00:01 | . 5 | 0.400248 | 1.949680 | 0.659079 | 00:01 | . 6 | 0.352511 | 1.605654 | 0.693427 | 00:01 | . 7 | 0.252898 | 1.634336 | 0.752155 | 00:01 | . 8 | 0.212198 | 1.842869 | 0.764547 | 00:01 | . 9 | 0.177986 | 1.873169 | 0.770205 | 00:02 | . This model get 2% increase in accuracy also with 10 epcohs as the previous one. . The vanilla RNN severely suffers from the vanishing gradient problem. To resolve the problem, several cell architectures are designed including Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU). I would implement these two method with Pytorch in the following sections. . Long Short-Term Memory (LSTM) . Instead of using a linear layer, LSTM use a much more sophisticated cells in a RNN. Here&#39;s the cell in codes: . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni+nh, nh) self.input_gate = nn.Linear(ni+nh, nh) self.outout_gate = nn.Linear(ni+nh, nh) self.cell_gate = nn.Linear(ni+nh, nh) self.n_size = ni+nh def forward(self, input_, state): # h and c from last time step, concatenate them into a single vector h, c = state h = torch.stack([h, input_], dim=1).view(-1, self.n_size) # gates forget = torch.sigmoid(self.forget_gate(h)) inp = torch.sigmoid(self.input_gate(h)) output = torch.sigmoid(self.outout_gate(h)) # update or forget content cell = torch.tanh(self.cell_gate(h)) c = forget * c + inp*cell h = torch.tanh(c) * output return h, (h, c) . In a LSTM cell, two states would be passed through the network: hidden state h and cell state c. Hidden state and the input (input_) at each time step would be concatenated together first. . h = torch.stack([h, input_], dim=1).view(-1, self.n_size) . And then they would be passed into three gates: forget gate, input gate, and output gate. . forget = torch.sigmoid(self.forget_gate(h)) inp = torch.sigmoid(self.input_gate(h)) output = torch.sigmoid(self.outout_gate(h)) . Since these three gates all use sigmoid function as activation function, they would produce three scores ranging from 0 to 1. A new cell state would be computed by passing hidden state into the cell state and then the tanh function. . cell = torch.tanh(self.cell_gate(h)) . The updated cell state would be a weighted average of the previous cell state and the one that&#39;s newly computed. The weights are the output of forget gate and input gate. . c = forget * c + inp * cell . This new cell state would also be passed into the tanh function and times the output gate score becoming the new hidden state. . h = torch.tanh(c) * output . There are two sets of output of the LSTM cell: h is the output of each time step and (h, c) are the states passed into the next time step. . Let&#39;s bulid a LSTM RNN and train it: . class MyLSTM(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = LSTMCell(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(bs, n_hidden) self.c = torch.zeros(bs, n_hidden) def forward(self, x): out = [] state = (self.h, self.c) for i in range(sl): output, state = self.rnn(self.i_h(x[:,i]), state) out.append(self.h_o(output)) self.h = self.h.detach() self.c = self.c.detach() return torch.stack(out, dim=1) def reset(self): self.h = self.h.zero_() self.c = self.c.zero_() . learn3 = Learner(dls, MyLSTM(len(vocab), 128), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn3.lr_find() . SuggestedLRs(lr_min=0.04365158379077912, lr_steep=0.009120108559727669) . learn3.fit_one_cycle(10, lr_max=1e-2) . epoch train_loss valid_loss accuracy time . 0 | 1.564131 | 1.658502 | 0.413254 | 00:06 | . 1 | 1.132675 | 1.635722 | 0.495824 | 00:06 | . 2 | 0.874189 | 1.553444 | 0.536099 | 00:06 | . 3 | 0.682400 | 1.353059 | 0.626549 | 00:06 | . 4 | 0.529528 | 1.235760 | 0.652883 | 00:06 | . 5 | 0.461710 | 1.070935 | 0.693023 | 00:06 | . 6 | 0.435143 | 1.129873 | 0.695043 | 00:06 | . 7 | 0.417539 | 1.090291 | 0.706897 | 00:05 | . 8 | 0.400883 | 1.208046 | 0.708176 | 00:05 | . 9 | 0.392933 | 1.203122 | 0.707099 | 00:06 | . We train it for 10 epochs and this model use only get 70% accuracy. There may be some other problems we have to deal with. . Two-layered LSTM . As in the vanilla RNN, we can also stack LSTM cells. I create a two-layered one here. . class MyLSTM2(Module): def __init__(self, vocab_sz, n_hidden): # networks self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn1 = LSTMCell(n_hidden, n_hidden) self.rnn2 = LSTMCell(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) # hidden states, 2 layers and 2 states each layer self.hs = [torch.zeros(bs, n_hidden) for _ in range(4)] def forward(self, x): out = [] # initial hidden states state1 = [self.hs[i] for i in [0,1]] state2 = [self.hs[i] for i in [2,3]] # go through all networks for i in range(sl): output, state1 = self.rnn1(self.i_h(x[:,i]), state1) output, state2 = self.rnn2(output, state2) out.append(self.h_o(output)) # detach hidden states for i in range(4): self.hs[i] = self.hs[i].detach() return torch.stack(out, dim=1) def reset(self): for i in range(4): self.hs[i] = self.hs[i].zero_() . Since we have two LSTM cells in the RNN, two sets of states are computed (state1 and state2). The output hidden state of the first LSTM cell would be passed into the next LSTM cell and the next LSTM cell would generate the probability of each word. . learn4 = Learner(dls, MyLSTM2(len(vocab), 128), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn4.lr_find() . SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.019054606556892395) . learn4.fit_one_cycle(10, lr_max=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 1.367824 | 1.692747 | 0.461072 | 00:06 | . 1 | 1.101825 | 1.752430 | 0.499663 | 00:06 | . 2 | 0.887319 | 1.522436 | 0.534079 | 00:05 | . 3 | 0.752598 | 1.426126 | 0.591123 | 00:07 | . 4 | 0.645684 | 1.208018 | 0.659011 | 00:05 | . 5 | 0.572723 | 1.224382 | 0.638200 | 00:06 | . 6 | 0.514688 | 1.094028 | 0.664130 | 00:06 | . 7 | 0.484465 | 1.107556 | 0.669114 | 00:05 | . 8 | 0.458131 | 1.096884 | 0.675647 | 00:05 | . 9 | 0.444725 | 1.108794 | 0.673155 | 00:05 | . Gated Recurrent Unit (GRU) . GRU also attempts to address vanishing gradient problem as LSTM but it is less complicated than LSTM. . class GRUcell(Module): def __init__(self, ni, nh): self.u_gate = nn.Linear(ni+nh, nh) self.r_gate = nn.Linear(ni+nh, nh) self.g = nn.Linear(ni+nh, nh) self.n_size = ni+nh def forward(self, x, h): # combine h from last time step and x from this time step h_t = torch.stack([h, x], dim=1).view(-1, self.n_size) # update gate and reset gate u = torch.sigmoid(self.u_gate(h_t)) r = torch.sigmoid(self.r_gate(h_t)) # update or rest content rh = h * r x_rh = torch.stack([x, rh], dim=1).view(-1, self.n_size) g = torch.tanh(self.g(x_rh)) return u*h + (1-u)*g . In contrast to a LSTM cell, a GRU cell only has one hidden state (h) and two gates (self.u_gate and self.r_gate). As in the LSTM cell, the hidden state and the input at current time step would first be stacked together. . h_t = torch.stack([h, x], dim=1).view(-1, self.n_size) . This hidden state would be passed into two gates: update gate and reset gate. . u = torch.sigmoid(self.u_gate(h_t)) r = torch.sigmoid(self.r_gate(h_t)) . Next, we compute a new hidden state. . rh = h * r x_rh = torch.stack([x, rh], dim=1).view(-1, self.n_size) g = torch.tanh(self.g(x_rh)) . The final output is a weighted average of the original hidden state and the new hidden state. . u * h + (1-u) * g . Let&#39;s apply this GRU cell to a RNN and train this RNN: . class MyGRU(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = GRUcell(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(bs, n_hidden) def forward(self, x): out = [] for i in range(sl): self.h = self.rnn(self.i_h(x[:,i]), self.h) out.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(out, dim=1) def reset(self): self.h.zero_() . learn5 = Learner(dls, MyGRU(len(vocab), 128), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn5.lr_find() . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.009120108559727669) . learn5.fit_one_cycle(10, lr_max=5e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.505964 | 1.656996 | 0.391501 | 00:03 | . 1 | 1.137567 | 1.522142 | 0.539534 | 00:03 | . 2 | 0.768740 | 1.856354 | 0.604054 | 00:03 | . 3 | 0.494808 | 1.249206 | 0.647495 | 00:03 | . 4 | 0.296579 | 1.079444 | 0.737204 | 00:03 | . 5 | 0.154340 | 0.889941 | 0.789871 | 00:03 | . 6 | 0.091274 | 0.959103 | 0.833311 | 00:03 | . 7 | 0.057189 | 0.823301 | 0.849811 | 00:03 | . 8 | 0.032112 | 0.825291 | 0.856075 | 00:03 | . 9 | 0.024684 | 0.823506 | 0.856883 | 00:03 | . Surprisingly, this GRU model gets 85% accuray which is much better than the LSTM one. . . Note: LSTM and GRU makes it easier to retain long-term information by adjusting those gates. . LSTM + Regularization techniques . To get a good performance, the last model I train uses Pytorch&#39;s nn.LSTM module and serveral regularization techniques. . class LSTM_with_regularization(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() . learn6 = Learner(dls, LSTM_with_regularization(len(vocab), 128, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) learn6.lr_find() . SuggestedLRs(lr_min=0.10964782238006592, lr_steep=0.007585775572806597) . In this model, three regularization method are adopted to prevent overfitting: . Dropout: randomly setting some weights of a unit to zero, implemented with nn.Dropout, the probability is set to 0.5. | Activation Regularization (AR) and Temporal Activation Regularization (TAR): Similar to weight decay, trying to make the activation (AR) and the difference between consecutive activations (TAR) as small as possible, implemented with the callback RNNRegularizer. | Weight tying: setting the input embedding equal to the output embedding, self.h_o.weight = self.i_h.weight in the code. | . Let&#39;s see how this model performs: . learn6.fit_one_cycle(6, 1e-2, wd=0.7) . epoch train_loss valid_loss accuracy time . 0 | 1.069000 | 0.940814 | 0.658540 | 00:05 | . 1 | 0.348538 | 0.325210 | 0.894868 | 00:04 | . 2 | 0.183276 | 0.245054 | 0.917565 | 00:04 | . 3 | 0.138813 | 0.188584 | 0.956829 | 00:04 | . 4 | 0.114447 | 0.170680 | 0.958648 | 00:04 | . 5 | 0.102175 | 0.182135 | 0.954943 | 00:04 | . This final model gets a great accuracy of 95.5%! .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/rnn/fastai/pytorch/2021/02/21/Chinese_numbers.html",
            "relUrl": "/category/project/rnn/fastai/pytorch/2021/02/21/Chinese_numbers.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Find cat's mouth with fastai",
            "content": "Introduction . . from fastai.vision.all import * . !mkdir ../root/.kaggle/ !mv kaggle.json ../root/.kaggle/ !chmod 600 ../root/.kaggle/kaggle.json . !kaggle datasets download -d crawford/cat-dataset . Downloading cat-dataset.zip to /content 100% 4.03G/4.04G [01:18&lt;00:00, 46.5MB/s] 100% 4.04G/4.04G [01:18&lt;00:00, 55.0MB/s] . !unzip -q cat-dataset.zip . path = Path(&quot;.&quot;) folders = [f&quot;CAT_0{i}&quot; for i in range(7)] . def get_position(x): loc_list = np.genfromtxt(Path(f&quot;{str(x)}.cat&quot;)) positions = [loc_list[[i, i+1]] for i in range(1, 7, 2)] return tensor(positions) . def get_items(path): fnames = get_image_files(path, folders=folders) return fnames . cats = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_items, get_y=get_position, splitter=RandomSplitter(), item_tfms=Resize(300, method=&quot;squish&quot;), batch_tfms=Normalize.from_stats(*imagenet_stats) ) dls = cats.dataloaders(path, bs=32) . dls.show_batch() . learn = cnn_learner(dls, resnet18, y_range=(-1, 1)) learn.lr_find() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . SuggestedLRs(lr_min=0.006918309628963471, lr_steep=2.75422871709452e-06) . learn.fit_one_cycle(40, lr_max=1e-2) . epoch train_loss valid_loss time . 0 | 0.296482 | 0.129332 | 01:57 | . 1 | 0.119968 | 0.055120 | 01:56 | . 2 | 0.042793 | 0.026059 | 01:55 | . 3 | 0.024017 | 0.015928 | 01:55 | . 4 | 0.017797 | 0.012374 | 01:55 | . 5 | 0.014040 | 0.009886 | 01:55 | . 6 | 0.009658 | 0.006935 | 01:55 | . 7 | 0.009536 | 0.006449 | 01:55 | . 8 | 0.011302 | 0.008175 | 01:55 | . 9 | 0.009440 | 0.007143 | 01:55 | . 10 | 0.009881 | 0.009669 | 01:56 | . 11 | 0.008684 | 0.007935 | 01:57 | . 12 | 0.009724 | 0.004974 | 01:56 | . 13 | 0.009400 | 0.007061 | 01:57 | . 14 | 0.008699 | 0.006851 | 01:56 | . 15 | 0.009108 | 0.007808 | 01:55 | . 16 | 0.007757 | 0.006127 | 01:55 | . 17 | 0.007726 | 0.006757 | 01:55 | . 18 | 0.007713 | 0.007839 | 01:55 | . 19 | 0.008108 | 0.005453 | 01:55 | . 20 | 0.007298 | 0.003527 | 01:55 | . 21 | 0.006495 | 0.004359 | 01:55 | . 22 | 0.007149 | 0.006896 | 01:55 | . 23 | 0.006911 | 0.005034 | 01:55 | . 24 | 0.006115 | 0.003656 | 01:56 | . 25 | 0.006072 | 0.003372 | 01:55 | . 26 | 0.006040 | 0.003762 | 01:56 | . 27 | 0.005692 | 0.003424 | 01:55 | . 28 | 0.005528 | 0.003468 | 01:55 | . 29 | 0.005169 | 0.003033 | 01:55 | . 30 | 0.004689 | 0.003298 | 01:55 | . 31 | 0.005181 | 0.003043 | 01:55 | . 32 | 0.005120 | 0.003019 | 01:55 | . 33 | 0.004678 | 0.002923 | 01:55 | . 34 | 0.004722 | 0.003022 | 01:55 | . 35 | 0.004634 | 0.002970 | 01:56 | . 36 | 0.004586 | 0.003091 | 01:56 | . 37 | 0.004500 | 0.002810 | 01:56 | . 38 | 0.004204 | 0.002859 | 01:57 | . 39 | 0.004745 | 0.002807 | 01:56 | . learn.show_results(ds_idx=1, max_n=10, figsize=(10, 20)) . .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/deep%20learning/2021/01/12/Find-cat's-mouth-with-fastai.html",
            "relUrl": "/category/project/deep%20learning/2021/01/12/Find-cat's-mouth-with-fastai.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Applying Transfer Learning to CIFAR10",
            "content": "from fastai.vision.all import * import warnings warnings.filterwarnings(&quot;ignore&quot;) . Prepare for the data . path = untar_data(URLs.CIFAR) . path.ls() . (#3) [Path(&#39;/root/.fastai/data/cifar10/labels.txt&#39;),Path(&#39;/root/.fastai/data/cifar10/test&#39;),Path(&#39;/root/.fastai/data/cifar10/train&#39;)] . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(), item_tfms=RandomResizedCrop(256, min_scale=0.3), batch_tfms=[*aug_transforms(size=96), Normalize]) dls = dblock.dataloaders(path/&quot;train&quot;, bs=128) . dls.show_batch(max_n=12) . test_dl = dls.test_dl(get_image_files(path/&quot;test&quot;), with_labels=True) . Train the model . learn = cnn_learner(dls, resnet50, metrics=accuracy) . learn.lr_find() . SuggestedLRs(valley=tensor(0.0003)) . learn.fit_one_cycle(3, lr_max=1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.761423 | 0.317356 | 0.894200 | 02:02 | . 1 | 0.485832 | 0.219827 | 0.924800 | 02:00 | . 2 | 0.420892 | 0.200995 | 0.930300 | 02:00 | . #learn = learn.load(&quot;freezed1&quot;) . learn.unfreeze() learn.lr_find() . SuggestedLRs(valley=tensor(2.0893e-05)) . learn.fit_one_cycle(20, lr_max=1e-4) . epoch train_loss valid_loss accuracy time . 0 | 0.399815 | 0.183101 | 0.936000 | 02:16 | . 1 | 0.371782 | 0.165345 | 0.942600 | 02:15 | . 2 | 0.336096 | 0.170846 | 0.944500 | 02:15 | . 3 | 0.342313 | 0.175274 | 0.941200 | 02:16 | . 4 | 0.318031 | 0.173293 | 0.940800 | 02:16 | . 5 | 0.291324 | 0.190444 | 0.937300 | 02:16 | . 6 | 0.274913 | 0.145252 | 0.950900 | 02:16 | . 7 | 0.238502 | 0.147286 | 0.949900 | 02:15 | . 8 | 0.226633 | 0.151125 | 0.950300 | 02:16 | . 9 | 0.192331 | 0.136279 | 0.955000 | 02:15 | . 10 | 0.186196 | 0.128830 | 0.958700 | 02:16 | . 11 | 0.162285 | 0.131691 | 0.958900 | 02:16 | . 12 | 0.154912 | 0.118612 | 0.962200 | 02:16 | . 13 | 0.117298 | 0.118969 | 0.963000 | 02:15 | . 14 | 0.109438 | 0.114675 | 0.963500 | 02:15 | . 15 | 0.109949 | 0.112976 | 0.964100 | 02:16 | . 16 | 0.092123 | 0.114393 | 0.965500 | 02:16 | . 17 | 0.085689 | 0.109543 | 0.966200 | 02:15 | . 18 | 0.087017 | 0.109886 | 0.966200 | 02:15 | . 19 | 0.086428 | 0.109186 | 0.966300 | 02:16 | . Evalute the model on the test set . pred, targ = learn.tta(dl=test_dl) accuracy(pred, targ) . . TensorBase(0.9704) .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/fastai/pytorch/2021/01/05/Applying_Transfer_Learning_to_CIFAR_10_with_fastai.html",
            "relUrl": "/category/project/fastai/pytorch/2021/01/05/Applying_Transfer_Learning_to_CIFAR_10_with_fastai.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Predicting Prices of Machine Learning Books",
            "content": "Introduction . I like books and, as an aspiring data scientist, I must learn Machine Learning (ML). ML books are fascinating to me and, thus, I do a project on predicting prices of ML books to sharpen my practical ML skill. This post shows how I explore the data on Machine Learning books and make models to predict prices of Machine Learning books. . The data I used in this project was obtained by scraping book depository. The web scraping process are documented in notebooks stored this project&#39;s repository. The data contains 18 variables, each of which indicates a attribute of a book such as title, author, price, etc. . This project starts with importing data, getting basic information of the data, and some basic preprocessing with Pandas. After that, I conducted exploratory data analysis, in which I computed statistics and made graphs to get insights on the data with Matplotlib and Seaborn. Lastly, I built machine learning models to predict books prices with Scikit-learn and fastai. . Import the data and basic preprocessing . First, we import the data and get basic information about it. . from fastai.tabular.all import * import seaborn as sns plt.rcParams[&quot;figure.figsize&quot;] = (10, 8) plt.rcParams[&quot;figure.dpi&quot;] = 80 books = pd.read_csv(&quot;books_cleaned_final.csv&quot;) books.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1914 entries, 0 to 1913 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Name 1914 non-null object 1 Authors 1914 non-null object 2 Price 1914 non-null float64 3 Image-url 1914 non-null object 4 Rating 615 non-null float64 5 NumberOfPages 1902 non-null float64 6 Format 1914 non-null object 7 Publication date 1914 non-null object 8 Publisher 1914 non-null object 9 Language 1907 non-null object 10 ISBN10 1798 non-null object 11 ISBN13 1914 non-null int64 12 Publication City 982 non-null object 13 Publication Country 982 non-null object 14 Width 1696 non-null float64 15 Height 1696 non-null float64 16 Thickness 1696 non-null float64 17 Weight 1696 non-null float64 dtypes: float64(7), int64(1), object(10) memory usage: 269.3+ KB . From the table above, we know there are 1914 observations and 18 variables in the data. I think most variable names are self-explained but a few may cause confusion. Name is the title of the book and Image-url is the url to the cover image. . Price is in NTD (New Taiwan Dollar). Width, Hidth, and Thickness are measured in mm (millimeter). Weight is in g(gram). . Image-url, ISBN10, and ISBN13 do not give much information about the book and are useless for predicting books prices, so I eliminate them from the data. Also, to get more information, I add some variables about the publication date. . books.drop([&quot;Image-url&quot;, &quot;ISBN10&quot;, &quot;ISBN13&quot;], axis=1, inplace=True) books = add_datepart(books, &quot;Publication date&quot;) books.columns . Index([&#39;Name&#39;, &#39;Authors&#39;, &#39;Price&#39;, &#39;Publication Week&#39;, &#39;Rating&#39;, &#39;NumberOfPages&#39;, &#39;Format&#39;, &#39;Publisher&#39;, &#39;Language&#39;, &#39;Publication City&#39;, &#39;Publication Country&#39;, &#39;Width&#39;, &#39;Height&#39;, &#39;Thickness&#39;, &#39;Weight&#39;, &#39;Publication Year&#39;, &#39;Publication Month&#39;, &#39;Publication Day&#39;, &#39;Publication Dayofweek&#39;, &#39;Publication Dayofyear&#39;, &#39;Publication Is_month_end&#39;, &#39;Publication Is_month_start&#39;, &#39;Publication Is_quarter_end&#39;, &#39;Publication Is_quarter_start&#39;, &#39;Publication Is_year_end&#39;, &#39;Publication Is_year_start&#39;, &#39;Publication Elapsed&#39;], dtype=&#39;object&#39;) . Exploratory data analysis . Now, we start getting insights from the data by making some graphs and tables. . Investigating the target variable - Price . The target variable, Price, is of our primary interset, so we investigate it first. . books[&quot;Price&quot;].describe() . count 1914.000000 mean 1838.880878 std 2061.328347 min 236.000000 25% 697.000000 50% 1224.500000 75% 2514.000000 max 42667.000000 Name: Price, dtype: float64 . plt.figure(figsize=(10, 4)) sns.histplot(books, x=&quot;Price&quot;); . The graph and table above tell us that the Price variable is skewed to the right, so I take log of all the prices. . plt.figure(figsize=(10, 4)) sns.histplot(books, x=&quot;Price&quot;, log_scale=True); . books.loc[:, &quot;Price&quot;] = np.log(books[&quot;Price&quot;]) . Correlations between variables . To observe the correlations betweens variable, I make a correlation heatmap. Since Price is of our primary concern, my focus is on variabels correlated to Price. . sns.heatmap(books.corr(), square=True, cmap=sns.diverging_palette(20, 220, as_cmap=True), vmin=-1, vmax=1); . It seems that variables correlated to Price is on the upper left of the heatmap, so we zoom in to the upper left region to get clearer insight. . num_var = [&quot;Price&quot;, &quot;NumberOfPages&quot;, &quot;Width&quot;, &quot;Height&quot;, &quot;Thickness&quot;, &quot;Weight&quot;, &quot;Publication Year&quot;] sns.heatmap(books.loc[:,num_var].corr(), square=True, annot=True, cmap=sns.diverging_palette(20, 220, as_cmap=True), vmin=-1, vmax=1); . Compared to the other variables, NumberOfPages, Thickness and Weight are positively correlated to Price, which tells us that books in bigger size may cost more than ones in smaller size. In contrast, Pubshication Year has negative correlation with Price, which implies new books may be more expensive than old ones. . Other variables . In this section, I will investigate variables other than Price and how these varables relate to Price. . Format . books[&quot;Format&quot;].value_counts() . Paperback 1422 Hardback 492 Name: Format, dtype: int64 . Formate indicates if a book is in paperback or in hardback. From the table above, we can know that 1422 paperback books and 492 hardback books are in our dataset. Let&#39;s see the difference in Price between different format of books. . fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5)) sns.histplot(data=books, x=&quot;Price&quot;, hue=&quot;Format&quot;, ax=ax[0]) sns.boxplot(data=books, x=&quot;Format&quot;, y=&quot;Price&quot;, ax=ax[1]); . Not Surprisingly, most hardback books are more expensive than paperback books. . Publisher . books[&quot;Publisher&quot;].nunique() . 187 . As you can imagine, many pubishers published ML books, so we can expect that Publisher contains multiple values. From the code above, we know that 187 pubishers are in our dataset. To get information easily, I will focus on top-10 pushlishers in the dataset. . books[&quot;Publisher&quot;].value_counts()[:10] . Springer 398 Independently Published 342 Packt Publishing Limited 153 Createspace Independent Publishing Platform 94 LAP Lambert Academic Publishing 76 Taylor &amp; Francis 64 aPress 57 Wiley 56 Charlie Creative Lab 31 O&#39;Reilly Media, Inc, USA 28 Name: Publisher, dtype: int64 . pname = books[&quot;Publisher&quot;].value_counts()[:10].index plt.figure(figsize=(20, 20)) sns.boxplot(data=books, y=&#39;Publisher&#39;, x=&quot;Price&quot;, order=pname) plt.setp(plt.gca().get_yticklabels(), fontsize=20) plt.ylabel(&quot;&quot;) plt.xlabel(&quot;Price&quot;, fontsize=20); . As suggested by the plot, books published by Springer, Taylor &amp; Francis, and Wiley have higher prices on average. On the other hand, independetly published books have lower prices. . Language . books[&quot;Language&quot;].value_counts() . English 1869 German 17 Spanish 16 Portuguese 2 French 2 English, German 1 Name: Language, dtype: int64 . ML books are wriiten in 5 languages, mostly in English. One book was marked as being written in both English and Ferman. Let&#39;s find out which book it is. . books[books[&quot;Language&quot;] == &quot;English, German&quot;].iloc[:, :10] . Name Authors Price Publication Week Rating NumberOfPages Format Publisher Language Publication City . 1296 Susanne Huth - Analog Algorithm - Landscapes of Machine Learning | Maren Lubbke-tidow, Susanne Huth | 6.767343 | 33 | NaN | 80.0 | Paperback | Fotohof | English, German | Salzburg | . After some research, I decide to change the laguage of this book into German for the sake of simplicity. . books.loc[1296, &quot;Language&quot;] = &quot;German&quot; . Publication Country . Next, I study which country ML books are published in. . books[&quot;Publication Country&quot;].value_counts() . United States 337 United Kingdom 259 Switzerland 218 Germany 83 Singapore 59 Netherlands 16 Canada 5 India 4 Austria 1 Name: Publication Country, dtype: int64 . sns.boxplot(x=&quot;Price&quot;, y=&quot;Publication Country&quot;, data=books,); . Most ML book are published in United States, United Kingdom and Switzerland. . Publication City . Publication City also contains a number of values. Let&#39;s see distinct values in this variable. . books[&quot;Publication City&quot;].unique() . array([&#39;New York, NY&#39;, &#39;Cambridge&#39;, &#39;New York&#39;, nan, &#39;Cham&#39;, &#39;Bosa Roca&#39;, &#39;Portland&#39;, &#39;London&#39;, &#39;OH&#39;, &#39;Washington&#39;, &#39;San Diego&#39;, &#39;New Jersey&#39;, &#39;Singapore&#39;, &#39;Bristol&#39;, &#39;Hoboken&#39;, &#39;Oxford&#39;, &#39;Bingley&#39;, &#39;Berlin&#39;, &#39;England&#39;, &#39;Hershey&#39;, &#39;Oakville&#39;, &#39;San Rafael&#39;, &#39;Dordrecht&#39;, &#39;Stevenage&#39;, &#39;Chicago, IL&#39;, &#39;Maryland&#39;, &#39;Harrisburg, PA&#39;, &#39;Norwood&#39;, &#39;Boca Raton, FL&#39;, &#39;Morrisville&#39;, &#39;Philadelphia&#39;, &#39;Lanham, MD&#39;, &#39;Annopolis&#39;, &#39;Pennsauken&#39;, &#39;Bern&#39;, &#39;Sebastopol&#39;, &#39;Birmingham&#39;, &#39;Boston&#39;, &#39;Shelter Island, N.Y.&#39;, &#39;San Francisco&#39;, &#39;Berkley&#39;, &#39;Farnham&#39;, &#39;Raleigh&#39;, &#39;Preston&#39;, &#39;Bradley Beach&#39;, &#39;Chichester, England&#39;, &#39;Hanover&#39;, &#39;New Delhi&#39;, &#39;Sim Valley, CA&#39;, &#39;Weisbaden&#39;, &#39;Saarbrucken&#39;, &#39;Santa Monica, CA&#39;, &#39;Salzburg&#39;, &#39;Rockland, MA&#39;, &#39;Houston&#39;, &#39;San Rafael, CA&#39;, &#39;[Bellevue, Washington]&#39;], dtype=object) . There are two values indicating that a book is published in New York City. Let&#39;s set them into the same value. . books[&quot;Publication City&quot;].replace(&quot;New York&quot;, &quot;New York, NY&quot;, inplace=True) . As in Publisher part, I only focus on the cities books are most pushished in and see how the difference of prices in these cities. . city_name = books[&quot;Publication City&quot;].value_counts()[:20].index city_name . Index([&#39;Cham&#39;, &#39;Birmingham&#39;, &#39;New York, NY&#39;, &#39;Berlin&#39;, &#39;Singapore&#39;, &#39;London&#39;, &#39;Berkley&#39;, &#39;Cambridge&#39;, &#39;Sebastopol&#39;, &#39;Hershey&#39;, &#39;San Diego&#39;, &#39;Dordrecht&#39;, &#39;Hoboken&#39;, &#39;San Rafael&#39;, &#39;Hanover&#39;, &#39;England&#39;, &#39;Bosa Roca&#39;, &#39;Boston&#39;, &#39;Portland&#39;, &#39;Saarbrucken&#39;], dtype=&#39;object&#39;) . plt.figure(figsize=(8, 10)) sns.boxplot(x=&quot;Price&quot;, y=&quot;Publication City&quot;, data=books, order=city_name); . . Predict prices . This section consists of two parts: preprocessing and modeling. In proceessing stage, I dealt with missing values by imputation and split the data set into training and test set. In the modeling stage, I first decide to use rooted mean squared error(rmse)as metric, and the use the mean of Price to predict book prices as a baseline method. Next, I built machine learning models including linear regrssion, random forest and neural network, and used cross-validtion to tune the hyperparameters of the first two models. . Preprocessing . Deal with missing values . I impute the missing values in numeraical variables with median which is commonly used and more robust than mean. . median_impute_var = [&quot;NumberOfPages&quot;, &quot;Rating&quot;, &quot;Width&quot;, &quot;Height&quot;, &quot;Thickness&quot;, &quot;Weight&quot;] books.loc[:, median_impute_var] = books[median_impute_var].fillna(books[median_impute_var].median()) . Language contains only 7 missing values and I replace these values with the most frequent value, &quot;English&quot;. . books.loc[:, &quot;Language&quot;] = books[&quot;Language&quot;].fillna(&quot;English&quot;) . Since there are almost 1000 missing values in Publication City and Publication Country, it might be inappropriate to impute them with the most frequent values, so I use a self-defined value, &quot;unknown&quot; to fill out these values. . unknown_impute = [&quot;Publication City&quot;, &quot;Publication Country&quot;] books.loc[:, unknown_impute] = books.loc[:,unknown_impute].fillna(&quot;unknown&quot;) . Now, we have a dataset withour missing values. . books.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1914 entries, 0 to 1913 Data columns (total 27 columns): # Column Non-Null Count Dtype -- -- 0 Name 1914 non-null object 1 Authors 1914 non-null object 2 Price 1914 non-null float64 3 Publication Week 1914 non-null UInt32 4 Rating 1914 non-null float64 5 NumberOfPages 1914 non-null float64 6 Format 1914 non-null object 7 Publisher 1914 non-null object 8 Language 1914 non-null object 9 Publication City 1914 non-null object 10 Publication Country 1914 non-null object 11 Width 1914 non-null float64 12 Height 1914 non-null float64 13 Thickness 1914 non-null float64 14 Weight 1914 non-null float64 15 Publication Year 1914 non-null int64 16 Publication Month 1914 non-null int64 17 Publication Day 1914 non-null int64 18 Publication Dayofweek 1914 non-null int64 19 Publication Dayofyear 1914 non-null int64 20 Publication Is_month_end 1914 non-null bool 21 Publication Is_month_start 1914 non-null bool 22 Publication Is_quarter_end 1914 non-null bool 23 Publication Is_quarter_start 1914 non-null bool 24 Publication Is_year_end 1914 non-null bool 25 Publication Is_year_start 1914 non-null bool 26 Publication Elapsed 1914 non-null object dtypes: UInt32(1), bool(6), float64(7), int64(5), object(8) memory usage: 319.7+ KB . Select target and feeatures, split the dataset . First, I drop two variables that we do not use in modeling. . books.drop([&quot;Name&quot;, &quot;Authors&quot;], axis=1, inplace=True) . Next, I normalize the numerical variables, use entity embedding for categorical variables and randomly preseerve 20 percent of data as test set. . dep_var = &quot;Price&quot; # preprocessing proc = [Categorify, Normalize] # find out which variable is continuous and which is categorical cont, cat = cont_cat_split(books, max_card=2,dep_var=dep_var) # split the dataset train_size = int(len(books)*0.8) splits = RandomSplitter()(books.index) # wrap the preprocessing and the split into fastai&#39;s TabularPandas to = TabularPandas(books, proc, cat, cont, y_names=dep_var, splits=splits) # get target and features for training and testing xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y . Modeling . In the modeling stage, I first define the metric, rmse, and write helper functions for computing cv scores and getting test score. . from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer def r_mse(inp, tar):return np.sqrt(np.mean((inp-tar)**2)) my_scorer = make_scorer(r_mse, greater_is_better=False) def get_cv_error(model, xs=xs, y=y): cv_score = cross_val_score(model, X=xs, y=y, cv=5, n_jobs=-1, scoring=my_scorer) * -1 print(f&quot;Mean CV RMSE: {cv_score.mean():.4f}, Std: {cv_score.std():.4f}&quot;) def get_test_error(model, valid_xs=valid_xs, valid_y=valid_y): return r_mse(model.predict(valid_xs), valid_y) . Baseline - predicting with mean price . To understand how good your machine learning models are, I use the mean of price to predict book proces as a baseline method. . r_mse(y.mean(), valid_y) . 0.773462699710809 . Linear Regression . from sklearn.linear_model import LinearRegression linear_reg = LinearRegression().fit(xs, y) get_cv_error(linear_reg) . Mean CV RMSE: 0.4873, Std: 0.0144 . A linear regression model has a 0.487 rmse of 5-fold cv score using training set. How does it perform on the test set? . get_test_error(linear_reg) . 0.4592763516940994 . It get a 0.487 rmse on the test test which is much better than 0.77 from the baseline method. . Random Forest . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(oob_score=True) get_cv_error(rf) . Mean CV RMSE: 0.3564, Std: 0.0125 . A random forest model get a rmse of only 0.356 without tuning. Before tuning the hyperparameters of random forest, I look into the feature importance to see if I can eliminate some variables. . rf = RandomForestRegressor(n_jobs=-1).fit(xs, y) def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf, xs) def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Several variables associated with time seems not useful for us to predict books prices, so I eliminate them. . to_keep = fi[fi.imp&gt;0.002].cols . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] rf = RandomForestRegressor() get_cv_error(rf, xs=xs_imp, y=y) . Mean CV RMSE: 0.3524, Std: 0.0119 . After the elimination, we still get a close (even better) rmse to the score using the whole dataset. . Let&#39;s tune the random forest. . I use random search to tune the model. Random search generate 50 different set of hyperparameter randomly, train the model with these hyperparameters, and store the one with highest rmse. . from sklearn.model_selection import RandomizedSearchCV rf = RandomForestRegressor(n_jobs=-1) params_range = {&#39;n_estimators&#39;:np.arange(100, 1000), &#39;max_depth&#39;:np.arange(5, 100), &#39;min_samples_leaf&#39;:np.arange(1, 50), &#39;max_features&#39;:[0.3, 0.5, 0.7]} rscv = RandomizedSearchCV(rf, params_range, scoring=my_scorer, n_jobs=-1, cv=5, n_iter=50).fit(xs_imp, y) best_rf = rscv.best_estimator_ rscv.best_score_ * -1 . 0.3518920631011618 . get_test_error(best_rf, valid_xs=valid_xs_imp) . 0.3439916357973344 . After tuning, our random forest model get 0.344 rmse on test set with less variables. . Neural Network . In this part, I build neural network with fastai which is based on Pytorch. . dls = to.dataloaders(16) . y = to.train.y y.min(), y.max() . (5.497168064117432, 10.661181449890137) . I use a neural network with 2 hidden layers with 100 and 50 hidden units respectively. . learn = tabular_learner(dls, y_range=(5,11), layers=[100, 50], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . SuggestedLRs(lr_min=0.006918309628963471, lr_steep=1.9054607491852948e-06) . learn.fit_one_cycle(5, lr_max=5e-2) . epoch train_loss valid_loss time . 0 | 0.338614 | 0.546071 | 00:01 | . 1 | 0.242191 | 0.146987 | 00:01 | . 2 | 0.158445 | 0.131846 | 00:01 | . 3 | 0.111698 | 0.124844 | 00:01 | . 4 | 0.074375 | 0.124781 | 00:01 | . preds, targs = learn.get_preds() . (preds - targs).pow(2).mean().pow(1/2) . tensor(0.3532) . After only 5 second of training, we get 0.353 of rmse. . The random forest model get 0.34 rmse score. This rmse has been rescaled into log scale, so it&#39;s 1.41 ($e^{0.344}$) on NTD scale which is quite small! .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/2020/12/25/Predicting-prices-of-Machine-Learning-Books.html",
            "relUrl": "/category/project/2020/12/25/Predicting-prices-of-Machine-Learning-Books.html",
            "date": " • Dec 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Find quality wine with machine learning algorithms",
            "content": "Introduction . In this project, I build machine learning models to find high quality wines (ones with quaity greater than and equal to 7). The data is obtained from the UCI machine learning repository (here) and originated from this paper. . The analysis proceeds in this order: . preliminary data processing | data exploring | preprocessing before modeling | determine the evaluation metric | train and tune models | . Here&#39;s Python libraries used in this project and some settings: . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns plt.rcParams[&quot;figure.dpi&quot;] = 100 import warnings warnings.filterwarnings(&#39;ignore&#39;) . Preliminary Data processing . We first import two datasets containing different type of wines: . red = pd.read_csv(&quot;data/winequality-red.csv&quot;, sep=&quot;;&quot;) white = pd.read_csv(&quot;data/winequality-white.csv&quot;, sep=&quot;;&quot;) . Take a brief look at our datasets: . red.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . 3 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17.0 | 60.0 | 0.9980 | 3.16 | 0.58 | 9.8 | 6 | . 4 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . white.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . The data contains: . ingredients: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, sulphates | properties: density, pH, alcohol, quality | . Next, we combine two datasets into one, add a &#39;type&#39; variable, and shuffle the dataset. . white[&quot;type&quot;] = &quot;white&quot; red[&quot;type&quot;] = &quot;red&quot; # concatenate two datasets wine = pd.concat([white, red], axis=0) # shuffle wine = wine.sample(frac=1).reset_index(drop=True) wine.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality type . 0 8.2 | 0.28 | 0.42 | 1.80 | 0.031 | 30.0 | 93.0 | 0.99170 | 3.09 | 0.39 | 11.4 | 5 | white | . 1 7.0 | 0.13 | 0.37 | 12.85 | 0.042 | 36.0 | 105.0 | 0.99581 | 3.05 | 0.55 | 10.7 | 6 | white | . 2 8.1 | 0.20 | 0.49 | 11.80 | 0.048 | 46.0 | 212.0 | 0.99680 | 3.09 | 0.46 | 10.0 | 7 | white | . 3 5.4 | 0.27 | 0.22 | 4.60 | 0.022 | 29.0 | 107.0 | 0.98889 | 3.33 | 0.54 | 13.8 | 6 | white | . 4 7.8 | 0.56 | 0.19 | 2.00 | 0.081 | 17.0 | 108.0 | 0.99620 | 3.32 | 0.54 | 9.5 | 5 | red | . Explore the data . Information about the dataset: . wine.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 13 columns): # Column Non-Null Count Dtype -- -- 0 fixed acidity 6497 non-null float64 1 volatile acidity 6497 non-null float64 2 citric acid 6497 non-null float64 3 residual sugar 6497 non-null float64 4 chlorides 6497 non-null float64 5 free sulfur dioxide 6497 non-null float64 6 total sulfur dioxide 6497 non-null float64 7 density 6497 non-null float64 8 pH 6497 non-null float64 9 sulphates 6497 non-null float64 10 alcohol 6497 non-null float64 11 quality 6497 non-null int64 12 type 6497 non-null object dtypes: float64(11), int64(1), object(1) memory usage: 660.0+ KB . The dataset contains 13 variable, 11 numerical and 1 categorical. It has 6497 observations in it with no missing values. quality is our target variable and the others are our independent variables. . Quality . First, let&#39;s see the distribution of our target variable qualty. . wine_value_counts = wine.quality.value_counts().sort_index() # draw a bar chart wine_value_counts.plot(kind=&#39;barh&#39;) plt.ylabel(&quot;quality&quot;) plt.xlabel(&quot;counts&quot;); . (wine_value_counts.cumsum()/6497).round(2) . 3 0.00 4 0.04 5 0.37 6 0.80 7 0.97 8 1.00 9 1.00 Name: quality, dtype: float64 . We can know 80% of wine in our data set has quality lower than 6. Based on this information, we consider wine with quality higher than 6 as high quality wine. . Correlations . Next, we investigate which independent variable is correlated with our target quality. . plt.figure(figsize=(12, 8)) sns.heatmap(wine.corr(), cmap=&quot;Blues&quot;, annot=True, vmin=-1); . Three variables are correlated with quality: alcohol, density, and chlorides. Using boxplots, we can observe the distribution of these three variables between wines with different quality: . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;alcohol&quot;, x=&quot;quality&quot;, data=wine); . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;density&quot;, x=&quot;quality&quot;, data=wine) plt.ylim(0.98, 1.01); . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;chlorides&quot;, x=&quot;quality&quot;, data=wine) plt.ylim(0, 0.2); . . Predictive Modeling - is high quality or not? . Now that we have a preliminary understanding of the data, we can start to build our models. However, the data set requires some modifications to be consumed by our machine learning models. . Preprocessing . Three modifications are conducted: . form independent variables and the target | standardizing and label encoding | split train and test sets | . form independent variables and the target . wine[&quot;isHigh&quot;] = wine.quality.apply(lambda x:1 if x &gt;= 7 else 0) X = wine.drop([&quot;quality&quot;, &quot;isHigh&quot;], axis=1) y = wine.isHigh.values . standardizing and label encoding . from sklearn.preprocessing import StandardScaler, LabelEncoder # split the numerical variables and categorical variable num_cols = X.columns[:-1] cat_cols = X.columns[-1] # standardizing numerical variables std_scaler = StandardScaler() X.loc[:, num_cols] = std_scaler.fit_transform(X[num_cols]) # one-hot encoding categorical variable lbl_enc = LabelEncoder() X.loc[:, cat_cols] = lbl_enc.fit_transform(X[cat_cols]) . split train and test sets . wine.isHigh.value_counts()/6497 . 0 0.803448 1 0.196552 Name: isHigh, dtype: float64 . Since the target variable is unbalaned, we split our data set in a stratified manner. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) . Evaluation metric . Since we get an unbalanced dataset, f1 score is used as the evaluation metric. . Here we write a function to evaluate our predicting result: . from sklearn.metrics import confusion_matrix, f1_score def print_metric(y_true, y_pred): f1 = f1_score(y_true, y_pred) confusion = pd.DataFrame(confusion_matrix(y_test, y_pred), index=[&quot;True_low&quot;, &quot;True_high&quot;], columns=[&quot;Pred_low&quot;, &quot;Pred_high&quot;]) print(f&quot;F1 score: {f1:.4f}&quot;) print() print(confusion) . Models and Hyperparameter tuning . Four models are considered: . baseline: use the mean of each class | logistic regression | random forest | xgboost | . Baseline method - use the mean of each class . This method determine whether a wine is high quality one or not by computing the distance between the mean values of high-quality wine and low-quality wine. . def compute_mean(X, y): high_mean = X[y == 1].mean(0) low_mean = X[y == 0].mean(0) return high_mean, low_mean # to compute the distance to high_mean and low_mean def wine_distance(x1, x2):return (x1-x2).abs().mean(1) # make predictions by the distance def is_high(x, X_train, y_train): high_mean, low_mean = compute_mean(X_train, y_train) return wine_distance(x, high_mean) &lt; wine_distance(x, low_mean) # make predictions y_pred = is_high(X_test, X_train, y_train) print_metric(y_test, y_pred) . F1 score: 0.4806 Pred_low Pred_high True_low 863 443 True_high 78 241 . Using this baseline method we get 0.48 f1 score. We expect that our machine learning models could get scores better than this. So next, we start using machine learning models. . Machine learning models . I use 5-fold cross-validation and randomized search to tune the hyperparemeters and use the test set to evaluate the final preformance. . Here are function implementing the stratified cv precedures: . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import StratifiedKFold, cross_val_score from sklearn.model_selection import RandomizedSearchCV strat_cv = StratifiedKFold(n_splits=5, shuffle=True) # cv result without tuning def get_cv_result(model): &quot;&quot;&quot;Take into a model and return cv f1 score&quot;&quot;&quot; cv_score = cross_val_score(model, X=X_train, y=y_train, scoring=&#39;f1&#39;, cv=strat_cv, n_jobs=-1) print(f&quot;Mean CV score: {cv_score.mean():.4f}&quot;) print(f&quot;Std: {cv_score.std():.4f}&quot;) # using random search to tuning the hyperparameters def tuning(model, params): randomCV = RandomizedSearchCV( estimator=model, param_distributions=params, cv=strat_cv, n_jobs=-1, scoring=&quot;f1&quot;, n_iter=50 ).fit(X_train, y_train) print(f&quot;Best f1 score: {randomCV.best_score_:.4f}&quot;) print(&quot;Best parameters:&quot;) for para, value in randomCV.best_params_.items(): print(f&quot; {para:10}: {value:.4f}&quot;) return randomCV.best_estimator_ . In the following subsection, we will first train the model without tuning and then tune the model so that we can see the improvement after tuning. . Logistic Regression . logit = LogisticRegression() get_cv_result(logit); . Mean CV score: 0.3594 Std: 0.0265 . logit_params = {&quot;C&quot;:np.linspace(0.001, 100, 200)} logit_tuned = tuning(logit, logit_params) . Best f1 score: 0.3558 Best parameters: C : 6.0311 . logit_tuned.fit(X_train, y_train) y_pred = logit_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.3684 Pred_low Pred_high True_low 1253 53 True_high 235 84 . Random Forest . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() get_cv_result(rf) . Mean CV score: 0.6275 Std: 0.0314 . rf_params = { &quot;n_estimators&quot;:np.arange(100, 1500), &quot;max_depth&quot;:np.arange(1, 50), &quot;max_features&quot;:[0.3, 0.5, 0.7], &quot;min_samples_leaf&quot;:np.arange(1, 30) } rf_tuned = tuning(rf, rf_params) . Best f1 score: 0.6316 Best parameters: n_estimators: 160.0000 min_samples_leaf: 1.0000 max_features: 0.3000 max_depth : 13.0000 . rf_tuned.fit(X_train, y_train) y_pred = rf_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.6415 Pred_low Pred_high True_low 1265 41 True_high 149 170 . XGBoost . import xgboost as xgb xgb_cls = xgb.XGBClassifier(eval_metric=&quot;logloss&quot;, use_label_encoder=False) get_cv_result(xgb_cls) . Mean CV score: 0.6506 Std: 0.0089 . xgb_params = { &quot;n_estimators&quot;:np.arange(1, 1500), &quot;max_depth&quot;:np.arange(1, 50), &quot;learning_rate&quot;:np.linspace(0.01, 0.1, 100), &quot;gamma&quot;:np.linspace(0.05, 1, 100), &quot;min_child_weight&quot;:[1,3,5,7], &quot;subsample&quot;:[0.6, 0.7, 0.8, 0.9, 1.0], &quot;colsample_bytree&quot;:[0.6, 0.7, 0.8, 0.9, 1.0], &quot;reg_lambda&quot;:np.linspace(0.01, 1, 100) } xgb_tuned = tuning(xgb_cls, xgb_params) . Best f1 score: 0.6647 Best parameters: subsample : 0.7000 reg_lambda: 0.6900 n_estimators: 1440.0000 min_child_weight: 3.0000 max_depth : 10.0000 learning_rate: 0.0373 gamma : 0.6066 colsample_bytree: 0.9000 . xgb_tuned.fit(X_train, y_train) y_pred = xgb_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.6667 Pred_low Pred_high True_low 1251 55 True_high 132 187 . Summary . From the exploratory data analysis, we determine what is a high-qulity wine. Also, we know that alcohol, density, and chlorides are correlated with our target quality. Since only 20 precent of wines are high-qualty, we use f1 score as evaluation metrics rather than accuracy. From a baseline method, we get a f1 score of 0.48. We don&#39;t get a better result using logistic regression model but we get much better result using more sophisticate ensemble models, random forest and xgboost, which get f1 score of 0.64 and 0.67 respectively. .",
            "url": "https://peiyihung.github.io/mywebsite/project/2020/12/20/Wine-types-and-quality.html",
            "relUrl": "/project/2020/12/20/Wine-types-and-quality.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Why do we need nonlinear activation functions in a neural network ?",
            "content": "Introduction . A neural network is composed of neurons. A neuron usually multiplies inputs values by weights, adds a bias to the product of weights and inputs, and passes the sum into a nonlinear activation function. In an equation form, the computation a neuron does is $$a = f( sum_{j=1}^{n}{w_jx_j} + b)$$ where $f$ is a nonlinear function such as tanh or ReLU, $w_j$ are weights, and $b$ is a bias. . The question I intended to answer in this article is why we need a nonlinear activation function. . We need a nonlinear function because our model would only learn linear patterns if we don&#39;t use one. I will illustrate this point by comparing two simple neural networks, one with nonlinear activation functions and the other without nonlinear functions. I will fit these two models to the data I generate from a nonlinear function and show you that the model without nonlinear functions can not capture a nonlinear pattern. . The illustration starts with generating data, proceeds with building and training models, and ends with comparing prediction performances of two models. Let&#39;s get started! . Generating data . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (10, 5) plt.style.use(&#39;fivethirtyeight&#39;) . . The data is generated from this equation: $$y = x^2 + 4x + 3 + epsilon, : epsilon sim N(0, 50)$$ where $y$ is a quadratic function adding a normally distributed error term $ epsilon$. We want our model to approximate this function. . Here&#39;s how the data looks like: . np.random.seed(5) x = np.linspace(-20, 20, 100) # independent variable y = x**2 + 4*x + 3 + np.random.randn(100)*50 # dependent variable plt.figure(dpi=100) plt.plot(x, y, &quot;.&quot;, label=&quot;Data points&quot;) plt.plot(x, 3 + x**2 + 4*x, &quot;-&quot;, label=&quot;True pattern&quot;) plt.legend(loc=&quot;upper center&quot;); . Build and Train Neural Networks . I contruct two neural networks with similar architecture. They are fully-connected networks with 4 hidden layers and 8 hidden units in each layer like this: . . The only difference is activation functions. One uses identity functions $f(x)=x$ and the other is equipped with ReLU functions $ReLU(x)=max(0, x)$. . The networks are contructed with PyTroch: . import torch from torch import nn from torch.utils.data import DataLoader # transfer numpy array to pytorch tennsor x = torch.tensor(x).unsqueeze(1).float() y = torch.tensor(y).unsqueeze(1).float() # form dataset and dataloader ds = [(xi, yi) for xi, yi in zip(x, y)] dl = DataLoader(ds, batch_size=20, shuffle=True) # the model without nonlinear function model1 = nn.Sequential( nn.Linear(1, 8), nn.Linear(8, 8), nn.Linear(8, 8), nn.Linear(8, 8), nn.Linear(8, 1) ) # the model with ReLU functions model2 = nn.Sequential( nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 1) ) . I take the mean squared error as loss function and use Adam as the optimization method. Here&#39;s how I train these two models: . from torch import optim import torch.nn.functional as F # set loss functin and optimizatino method loss_func = F.mse_loss opt1 = optim.Adam(model1.parameters()) opt2 = optim.Adam(model2.parameters()) # training lr = 5 for epoch in range(800): for xb, yb in dl: pred = model1(xb) loss = loss_func(pred, yb) loss.backward() opt1.step() opt1.zero_grad() for epoch in range(800): for xb, yb in dl: pred = model2(xb) loss = loss_func(pred, yb) loss.backward() opt2.step() opt2.zero_grad() . What&#39;s the difference between two models? . Now that these two model are trained we can see how they perform on capturing the nonlinear pattern. . plt.figure(figsize=(10, 5), dpi=120) plt.plot(x, y, &quot;k.&quot;) plt.plot(x, x**2 + 4*x + 3, label=&quot;True pattern&quot;) plt.plot(x, model1(x).detach().numpy(), label=&quot;Linear model&quot;) plt.plot(x, model2(x).detach().numpy(), label=&quot;ReLU model&quot;) plt.legend(fontsize=8); . . It is clear that the model with ReLU functions works better than the one without nonlinear functions. As suggested in the graph, model without nonlinear activation functions can only learn linear pattern. We will not encounter linear pattern every time when we analyze the data since the real world is so complex. Therefore, nonlinear activation functions are necessary for discovering the underlying pattern of the data. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/deep%20learning/machine%20learning/python/pytorch/2020/12/16/Why-do-we-need-nonlinear-activation-function.html",
            "relUrl": "/learning/deep%20learning/machine%20learning/python/pytorch/2020/12/16/Why-do-we-need-nonlinear-activation-function.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Interactive Visualization of Female Share of Bachelor’s degrees by Majors with Altair",
            "content": "In this project, I make some simple interactive graphs to show annual changes in proportions of female college major in different disciplines. The data I used is obtained from The Department of Education Statistics. It contains the proportions of female college majors in 17 disciplines in each year from 1970 to 2011. To make these analysis more easily understood, I will split these disciplines into three groups: STEM, Liberal Arts and the others, and plot each disciplines in these three groups. . I use Python for this project. The programming tool I used for visualizing the data is Altair, a declarative statistical visualization library for Python. The GitHub repository of this project is here. . This project consists of three parts. First, we will have a preview of our data to gain a brief understanding og it. Secondly, we process the data to make it easier to be visualized. Lastly, we make interactive graphs to discovery information from the data. . A preview of the data . import pandas as pd women_degrees = pd.read_csv(&#39;percent-bachelors-degrees-women-usa.csv&#39;) women_degrees.iloc[:, :8].head() . Year Agriculture Architecture Art and Performance Biology Business Communications and Journalism Computer Science . 0 1970 | 4.229798 | 11.921005 | 59.7 | 29.088363 | 9.064439 | 35.3 | 13.6 | . 1 1971 | 5.452797 | 12.003106 | 59.9 | 29.394403 | 9.503187 | 35.5 | 13.6 | . 2 1972 | 7.420710 | 13.214594 | 60.4 | 29.810221 | 10.558962 | 36.6 | 14.9 | . 3 1973 | 9.653602 | 14.791613 | 60.2 | 31.147915 | 12.804602 | 38.4 | 16.4 | . 4 1974 | 14.074623 | 17.444688 | 61.9 | 32.996183 | 16.204850 | 40.5 | 18.9 | . We can clearly see that the data contains the percentage of female students in each disciplines. Each row represents a year, and columns indicate the diciplines. Here&#39;s a lists of disciplines: . women_degrees.columns . Index([&#39;Year&#39;, &#39;Agriculture&#39;, &#39;Architecture&#39;, &#39;Art and Performance&#39;, &#39;Biology&#39;, &#39;Business&#39;, &#39;Communications and Journalism&#39;, &#39;Computer Science&#39;, &#39;Education&#39;, &#39;Engineering&#39;, &#39;English&#39;, &#39;Foreign Languages&#39;, &#39;Health Professions&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Psychology&#39;, &#39;Public Administration&#39;, &#39;Social Sciences and History&#39;], dtype=&#39;object&#39;) . Preprocessing . In this part, we tidy the data and divide disciplines into three groups. . Tidying the data . In order to easily visualize the data, we transform our data to a tidy data. But what&#39;s a tidy data? By the definition from Hadley Wickham&#39;s paper, a tidy data is . Each variable forms a column. | Each observation forms a row. | Each type of observational unit forms a table | We can use melt method to do that. . tidy_data = women_degrees.melt(id_vars=&quot;Year&quot;, value_vars=women_degrees.columns[1:], var_name=&quot;Discipline&quot;, value_name=&quot;Female proportion&quot;) tidy_data.head(10) . Year Discipline Female proportion . 0 1970 | Agriculture | 4.229798 | . 1 1971 | Agriculture | 5.452797 | . 2 1972 | Agriculture | 7.420710 | . 3 1973 | Agriculture | 9.653602 | . 4 1974 | Agriculture | 14.074623 | . 5 1975 | Agriculture | 18.333162 | . 6 1976 | Agriculture | 22.252760 | . 7 1977 | Agriculture | 24.640177 | . 8 1978 | Agriculture | 27.146192 | . 9 1979 | Agriculture | 29.633365 | . Divide disciplines into groups . Since there are 18 disciplines in the data, it would be messy if we plot all disciplines at once in our graph. Consequently, we group dsciplines into three groups, STEM, Liberal Arts and the others, and add a type column to store group information. . stem_cats = [&#39;Psychology&#39;, &#39;Biology&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Computer Science&#39;, &#39;Engineering&#39;] lib_arts_cats = [&#39;Foreign Languages&#39;, &#39;English&#39;, &#39;Communications and Journalism&#39;, &#39;Art and Performance&#39;, &#39;Social Sciences and History&#39;] other_cats = [&#39;Health Professions&#39;, &#39;Public Administration&#39;, &#39;Education&#39;, &#39;Agriculture&#39;, &#39;Business&#39;, &#39;Architecture&#39;] def which_type(subject): if subject in stem_cats: return &quot;STEM&quot; elif subject in lib_arts_cats: return &quot;Liberal Arts&quot; else : return &quot;Others&quot; tidy_data[&quot;Type&quot;] = tidy_data[&quot;Discipline&quot;].apply(which_type) tidy_data.head() . Year Discipline Female proportion Type . 0 1970 | Agriculture | 4.229798 | Others | . 1 1971 | Agriculture | 5.452797 | Others | . 2 1972 | Agriculture | 7.420710 | Others | . 3 1973 | Agriculture | 9.653602 | Others | . 4 1974 | Agriculture | 14.074623 | Others | . Plotting . We start visualizing the data in this section. Every graphs in this section are interactive. You can point to a data point in the graphs to see the detailed information of the data point. Further, you can click a label in the legend on the right to observe the trend of certain type or discipline. . Trends of groups . import altair as alt selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=tidy_data).mark_point(filled=True).encode( x=&quot;Year:N&quot;, y=&quot;median(Female proportion)&quot;, color=&quot;Type&quot;, tooltip=[&quot;Type&quot;, &quot;Year&quot;, &quot;median(Female proportion)&quot;] ).properties( width=700, height=300 ) line = point.mark_line() (point + line).encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . You can see an upward trend in STEM and Others disciplines from the 70s to the mid 80s and the percentage stayed unchanged after that period. In Liberal Arts, the female percentage has been arounnd 60% since 1970. Next, we delve into each disciplines in these groups. . STEM . stem = tidy_data[tidy_data.Type == &quot;STEM&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=stem).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in STEM major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . Most female percentages went upward from 1970 to 2011 except Computer Science and Math and Statistics. The female percentage of Engineer was extremely low (0.8%) in 1970s and reached about 18% in the 2000s. Only 45% of students of Psychology were female in the 1970s. This percentage grew by 30% to 76% in 2011. Throught these years, the percentage in Math and Statistics has remained stable (around 45%). In Computer Science, the trend went upward first and went downward latter. There is a peak of 35% in the 80s and in other period the precentage was around 18% to 25%. . Liberal Arts . liberal = tidy_data[tidy_data.Type == &quot;Liberal Arts&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=liberal).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in Liberal Arts major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . In this group, most precebtages of female students are higher than the ones in STEM. Most trend are stable in this group. However, unlike most disciplines, the female percentage of Communication and Journalism increased significantly from 35% to 62% and the one of Social Science and History increased about 10%. . Others . other = tidy_data[tidy_data.Type == &quot;Others&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=other).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in other majors&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . There are two types of trends in this group. Education, Health Professions and Publc Administration have much higher percentage of female stduent (Education:80%, Health Professions:85%, Publc Administration:75%). The percentage of the other three disciplines are all very low in the 70s and they all drastically increased to about 50% throuth these years. . In summary, until 2011, much more girls got into the disciplines that is normally for boys in the 1970s. For instance, only 4.2% of Agriculture students are female whereas the percentage reached 50% in 2011. Also, we can observe that some diciplines have imbalanced gender proportions. For example, although the precentage of female student in Engineer grew significantly, only 17% of students in Engineer is female. In contrast, 85% of Health Professions strudents are female. .",
            "url": "https://peiyihung.github.io/mywebsite/project/data%20visualization/python/2020/12/07/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "relUrl": "/project/data%20visualization/python/2020/12/07/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "What's the difference between a metric and a loss?",
            "content": "In machine learning, we usually use two values to evaluate our model: a metric and a loss. For instance, if we are doing a binary classification task, our metric may be the accuracy and our loss would be the cross-entroy. They both show how good our model performs. However, why do we need both rather than just use one of them? Also, what&#39;s the difference between them? . The short answer is that the metric is for human while the loss is for your model. . Based on the metric, machine learning practitioners such as data scientists and researchers assess a machine learning model. On the assessment, ML practitioners make decisions to address their problems or achieve their business goals. For example , say a data scientist aims to build a spam classifier to distinguish normal email from spam with 95% accuracy. First, the data scientist build a model with 90% accuracy. Apparently, this result doesn&#39;t meet his business goal, so he tries to build a better one. After implementing some techniques, he might get a classifier with 97% accuracy, which goes beyond his goal. Since the goal is met, the data scientist decides to integrate this model into his data product. ML partitioners use the metric to tell whether their model is good enough. . On the other hand, a loss indicates in what direction your model should improve. The difference between machine learning and traditional programming is how they get the ability to solve a problem. Traditional programs solve problems by following exact instructions given by programmers. In contrast, machine learning models learn how to solve a problem by taking into some examples (data) and discovering the underlying patterns of the problem. How does a machine learning model learn? Most ML models learn using a gradient-based method. Here&#39;s how a gradient-based method (be specifically, a gradient descent method in supervised learning context) works: . A model takes into data and makes predictions. | Compute the loss based on the predictions and the true data. | Compute the gradients of the loss with respect to parameters of the model. | Updating these parameters based on these gradients. | The gradient of the loss helps our model to get better and better. The reason why we need a loss is that a loss is sensitive enough to small changes so our model can improve based on it. More precisely, the gradient of the loss should vary if our parameters change slightly. In our spam classification example, accuracy is obviously not suitable for being a loss since it only changes when some examples are classified differently. The cross-entrpy is relatively smoother and so it is a good candidate for a loss. However, a metric do not have to be different from a loss. A metric can be a loss as long as it is sensitive enough. For instance, in a regression setting, MSE (mean squared error) can be both a metric and a loss. . In summary, a metric helps ML partitioners to evaluate their models and a loss facilitates the learning process of a ML model. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "relUrl": "/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
  
    
        ,"post13": {
            "title": "A Spam Classifier",
            "content": "Introduction . In this project, I built a spam classifer by implementing machine learning models. Models were trained by the datasets from Apache SpamAssassin website. . Get the data . Download emails and load them into my program . import os import urllib import tarfile import urllib.request download_root = &quot;https://spamassassin.apache.org/old/publiccorpus/&quot; file_names = [&quot;20030228_easy_ham.tar.bz2&quot;, &quot;20030228_easy_ham_2.tar.bz2&quot;, &quot;20030228_hard_ham.tar.bz2&quot;, &quot;20030228_spam.tar.bz2&quot;, &quot;20030228_spam_2.tar.bz2&quot;] store_path = os.path.join(&quot;data&quot;) def fetch_data(root_url=download_root, file_names=file_names, store_path=store_path): # make directory storing emails os.makedirs(store_path, exist_ok=True) # download files for file in file_names: file_url = os.path.join(download_root, file) path = os.path.join(store_path, file) urllib.request.urlretrieve(file_url, path) # extract emails for file in file_names: path = os.path.join(store_path, file) with tarfile.open(path, &#39;r&#39;) as f: f.extractall(path=store_path) #fetch_data() # get file names of emails email_folders = [&quot;hard_ham&quot;, &quot;easy_ham&quot;, &quot;easy_ham_2&quot;, &quot;spam&quot;, &quot;spam_2&quot;] ham_names = {} for ham in email_folders[:3]: ham_path = os.path.join(store_path, ham) names = [name for name in sorted(os.listdir(ham_path)) if len(name) &gt; 20] ham_names[ham] = names spam_names = {} for spam in email_folders[3:]: spam_path = os.path.join(store_path, spam) names = [name for name in sorted(os.listdir(spam_path)) if len(name) &gt; 20] spam_names[spam] = names # parse emails import email import email.policy def load_email(directory, filename, spam_path=store_path): path = os.path.join(spam_path, directory) with open(os.path.join(path, filename), &quot;rb&quot;) as f: return email.parser.BytesParser(policy=email.policy.default).parse(f) hams = [] for ham in email_folders[:3]: emails = [load_email(ham, filename=name) for name in ham_names[ham]] hams.extend(emails) spams = [] for spam in email_folders[3:]: emails = [load_email(spam, filename=name) for name in spam_names[spam]] spams.extend(emails) . . explain how to download the emails and load them in my notebook . len(hams), len(spams), len(spams) / (len(hams) + len(spams)) . (4150, 1897, 0.31370927732760046) . Accuracy of random guess is 70%, so we must do better than that. . Take a look at the emails . headers . hams[1].items() . [(&#39;Return-Path&#39;, &#39;&lt;malcolm-sweeps@mrichi.com&gt;&#39;), (&#39;Delivered-To&#39;, &#39;rod@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16821 invoked by uid 505); 7 May 2002 14:37:01 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 500 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.260914 secs); 07 May 2002 14:37:01 -0000&#39;), (&#39;Delivered-To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16811 invoked by uid 505); 7 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 502 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.250416 secs); 07 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from bocelli.siteprotect.com (64.41.120.21) by h0090272a42db.ne.client2.attbi.com with SMTP; 7 May 2002 14:36:59 -0000&#39;), (&#39;Received&#39;, &#39;from mail.mrichi.com ([208.33.95.187]) tby bocelli.siteprotect.com (8.9.3/8.9.3) with SMTP id JAA14328; tTue, 7 May 2002 09:37:01 -0500&#39;), (&#39;From&#39;, &#39;malcolm-sweeps@mrichi.com&#39;), (&#39;Message-Id&#39;, &#39;&lt;200205071437.JAA14328@bocelli.siteprotect.com&gt;&#39;), (&#39;To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Subject&#39;, &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39;), (&#39;Date&#39;, &#39;Tue, 07 May 2002 09:38:27 -0600&#39;), (&#39;X-Mailer&#39;, &#39;sendEmail-v1.33&#39;)] . hams[1][&quot;Subject&quot;] . &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39; . Contents . print(hams[1].get_content()[:600]) . May 7, 2002 Dear rod-3ds@arsecandle.org: Congratulations! On behalf of Frito-Lay, Inc., we are pleased to advise you that you&#39;ve won Fourth Prize in the 3D&#39;s(R) Malcolm in the Middle(TM) Sweepstakes. Fourth Prize consists of 1 manufacturer&#39;s coupon redeemable at participating retailers for 1 free bag of 3D&#39;s(R) brand snacks (up to 7 oz. size), with an approximate retail value of $2.59 and an expiration date of 12/31/02. Follow these instructions to claim your prize: 1. Print out this email message. 2. Complete ALL of the information requested. Print clearly and legibly. Sign . Get email structure . There are some emails that have multiple parts. . from collections import Counter def get_email_structure(email): if isinstance(email, str): return email payload = email.get_payload() if isinstance(payload, list): return &quot;multipart({})&quot;.format(&quot;, &quot;.join([ get_email_structure(sub_email) for sub_email in payload ])) else: return email.get_content_type() def structure_counter(emails): structures = [get_email_structure(email) for email in emails] return Counter(structures) . structure_counter(hams).most_common() . [(&#39;text/plain&#39;, 3832), (&#39;text/html&#39;, 120), (&#39;multipart(text/plain, application/pgp-signature)&#39;, 101), (&#39;multipart(text/plain, text/html)&#39;, 63), (&#39;multipart(text/plain, text/plain)&#39;, 5), (&#39;multipart(text/plain)&#39;, 3), (&#39;multipart(text/plain, application/x-pkcs7-signature)&#39;, 2), (&#39;multipart(text/html)&#39;, 2), (&#39;multipart(text/plain, application/ms-tnef, text/plain)&#39;, 2), (&#39;multipart(text/plain, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)&#39;, 2), (&#39;multipart(text/plain, image/bmp)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html))&#39;, 1), (&#39;multipart(text/plain, image/png, image/png)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, text/enriched)&#39;, 1), (&#39;multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(text/plain, video/mng)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))&#39;, 1), (&#39;multipart(text/plain, application/x-java-applet)&#39;, 1), (&#39;multipart(text/plain, application/x-patch)&#39;, 1), (&#39;multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, application/ms-tnef)&#39;, 1), (&#39;multipart(text/plain, text/plain, text/plain)&#39;, 1)] . structure_counter(spams).most_common() . [(&#39;text/plain&#39;, 816), (&#39;text/html&#39;, 772), (&#39;multipart(text/plain, text/html)&#39;, 159), (&#39;multipart(text/html)&#39;, 49), (&#39;multipart(text/plain)&#39;, 44), (&#39;multipart(multipart(text/html))&#39;, 23), (&#39;multipart(multipart(text/plain, text/html))&#39;, 5), (&#39;multipart(text/plain, application/octet-stream)&#39;, 3), (&#39;multipart(text/html, text/plain)&#39;, 3), (&#39;multipart(text/plain, image/jpeg)&#39;, 3), (&#39;multipart(text/plain, application/octet-stream, text/plain)&#39;, 3), (&#39;multipart(text/html, application/octet-stream)&#39;, 2), (&#39;multipart/alternative&#39;, 2), (&#39;multipart(text/html, image/jpeg)&#39;, 2), (&#39;multipart(multipart(text/plain), application/octet-stream)&#39;, 2), (&#39;multipart(multipart(text/html), application/octet-stream, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)&#39;, 1), (&#39;text/plain charset=us-ascii&#39;, 1), (&#39;multipart(multipart(text/html), image/gif)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/jpeg)&#39;, 1)] . It seems that most hams are plain text, while spams are more often html. What we need to do next? . Preprocessing emails . write helper funtions and make pipeline . Split emails into train and test set . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split X = np.array(hams+spams) y = np.array([0] * len(hams) + [1] * len(spams)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, stratify=y) X_train.shape, X_test.shape . ((4837,), (1210,)) . Email to text . Parse HTML . from bs4 import BeautifulSoup def html_to_plain_text(html): soup = BeautifulSoup(html, &quot;lxml&quot;) strings = &quot;&quot; for i in soup.find_all(): if i.string: strings += i.string + &quot; n&quot; return strings . Turn email to plain text . def email_to_text(email): html = None for part in email.walk(): ctype = part.get_content_type() if not ctype in (&quot;text/plain&quot;, &quot;text/html&quot;): continue try: content = part.get_content() except: # in case of encoding issues content = str(part.get_payload()) if ctype == &quot;text/plain&quot;: return content else: html = content if html: return html_to_plain_text(html) . example_spam = email_to_text(spams[10]) print(example_spam) . Cellular Phone Accessories All At Below Wholesale Prices! http://202.101.163.34:81/sites/merchant/sales/ Hands Free Ear Buds 1.99! Phone Holsters 1.98! Booster Antennas Only $0.99 Phone Cases 1.98! Car Chargers 1.98! Face Plates As Low As 0.99! Lithium Ion Batteries As Low As 6.94! http://202.101.163.34:81/sites/merchant/sales/ Click Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, SAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below WHOLESALE PRICES! http://202.101.163.34:81/sites/merchant/sales/ ***If You Need Assistance Please Call Us (732) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be removed from future mailings please send your remove request to: removemenow68994@btamail.net.cn Thank You and have a super day :) . Replace url with &quot;URL&quot; . import re url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; example_spam = re.sub(url_pattern, &quot;URL&quot;, example_spam) example_spam . &#39;Cellular Phone Accessories All At Below Wholesale Prices! n nURL n nHands Free Ear Buds 1.99! nPhone Holsters 1.98! nBooster Antennas Only $0.99 nPhone Cases 1.98! nCar Chargers 1.98! nFace Plates As Low As 0.99! nLithium Ion Batteries As Low As 6.94! n nURL n nClick Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, nSAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below nWHOLESALE PRICES! n nURL n n***If You Need Assistance Please Call Us (732) 751-1457*** n n n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nTo be removed from future mailings please send your remove nrequest to: removemenow68994@btamail.net.cn nThank You and have a super day :) n n&#39; . Tokenize . import nltk from nltk.tokenize import word_tokenize nltk.download(&#39;punkt&#39;) example_spam_tokenized = word_tokenize(example_spam) example_spam_tokenized[:10] . [nltk_data] Downloading package punkt to /Users/hongpeiyi/nltk_data... [nltk_data] Package punkt is already up-to-date! . [&#39;Cellular&#39;, &#39;Phone&#39;, &#39;Accessories&#39;, &#39;All&#39;, &#39;At&#39;, &#39;Below&#39;, &#39;Wholesale&#39;, &#39;Prices&#39;, &#39;!&#39;, &#39;URL&#39;] . Stemming . def stemming_email(tokenized_email): stemmer = nltk.PorterStemmer() stemmed_words = [stemmer.stem(word) for word in tokenized_email] return &quot; &quot;.join(stemmed_words) stemmed_eamil = stemming_email(example_spam_tokenized) stemmed_eamil . &#39;cellular phone accessori all At below wholesal price ! url hand free ear bud 1.99 ! phone holster 1.98 ! booster antenna onli $ 0.99 phone case 1.98 ! car charger 1.98 ! face plate As low As 0.99 ! lithium ion batteri As low As 6.94 ! url click below for accessori On all nokia , motorola LG , nextel , samsung , qualcomm , ericsson , audiovox phone At below wholesal price ! url ***if you need assist pleas call Us ( 732 ) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be remov from futur mail pleas send your remov request to : removemenow68994 @ btamail.net.cn thank you and have a super day : )&#39; . Write a sklearn estimator to transform our email . from sklearn.base import BaseEstimator, TransformerMixin class EmailToTokenizedStemmed(BaseEstimator, TransformerMixin): def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True): self.strip_headers = strip_headers self.lower_case = lower_case self.remove_punctuation = remove_punctuation self.replace_urls = replace_urls self.replace_numbers = replace_numbers self.stemming = stemming def fit(self, X, y=None): return self def transform(self, X, y=None): X_transformed = [] for email in X: text = email_to_text(email) or &quot;&quot; if self.lower_case: text = text.lower() if self.replace_urls: url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; text = re.sub(url_pattern, &quot;URL&quot;, text) if self.replace_numbers: text = re.sub(r&#39; d+(?: . d*(?:[eE] d+))?&#39;, &#39;NUMBER&#39;, text) if self.remove_punctuation: text = re.sub(r&#39;[^a-zA-Z0-9]+&#39;, &#39; &#39;, text, flags=re.M) text = word_tokenize(text) text = stemming_email(text) X_transformed.append(text) return np.array(X_transformed) . Vectorizing . from sklearn.feature_extraction.text import TfidfVectorizer . Make Pipeline . from sklearn.pipeline import Pipeline email_pipeline = Pipeline([ (&quot;Tokenizing and Stemming&quot;, EmailToTokenizedStemmed()), (&quot;tf-idf Vectorizing&quot;, TfidfVectorizer()), (&quot;passthrough&quot;, None) ]) . The processed datasets . X_train_processed = email_pipeline.fit_transform(X_train) X_test_processed = email_pipeline.transform(X_test) . . Modeling . from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV from sklearn.metrics import confusion_matrix, classification_report from sklearn.naive_bayes import MultinomialNB from sklearn.linear_model import LogisticRegressionCV from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier # plotting import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # others from scipy.stats import uniform, randint, loguniform import joblib # storing models . Functions from evaluating and comparing models . models = {} # storing trained models models_names = [] # storing models names # add models and its name to dict def add_model(name, model, models_list=models, name_list=models_names): name_list.append(name) models_list[name] = model . def get_classification_report(model, X_test=X_test_processed, y_test=y_test): y_pred = model.predict(X_test) print(classification_report(y_test, y_pred, target_names=[&quot;not spam&quot;, &quot;spam&quot;], digits=4)) . Building models and tuning them . how I trained and tuned the models? what&#39;s the process? . Naive Bayes (baseline model) . nb = MultinomialNB().fit(X_train_processed, y_train) . add_model(&quot;Naive Bayes&quot;, nb) . Logistic regression . logitCV = LogisticRegressionCV(max_iter=1000, Cs=20, cv=10, scoring=&quot;accuracy&quot;) logitCV.fit(X_train_processed, y_train) . LogisticRegressionCV(Cs=20, class_weight=None, cv=10, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=1000, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, refit=True, scoring=&#39;accuracy&#39;, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0) . add_model(&quot;Logistic regression&quot;, logitCV) . SVM . svc = SVC() svc_params = {&#39;C&#39;: loguniform(1e0, 1e3), &#39;gamma&#39;: loguniform(1e-4, 1e-3), &#39;kernel&#39;: [&#39;rbf&#39;], &#39;class_weight&#39;:[&#39;balanced&#39;, None]} svc_grid = RandomizedSearchCV(svc, svc_params, n_jobs=-1, cv=10, n_iter=15, scoring=&quot;accuracy&quot;) svc_grid.fit(X_train_processed, y_train) svc_best = svc_grid.best_estimator_ #svc = joblib.load(&quot;tmp/svc.pkl&quot;) . svc.get_params() . {&#39;C&#39;: 280.3887191550727, &#39;break_ties&#39;: False, &#39;cache_size&#39;: 200, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;coef0&#39;: 0.0, &#39;decision_function_shape&#39;: &#39;ovr&#39;, &#39;degree&#39;: 3, &#39;gamma&#39;: 0.000984422644629166, &#39;kernel&#39;: &#39;rbf&#39;, &#39;max_iter&#39;: -1, &#39;probability&#39;: False, &#39;random_state&#39;: None, &#39;shrinking&#39;: True, &#39;tol&#39;: 0.001, &#39;verbose&#39;: False} . add_model(&quot;SVM&quot;, svc) . Random Forest . max_depths = [10, 50, 100, 150] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1500, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 10, oob accuracy: 0.8594 Max Depth: 50, oob accuracy: 0.9692 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 150, oob accuracy: 0.9694 . max_depths = [90, 100, 110, 120, 130] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 90, oob accuracy: 0.9700 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 110, oob accuracy: 0.9704 Max Depth: 120, oob accuracy: 0.9708 Max Depth: 130, oob accuracy: 0.9698 . rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=100) rf.fit(X_train_processed, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=100, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1, oob_score=True, random_state=44, verbose=0, warm_start=False) . add_model(&quot;Random forest&quot;, rf) . Evaluate on test set . for name in models_names: print(name) get_classification_report(models[name]) print(&quot;--&quot;) print() . Naive Bayes precision recall f1-score support not spam 0.8494 0.9988 0.9181 830 spam 0.9957 0.6132 0.7590 380 accuracy 0.8777 1210 macro avg 0.9226 0.8060 0.8385 1210 weighted avg 0.8953 0.8777 0.8681 1210 -- Logistic regression precision recall f1-score support not spam 0.9927 0.9880 0.9903 830 spam 0.9740 0.9842 0.9791 380 accuracy 0.9868 1210 macro avg 0.9833 0.9861 0.9847 1210 weighted avg 0.9868 0.9868 0.9868 1210 -- SVM precision recall f1-score support not spam 0.9891 0.9880 0.9885 830 spam 0.9738 0.9763 0.9750 380 accuracy 0.9843 1210 macro avg 0.9814 0.9821 0.9818 1210 weighted avg 0.9843 0.9843 0.9843 1210 -- Random forest precision recall f1-score support not spam 0.9776 0.9976 0.9875 830 spam 0.9945 0.9500 0.9717 380 accuracy 0.9826 1210 macro avg 0.9860 0.9738 0.9796 1210 weighted avg 0.9829 0.9826 0.9825 1210 -- . Comparing performance of models using ROC curve and AUC . from sklearn.metrics import roc_curve, roc_auc_score def plot_roc_curve(models_names=models_names, models=models): plt.figure(dpi=120) for name in models_names: if name == &quot;SVM&quot;: y_score = models[name].decision_function(X_test_processed) fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) else: y_score = models[name].predict_proba(X_test_processed)[:,1] fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) plt.plot([0, 1], [0,1], &quot;b--&quot;) plt.xlim(-0.01, 1.02) plt.ylim(-0.01, 1.02) plt.legend(title=&quot;Model (AUC score)&quot;,loc=(1.01, 0.4)) . plot_roc_curve() . Conclusion .",
            "url": "https://peiyihung.github.io/mywebsite/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "relUrl": "/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "My name is Peiyi Hung (洪培翊). I am an aspiring data scientist finding my first data science job. I strongly believe that we can better understand our world and solve a wide ragne of problems by making good use of data. To effectively utilize data, I am eager to learn all the skills related to data such as Statistics, Machine Learning and Programming. . I earned a bachelor’s degree and a master’s degree both in Econimics from National Chengchi University (Taipei, Taiwan) and National Tsing Hua University (Hsinchu, Taiwan) respectively. .",
          "url": "https://peiyihung.github.io/mywebsite/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Learning",
          "content": "My Learning Notes . Experiments with activation functions and weights initialization . Investigate the distributions of activations throught layers with different activation functions and weights initialization . Apr 2, 2021 . | Why do we need nonlinear activation functions in a neural network ? . Explaining why we need nonlinear activation fucntions and illutrating it with python code . Dec 16, 2020 . | What&#39;s the difference between a metric and a loss? . . Dec 6, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/learning/",
          "relUrl": "/learning/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "My Projects . Recognize insincere questions on Quora with BERT . To tell if a question is sincere or not with BERT and fastai . May 22, 2021 . | Identifying dog breeds using multiple CNNs . Concatnate multiple CNNs as feature extractors and reach an accuracy of 92.5% . May 12, 2021 . | Classifiying Fish Spieces with AlexNet . Build AlexNet with Pytorch from scratch and use it to recognize fish spieces . Mar 15, 2021 . | Building a Language Model for Chinese Numbers using LSTM and GRU . Make a dataset of Chinese numbers and train language models for it with LSTM and GRU which are built from scratch. . Feb 21, 2021 . | Find cat&#39;s mouth with fastai . Using deep learning find cat&#39;s mouth in a picture . Jan 12, 2021 . | Applying Transfer Learning to CIFAR10 . Tackling CIFAR10 with a pretrained Resnet50 . Jan 5, 2021 . | Predicting Prices of Machine Learning Books . Getting prices of ML books by web scraping and using ML methods to predict them . Dec 25, 2020 . | Find quality wine with machine learning algorithms . Playing with the popular wine dataset . Dec 20, 2020 . | Interactive Visualization of Female Share of Bachelor’s degrees by Majors with Altair . Building simple interactive graphs with Altair . Dec 7, 2020 . | Analyzing Sharing Bikes Usage Trends . A exploratory data analysis project answering questions about bike sharing. . Dec 2, 2020 . | A Spam Classifier . This project builds a spam classifier using Apache SpamAssassin&#39;s public datasets. . Sep 12, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://peiyihung.github.io/mywebsite/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}