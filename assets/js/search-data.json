{
  
    
        "post0": {
            "title": "Predicting prices of Machine Learning Books",
            "content": "from fastai.tabular.all import * import seaborn as sns plt.rcParams[&quot;figure.figsize&quot;] = (10, 8) plt.rcParams[&quot;figure.dpi&quot;] = 80 . books = pd.read_csv(&quot;books_cleaned_final.csv&quot;) . books.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1914 entries, 0 to 1913 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Name 1914 non-null object 1 Authors 1914 non-null object 2 Price 1914 non-null float64 3 Image-url 1914 non-null object 4 Rating 615 non-null float64 5 NumberOfPages 1902 non-null float64 6 Format 1914 non-null object 7 Publication date 1914 non-null object 8 Publisher 1914 non-null object 9 Language 1907 non-null object 10 ISBN10 1798 non-null object 11 ISBN13 1914 non-null int64 12 Publication City 982 non-null object 13 Publication Country 982 non-null object 14 Width 1696 non-null float64 15 Height 1696 non-null float64 16 Thickness 1696 non-null float64 17 Weight 1696 non-null float64 dtypes: float64(7), int64(1), object(10) memory usage: 269.3+ KB . books.drop([&quot;Image-url&quot;, &quot;Rating&quot;, &quot;ISBN10&quot;, &quot;ISBN13&quot;], axis=1, inplace=True) . books = add_datepart(books, &quot;Publication date&quot;) . books.columns . Index([&#39;Name&#39;, &#39;Authors&#39;, &#39;Price&#39;, &#39;Publication Week&#39;, &#39;NumberOfPages&#39;, &#39;Format&#39;, &#39;Publisher&#39;, &#39;Language&#39;, &#39;Publication City&#39;, &#39;Publication Country&#39;, &#39;Width&#39;, &#39;Height&#39;, &#39;Thickness&#39;, &#39;Weight&#39;, &#39;Publication Year&#39;, &#39;Publication Month&#39;, &#39;Publication Day&#39;, &#39;Publication Dayofweek&#39;, &#39;Publication Dayofyear&#39;, &#39;Publication Is_month_end&#39;, &#39;Publication Is_month_start&#39;, &#39;Publication Is_quarter_end&#39;, &#39;Publication Is_quarter_start&#39;, &#39;Publication Is_year_end&#39;, &#39;Publication Is_year_start&#39;, &#39;Publication Elapsed&#39;], dtype=&#39;object&#39;) . Exploratory data analysis . Investigating the target variable - Price . books[&quot;Price&quot;].describe() . count 1914.000000 mean 1838.880878 std 2061.328347 min 236.000000 25% 697.000000 50% 1224.500000 75% 2514.000000 max 42667.000000 Name: Price, dtype: float64 . plt.figure(figsize=(10, 4)) sns.histplot(books, x=&quot;Price&quot;); . plt.figure(figsize=(10, 4)) sns.histplot(books, x=&quot;Price&quot;, log_scale=True); . books.loc[:, &quot;Price&quot;] = np.log(books[&quot;Price&quot;]) . sns.heatmap(books.corr(), square=True, cmap=sns.diverging_palette(20, 220, as_cmap=True)); . num_var = [&quot;Price&quot;, &quot;NumberOfPages&quot;, &quot;Width&quot;, &quot;Height&quot;, &quot;Thickness&quot;, &quot;Weight&quot;] sns.heatmap(books.loc[:,num_var].corr(), square=True, annot=True, cmap=sns.diverging_palette(20, 220, as_cmap=True)); . Format . books[&quot;Format&quot;].value_counts() . Paperback 1422 Hardback 492 Name: Format, dtype: int64 . fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5)) sns.histplot(data=books, x=&quot;Price&quot;, hue=&quot;Format&quot;, ax=ax[0]) sns.boxplot(data=books, x=&quot;Format&quot;, y=&quot;Price&quot;, ax=ax[1]); . Publisher . books[&quot;Publisher&quot;].value_counts()[:10] . Springer 398 Independently Published 342 Packt Publishing Limited 153 Createspace Independent Publishing Platform 94 LAP Lambert Academic Publishing 76 Taylor &amp; Francis 64 aPress 57 Wiley 56 Charlie Creative Lab 31 O&#39;Reilly Media, Inc, USA 28 Name: Publisher, dtype: int64 . pname = books[&quot;Publisher&quot;].value_counts()[:10].index plt.figure(figsize=(20, 20)) sns.boxplot(data=books, y=&#39;Publisher&#39;, x=&quot;Price&quot;, order=pname) plt.setp(plt.gca().get_yticklabels(), fontsize=20) plt.ylabel(&quot;&quot;) plt.xlabel(&quot;Price&quot;, fontsize=20); . Language . books[&quot;Language&quot;].value_counts() . English 1869 German 17 Spanish 16 Portuguese 2 French 2 English, German 1 Name: Language, dtype: int64 . books[books[&quot;Language&quot;] == &quot;English, German&quot;] . Name Authors Price Publication Week NumberOfPages Format Publisher Language Publication City Publication Country ... Publication Day Publication Dayofweek Publication Dayofyear Publication Is_month_end Publication Is_month_start Publication Is_quarter_end Publication Is_quarter_start Publication Is_year_end Publication Is_year_start Publication Elapsed . 1296 Susanne Huth - Analog Algorithm - Landscapes of Machine Learning | Maren Lubbke-tidow, Susanne Huth | 6.767343 | 33 | 80.0 | Paperback | Fotohof | English, German | Salzburg | Austria | ... | 13 | 3 | 226 | False | False | False | False | False | False | 1597276800 | . 1 rows × 26 columns . books.loc[1296, &quot;Language&quot;] = &quot;German&quot; . books[&quot;Language&quot;].value_counts() . English 1869 German 18 Spanish 16 Portuguese 2 French 2 Name: Language, dtype: int64 . Publication Country . books[&quot;Publication Country&quot;].value_counts() . United States 337 United Kingdom 259 Switzerland 218 Germany 83 Singapore 59 Netherlands 16 Canada 5 India 4 Austria 1 Name: Publication Country, dtype: int64 . sns.boxplot(x=&quot;Price&quot;, y=&quot;Publication Country&quot;, data=books,); . Publication City . books[&quot;Publication City&quot;].unique() . array([&#39;New York, NY&#39;, &#39;Cambridge&#39;, &#39;New York&#39;, nan, &#39;Cham&#39;, &#39;Bosa Roca&#39;, &#39;Portland&#39;, &#39;London&#39;, &#39;OH&#39;, &#39;Washington&#39;, &#39;San Diego&#39;, &#39;New Jersey&#39;, &#39;Singapore&#39;, &#39;Bristol&#39;, &#39;Hoboken&#39;, &#39;Oxford&#39;, &#39;Bingley&#39;, &#39;Berlin&#39;, &#39;England&#39;, &#39;Hershey&#39;, &#39;Oakville&#39;, &#39;San Rafael&#39;, &#39;Dordrecht&#39;, &#39;Stevenage&#39;, &#39;Chicago, IL&#39;, &#39;Maryland&#39;, &#39;Harrisburg, PA&#39;, &#39;Norwood&#39;, &#39;Boca Raton, FL&#39;, &#39;Morrisville&#39;, &#39;Philadelphia&#39;, &#39;Lanham, MD&#39;, &#39;Annopolis&#39;, &#39;Pennsauken&#39;, &#39;Bern&#39;, &#39;Sebastopol&#39;, &#39;Birmingham&#39;, &#39;Boston&#39;, &#39;Shelter Island, N.Y.&#39;, &#39;San Francisco&#39;, &#39;Berkley&#39;, &#39;Farnham&#39;, &#39;Raleigh&#39;, &#39;Preston&#39;, &#39;Bradley Beach&#39;, &#39;Chichester, England&#39;, &#39;Hanover&#39;, &#39;New Delhi&#39;, &#39;Sim Valley, CA&#39;, &#39;Weisbaden&#39;, &#39;Saarbrucken&#39;, &#39;Santa Monica, CA&#39;, &#39;Salzburg&#39;, &#39;Rockland, MA&#39;, &#39;Houston&#39;, &#39;San Rafael, CA&#39;, &#39;[Bellevue, Washington]&#39;], dtype=object) . books[&quot;Publication City&quot;].replace(&quot;New York&quot;, &quot;New York, NY&quot;, inplace=True) . city_name = books[&quot;Publication City&quot;].value_counts()[:20].index city_name . Index([&#39;Cham&#39;, &#39;Birmingham&#39;, &#39;New York, NY&#39;, &#39;Berlin&#39;, &#39;Singapore&#39;, &#39;London&#39;, &#39;Berkley&#39;, &#39;Cambridge&#39;, &#39;Sebastopol&#39;, &#39;Hershey&#39;, &#39;San Diego&#39;, &#39;Dordrecht&#39;, &#39;Hoboken&#39;, &#39;San Rafael&#39;, &#39;Hanover&#39;, &#39;England&#39;, &#39;Bosa Roca&#39;, &#39;Boston&#39;, &#39;Portland&#39;, &#39;Saarbrucken&#39;], dtype=&#39;object&#39;) . plt.figure(figsize=(8, 10)) sns.boxplot(x=&quot;Price&quot;, y=&quot;Publication City&quot;, data=books, order=city_name); . . Predict prices . Preprocessing . Deal with missing values . median_impute_var = [&quot;NumberOfPages&quot;, &quot;Width&quot;, &quot;Height&quot;, &quot;Thickness&quot;, &quot;Weight&quot;] books.loc[:, median_impute_var] = books[median_impute_var].fillna(books[median_impute_var].median()) . books.loc[:, &quot;Language&quot;] = books[&quot;Language&quot;].fillna(&quot;English&quot;) . unknown_impute = [&quot;Publication City&quot;, &quot;Publication Country&quot;] books.loc[:, unknown_impute] = books.loc[:,unknown_impute].fillna(&quot;unknown&quot;) . books.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1914 entries, 0 to 1913 Data columns (total 26 columns): # Column Non-Null Count Dtype -- -- 0 Name 1914 non-null object 1 Authors 1914 non-null object 2 Price 1914 non-null float64 3 Publication Week 1914 non-null UInt32 4 NumberOfPages 1914 non-null float64 5 Format 1914 non-null object 6 Publisher 1914 non-null object 7 Language 1914 non-null object 8 Publication City 1914 non-null object 9 Publication Country 1914 non-null object 10 Width 1914 non-null float64 11 Height 1914 non-null float64 12 Thickness 1914 non-null float64 13 Weight 1914 non-null float64 14 Publication Year 1914 non-null int64 15 Publication Month 1914 non-null int64 16 Publication Day 1914 non-null int64 17 Publication Dayofweek 1914 non-null int64 18 Publication Dayofyear 1914 non-null int64 19 Publication Is_month_end 1914 non-null bool 20 Publication Is_month_start 1914 non-null bool 21 Publication Is_quarter_end 1914 non-null bool 22 Publication Is_quarter_start 1914 non-null bool 23 Publication Is_year_end 1914 non-null bool 24 Publication Is_year_start 1914 non-null bool 25 Publication Elapsed 1914 non-null object dtypes: UInt32(1), bool(6), float64(6), int64(5), object(8) memory usage: 304.8+ KB . Select target, split . books.drop([&quot;Name&quot;, &quot;Authors&quot;], axis=1, inplace=True) . proc = [Categorify, Normalize] . dep_var = &quot;Price&quot; . cont, cat = cont_cat_split(books, max_card=2,dep_var=dep_var) . train_size = int(len(books)*0.8) splits = RandomSplitter()(books.index) . to = TabularPandas(books, proc, cat, cont, y_names=dep_var, splits=splits) . xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y . . Modeling . from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer def r_mse(inp, tar):return np.sqrt(np.mean((inp-tar)**2)) my_scorer = make_scorer(r_mse, greater_is_better=False) def get_cv_error(model, xs=xs, y=y): cv_score = cross_val_score(model, X=xs, y=y, cv=5, n_jobs=-1, scoring=my_scorer) * -1 print(f&quot;Mean CV RMSE: {cv_score.mean():.4f}, Std: {cv_score.std():.4f}&quot;) def get_test_error(model, valid_xs=valid_xs, valid_y=valid_y): return r_mse(model.predict(valid_xs), valid_y) . Baseline - predicting with mean price . r_mse(y.mean(), valid_y) . 0.755951277002931 . Linear Regression . from sklearn.linear_model import LinearRegression linear_reg = LinearRegression().fit(xs, y) get_cv_error(linear_reg) . Mean CV RMSE: 0.4968, Std: 0.0206 . get_test_error(linear_reg) . 0.44991372904651133 . Random Forest . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(oob_score=True) get_cv_error(rf) . Mean CV RMSE: 0.3675, Std: 0.0077 . rf = RandomForestRegressor(n_jobs=-1).fit(xs, y) def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf, xs) def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . to_keep = fi[fi.imp&gt;0.002].cols . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] rf = RandomForestRegressor() get_cv_error(rf, xs=xs_imp, y=y) . Mean CV RMSE: 0.3673, Std: 0.0083 . Tuning Random Forest . from sklearn.model_selection import RandomizedSearchCV, GridSearchCV . rf = RandomForestRegressor(n_jobs=-1) params_range = {&#39;n_estimators&#39;:np.arange(100, 1000), &#39;max_depth&#39;:np.arange(5, 100), &#39;min_samples_leaf&#39;:np.arange(1, 50), &#39;max_features&#39;:[0.3, 0.5, 0.7]} rscv = RandomizedSearchCV(rf, params_range, scoring=my_scorer, n_jobs=-1, cv=5, n_iter=50).fit(xs_imp, y) best_rf = rscv.best_estimator_ rscv.best_score_ * -1 . 0.3568825051364775 . get_test_error(rscv.best_estimator_, valid_xs=valid_xs_imp) . 0.32530136993991754 . Neural Network . dls = to.dataloaders(16) . y = to.train.y y.min(), y.max() . (5.463831901550293, 10.661181449890137) . learn = tabular_learner(dls, y_range=(5,11), layers=[100, 50], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=6.309573450380412e-07) . learn.fit_one_cycle(5, lr_max=5e-2) . epoch train_loss valid_loss time . 0 | 0.306698 | 0.350308 | 00:01 | . 1 | 0.243518 | 0.171640 | 00:01 | . 2 | 0.170460 | 0.136690 | 00:01 | . 3 | 0.126323 | 0.122816 | 00:01 | . 4 | 0.087670 | 0.121768 | 00:01 | . preds, targs = learn.get_preds() . (preds - targs).pow(2).mean().pow(1/2) . tensor(0.3490) .",
            "url": "https://peiyihung.github.io/mywebsite/category/project/2020/12/25/Predicting-prices-of-Machine-Learning-Books.html",
            "relUrl": "/category/project/2020/12/25/Predicting-prices-of-Machine-Learning-Books.html",
            "date": " • Dec 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Find quality wine with machine learning algorithms",
            "content": "Introduction . In this project, I will build machine learning models to find high quality wines (with quaity greater than and equal to 7). The data is obtained from the UCI machine learning repository (here) oringinating from this paper. . The analysis proceeds in this order: . preliminary data processing: import the data and merge the datasets | data exploring | preprocessing before modeling | determine the evaluation metric | train and tune models | . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns plt.rcParams[&quot;figure.dpi&quot;] = 100 import warnings warnings.filterwarnings(&#39;ignore&#39;) . Preliminary Data processing . Import Data . red = pd.read_csv(&quot;data/winequality-red.csv&quot;, sep=&quot;;&quot;) white = pd.read_csv(&quot;data/winequality-white.csv&quot;, sep=&quot;;&quot;) . red.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . 3 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17.0 | 60.0 | 0.9980 | 3.16 | 0.58 | 9.8 | 6 | . 4 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . white.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . The data contains: . ingredients: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, sulphates | properties: density, pH, alcohol, quality | . Next, we need to combine two datasets into one and add a &#39;type&#39; variable. . white[&quot;type&quot;] = &quot;white&quot; red[&quot;type&quot;] = &quot;red&quot; wine = pd.concat([white, red], axis=0) wine = wine.sample(frac=1).reset_index(drop=True) #shuffle wine.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality type . 0 7.1 | 0.25 | 0.28 | 1.6 | 0.052 | 46.0 | 169.0 | 0.99260 | 3.05 | 0.41 | 10.5 | 5 | white | . 1 5.5 | 0.29 | 0.30 | 1.1 | 0.022 | 20.0 | 110.0 | 0.98869 | 3.34 | 0.38 | 12.8 | 7 | white | . 2 7.7 | 0.12 | 0.32 | 1.4 | 0.060 | 47.0 | 150.0 | 0.99520 | 3.37 | 0.42 | 9.2 | 6 | white | . 3 6.4 | 0.42 | 0.46 | 8.4 | 0.050 | 58.0 | 180.0 | 0.99495 | 3.18 | 0.46 | 9.7 | 6 | white | . 4 8.5 | 0.28 | 0.34 | 13.8 | 0.041 | 32.0 | 161.0 | 0.99810 | 3.13 | 0.40 | 9.9 | 6 | white | . Explore the data . Information about the dataset: . wine.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 13 columns): # Column Non-Null Count Dtype -- -- 0 fixed acidity 6497 non-null float64 1 volatile acidity 6497 non-null float64 2 citric acid 6497 non-null float64 3 residual sugar 6497 non-null float64 4 chlorides 6497 non-null float64 5 free sulfur dioxide 6497 non-null float64 6 total sulfur dioxide 6497 non-null float64 7 density 6497 non-null float64 8 pH 6497 non-null float64 9 sulphates 6497 non-null float64 10 alcohol 6497 non-null float64 11 quality 6497 non-null int64 12 type 6497 non-null object dtypes: float64(11), int64(1), object(1) memory usage: 660.0+ KB . The dataset contains 13 variable, 11 numerical and 1 categorical. It has 6497 observations in it with no missing values. quality is our target variable and the others are our independent variables. . Quality . First, let&#39;s see the distribution of our target variable qualty. . wine_value_counts = wine.quality.value_counts().sort_index() # draw a bar chart wine_value_counts.plot(kind=&#39;barh&#39;) plt.ylabel(&quot;quality&quot;) plt.xlabel(&quot;counts&quot;); . (wine_value_counts.cumsum()/6497).round(2) . 3 0.00 4 0.04 5 0.37 6 0.80 7 0.97 8 1.00 9 1.00 Name: quality, dtype: float64 . We can know 80% of wine in our data set has quality lower than 6. Based on this information, we consider wine with quality higher than 6 as high quality wine. . Correlations . Next, we investigate which independent variable is correlated with our target quality. . plt.figure(figsize=(12, 8)) sns.heatmap(wine.corr(), cmap=&quot;Blues&quot;, annot=True, vmin=-1); . Three variables are correlated with &quot;quality&quot;: &quot;alcohol&quot;, &quot;density&quot;, and &quot;chlorides&quot;. Using boxplots, we can observe the distribution of these three variables between wines with different quality: . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;alcohol&quot;, x=&quot;quality&quot;, data=wine); . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;density&quot;, x=&quot;quality&quot;, data=wine) plt.ylim(0.98, 1.01); . plt.figure(figsize=(10, 5)) sns.boxplot(y=&quot;chlorides&quot;, x=&quot;quality&quot;, data=wine) plt.ylim(0, 0.2); . . Predictive Modeling - is high quality or not? . Now that we have a preliminary understanding of the data, we can start to build our models. However, the data set requires some modifications to be consumed by our machine learning models. . Preprocessing . Three modifications are conducted: . form independent variables and the target | standardizing and label encoding | split train and test sets | . form independent variables and the target . wine[&quot;isHigh&quot;] = wine.quality.apply(lambda x:1 if x &gt;= 7 else 0) X = wine.drop([&quot;quality&quot;, &quot;isHigh&quot;], axis=1) y = wine.isHigh.values . standardizing and label encoding . from sklearn.preprocessing import StandardScaler, LabelEncoder num_cols = X.columns[:-1] cat_cols = X.columns[-1] std_scaler = StandardScaler() X.loc[:, num_cols] = std_scaler.fit_transform(X[num_cols]) lbl_enc = LabelEncoder() X.loc[:, cat_cols] = lbl_enc.fit_transform(X[cat_cols]) . split train and test sets . wine.isHigh.value_counts()/6497 . 0 0.803448 1 0.196552 Name: isHigh, dtype: float64 . Since the target variable is unbalaned, we split our data set in a stratified manner. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) . Evaluation metric . Since we get an unbalanced dataset, f1 score is used as the evaluation metric. Here we write a function to evaluate our predicting result: . from sklearn.metrics import confusion_matrix, f1_score def print_metric(y_true, y_pred): f1 = f1_score(y_true, y_pred) confusion = pd.DataFrame(confusion_matrix(y_test, y_pred), index=[&quot;True_low&quot;, &quot;True_high&quot;], columns=[&quot;Pred_low&quot;, &quot;Pred_high&quot;]) print(f&quot;F1 score: {f1:.4f}&quot;) print() print(confusion) . Models and Hyperparameter tuning . Four models are considered: . baseline: use the mean of each class | logistic regression | random forest | xgboost | . Baseline method - use the mean of each class . This method determine if a wine is high quality by computing the distance between the mean values of high-quality wine and low-quality wine. . def compute_mean(X, y): high_mean = X[y == 1].mean(0) low_mean = X[y == 0].mean(0) return high_mean, low_mean # to compute the distance to high_mean and low_mean def wine_distance(x1, x2):return (x1-x2).abs().mean(1) # make predictions by the distance def is_high(x, X_train, y_train): high_mean, low_mean = compute_mean(X_train, y_train) return wine_distance(x, high_mean) &lt; wine_distance(x, low_mean) # make predictions y_pred = is_high(X_test, X_train, y_train) print_metric(y_test, y_pred) . F1 score: 0.4806 Pred_low Pred_high True_low 863 443 True_high 78 241 . Now, we start using machine learning models . Machine learning models . We use a hold-out test set to evaluate the final performance and use 5-fold cross-validation as tuning procedure. . Here are function implementing the stratified cv precedures: . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import StratifiedKFold, cross_val_score from sklearn.model_selection import RandomizedSearchCV strat_cv = StratifiedKFold(n_splits=5, shuffle=True) # a function to show cv result without tuning def get_cv_result(model): &quot;&quot;&quot;Take into a model and return cv f1 score&quot;&quot;&quot; cv_score = cross_val_score(model, X=X_train, y=y_train, scoring=&#39;f1&#39;, cv=strat_cv, n_jobs=-1) print(f&quot;Mean CV score: {cv_score.mean():.4f}&quot;) print(f&quot;Std: {cv_score.std():.4f}&quot;) # a function using random search to tuning the hyperparameters def tuning(model, params): randomCV = RandomizedSearchCV( estimator=model, param_distributions=params, cv=strat_cv, n_jobs=-1, scoring=&quot;f1&quot;, n_iter=50 ).fit(X_train, y_train) print(f&quot;Best f1 score: {randomCV.best_score_:.4f}&quot;) print(&quot;Best parameters:&quot;) for para, value in randomCV.best_params_.items(): print(f&quot; {para:10}: {value:.4f}&quot;) return randomCV.best_estimator_ . In the following subsection, we will first train the model without tuning and then tune the model so that we can see the improvement after tuning. . Logistic Regression . logit = LogisticRegression() get_cv_result(logit); . Mean CV score: 0.3594 Std: 0.0265 . logit_params = {&quot;C&quot;:np.linspace(0.001, 100, 200)} logit_tuned = tuning(logit, logit_params) . Best f1 score: 0.3558 Best parameters: C : 6.0311 . logit_tuned.fit(X_train, y_train) y_pred = logit_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.3684 Pred_low Pred_high True_low 1253 53 True_high 235 84 . Random Forest . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() get_cv_result(rf) . Mean CV score: 0.6275 Std: 0.0314 . rf_params = { &quot;n_estimators&quot;:np.arange(100, 1500), &quot;max_depth&quot;:np.arange(1, 50), &quot;max_features&quot;:[0.3, 0.5, 0.7], &quot;min_samples_leaf&quot;:np.arange(1, 30) } rf_tuned = tuning(rf, rf_params) . Best f1 score: 0.6316 Best parameters: n_estimators: 160.0000 min_samples_leaf: 1.0000 max_features: 0.3000 max_depth : 13.0000 . rf_tuned.fit(X_train, y_train) y_pred = rf_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.6415 Pred_low Pred_high True_low 1265 41 True_high 149 170 . XGBoost . import xgboost as xgb xgb_cls = xgb.XGBClassifier(eval_metric=&quot;logloss&quot;, use_label_encoder=False) get_cv_result(xgb_cls) . Mean CV score: 0.6506 Std: 0.0089 . xgb_params = { &quot;n_estimators&quot;:np.arange(1, 1500), &quot;max_depth&quot;:np.arange(1, 50), &quot;learning_rate&quot;:np.linspace(0.01, 0.1, 100), &quot;gamma&quot;:np.linspace(0.05, 1, 100), &quot;min_child_weight&quot;:[1,3,5,7], &quot;subsample&quot;:[0.6, 0.7, 0.8, 0.9, 1.0], &quot;colsample_bytree&quot;:[0.6, 0.7, 0.8, 0.9, 1.0], &quot;reg_lambda&quot;:np.linspace(0.01, 1, 100) } xgb_tuned = tuning(xgb_cls, xgb_params) . Best f1 score: 0.6647 Best parameters: subsample : 0.7000 reg_lambda: 0.6900 n_estimators: 1440.0000 min_child_weight: 3.0000 max_depth : 10.0000 learning_rate: 0.0373 gamma : 0.6066 colsample_bytree: 0.9000 . xgb_tuned.fit(X_train, y_train) y_pred = xgb_tuned.predict(X_test) print_metric(y_test, y_pred) . F1 score: 0.6667 Pred_low Pred_high True_low 1251 55 True_high 132 187 .",
            "url": "https://peiyihung.github.io/mywebsite/project/2020/12/20/Wine-types-and-quality.html",
            "relUrl": "/project/2020/12/20/Wine-types-and-quality.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Why do we need nonlinear activation functions in a neural network ?",
            "content": "Introduction . A neural network is composed of neurons. A neuron usually multiplies inputs values by weights, adds a bias to the product of weights and inputs, and passes the sum into a nonlinear activation function. In an equation form, the computation a neuron does is $$a = f( sum_{j=1}^{n}{w_jx_j} + b)$$ where $f$ is a nonlinear function such as tanh or ReLU, $w_j$ are weights, and $b$ is a bias. . The question I intended to answer in this article is why we need a nonlinear activation function. . We need a nonlinear function because our model would only learn linear patterns if we don&#39;t use one. I will illustrate this point by comparing two simple neural networks, one with nonlinear activation functions and the other without nonlinear functions. I will fit these two models to the data I generate from a nonlinear function and show you that the model without nonlinear functions can not capture a nonlinear pattern. . The illustration starts with generating data, proceeds with building and training models, and ends with comparing prediction performances of two models. Let&#39;s get started! . Generating data . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (10, 5) plt.style.use(&#39;fivethirtyeight&#39;) . . The data is generated from this equation: $$y = x^2 + 4x + 3 + epsilon, : epsilon sim N(0, 50)$$ where $y$ is a quadratic function adding a normally distributed error term $ epsilon$. We want our model to approximate this function. . Here&#39;s how the data looks like: . np.random.seed(5) x = np.linspace(-20, 20, 100) # independent variable y = x**2 + 4*x + 3 + np.random.randn(100)*50 # dependent variable plt.figure(dpi=100) plt.plot(x, y, &quot;.&quot;, label=&quot;Data points&quot;) plt.plot(x, 3 + x**2 + 4*x, &quot;-&quot;, label=&quot;True pattern&quot;) plt.legend(loc=&quot;upper center&quot;); . Build and Train Neural Networks . I contruct two neural networks with similar architecture. They are fully-connected networks with 4 hidden layers and 8 hidden units in each layer like this: . . The only difference is activation functions. One uses identity functions $f(x)=x$ and the other is equipped with ReLU functions $ReLU(x)=max(0, x)$. . The networks are contructed with PyTroch: . import torch from torch import nn from torch.utils.data import DataLoader # transfer numpy array to pytorch tennsor x = torch.tensor(x).unsqueeze(1).float() y = torch.tensor(y).unsqueeze(1).float() # form dataset and dataloader ds = [(xi, yi) for xi, yi in zip(x, y)] dl = DataLoader(ds, batch_size=20, shuffle=True) # the model without nonlinear function model1 = nn.Sequential( nn.Linear(1, 8), nn.Linear(8, 8), nn.Linear(8, 8), nn.Linear(8, 8), nn.Linear(8, 1) ) # the model with ReLU functions model2 = nn.Sequential( nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 1) ) . I take the mean squared error as loss function and use Adam as the optimization method. Here&#39;s how I train these two models: . from torch import optim import torch.nn.functional as F # set loss functin and optimizatino method loss_func = F.mse_loss opt1 = optim.Adam(model1.parameters()) opt2 = optim.Adam(model2.parameters()) # training lr = 5 for epoch in range(800): for xb, yb in dl: pred = model1(xb) loss = loss_func(pred, yb) loss.backward() opt1.step() opt1.zero_grad() for epoch in range(800): for xb, yb in dl: pred = model2(xb) loss = loss_func(pred, yb) loss.backward() opt2.step() opt2.zero_grad() . What&#39;s the difference between two models? . Now that these two model are trained we can see how they perform on capturing the nonlinear pattern. . plt.figure(figsize=(10, 5), dpi=120) plt.plot(x, y, &quot;k.&quot;) plt.plot(x, x**2 + 4*x + 3, label=&quot;True pattern&quot;) plt.plot(x, model1(x).detach().numpy(), label=&quot;Linear model&quot;) plt.plot(x, model2(x).detach().numpy(), label=&quot;ReLU model&quot;) plt.legend(fontsize=8); . . It is clear that the model with ReLU functions works better than the one without nonlinear functions. As suggested in the graph, model without nonlinear activation functions can only learn linear pattern. We will not encounter linear pattern every time when we analyze the data since the real world is so complex. Therefore, nonlinear activation functions are necessary for discovering the underlying pattern of the data. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/deep%20learning/machine%20learning/python/pytorch/2020/12/16/Why-do-we-need-nonlinear-activation-function.html",
            "relUrl": "/learning/deep%20learning/machine%20learning/python/pytorch/2020/12/16/Why-do-we-need-nonlinear-activation-function.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Interactive Visualization of Change in Female College Major Percentage with Altair",
            "content": "In this project, I make some simple interactive graphs to show annual changes in proportions of female college major in different disciplines. The data I used is obtained from The Department of Education Statistics. It contains the proportions of female college majors in 17 disciplines in each year from 1970 to 2011. To make these analysis more easily understood, I will split these disciplines into three groups: STEM, Liberal Arts and the others, and plot each disciplines in these three groups. . I use Python for this project. The programming tool I used for visualizing the data is Altair, a declarative statistical visualization library for Python. The GitHub repository of this project is here. . This project consists of three parts. First, we will have a preview of our data to gain a brief understanding og it. Secondly, we process the data to make it easier to be visualized. Lastly, we make interactive graphs to discovery information from the data. . A preview of the data . import pandas as pd women_degrees = pd.read_csv(&#39;percent-bachelors-degrees-women-usa.csv&#39;) women_degrees.iloc[:, :8].head() . Year Agriculture Architecture Art and Performance Biology Business Communications and Journalism Computer Science . 0 1970 | 4.229798 | 11.921005 | 59.7 | 29.088363 | 9.064439 | 35.3 | 13.6 | . 1 1971 | 5.452797 | 12.003106 | 59.9 | 29.394403 | 9.503187 | 35.5 | 13.6 | . 2 1972 | 7.420710 | 13.214594 | 60.4 | 29.810221 | 10.558962 | 36.6 | 14.9 | . 3 1973 | 9.653602 | 14.791613 | 60.2 | 31.147915 | 12.804602 | 38.4 | 16.4 | . 4 1974 | 14.074623 | 17.444688 | 61.9 | 32.996183 | 16.204850 | 40.5 | 18.9 | . We can clearly see that the data contains the percentage of female students in each disciplines. Each row represents a year, and columns indicate the diciplines. Here&#39;s a lists of disciplines: . women_degrees.columns . Index([&#39;Year&#39;, &#39;Agriculture&#39;, &#39;Architecture&#39;, &#39;Art and Performance&#39;, &#39;Biology&#39;, &#39;Business&#39;, &#39;Communications and Journalism&#39;, &#39;Computer Science&#39;, &#39;Education&#39;, &#39;Engineering&#39;, &#39;English&#39;, &#39;Foreign Languages&#39;, &#39;Health Professions&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Psychology&#39;, &#39;Public Administration&#39;, &#39;Social Sciences and History&#39;], dtype=&#39;object&#39;) . Preprocessing . In this part, we tidy the data and divide disciplines into three groups. . Tidying the data . In order to easily visualize the data, we transform our data to a tidy data. But what&#39;s a tidy data? By the definition from Hadley Wickham&#39;s paper, a tidy data is . Each variable forms a column. | Each observation forms a row. | Each type of observational unit forms a table | We can use melt method to do that. . tidy_data = women_degrees.melt(id_vars=&quot;Year&quot;, value_vars=women_degrees.columns[1:], var_name=&quot;Discipline&quot;, value_name=&quot;Female proportion&quot;) tidy_data.head(10) . Year Discipline Female proportion . 0 1970 | Agriculture | 4.229798 | . 1 1971 | Agriculture | 5.452797 | . 2 1972 | Agriculture | 7.420710 | . 3 1973 | Agriculture | 9.653602 | . 4 1974 | Agriculture | 14.074623 | . 5 1975 | Agriculture | 18.333162 | . 6 1976 | Agriculture | 22.252760 | . 7 1977 | Agriculture | 24.640177 | . 8 1978 | Agriculture | 27.146192 | . 9 1979 | Agriculture | 29.633365 | . Divide disciplines into groups . Since there are 18 disciplines in the data, it would be messy if we plot all disciplines at once in our graph. Consequently, we group dsciplines into three groups, STEM, Liberal Arts and the others, and add a type column to store group information. . stem_cats = [&#39;Psychology&#39;, &#39;Biology&#39;, &#39;Math and Statistics&#39;, &#39;Physical Sciences&#39;, &#39;Computer Science&#39;, &#39;Engineering&#39;] lib_arts_cats = [&#39;Foreign Languages&#39;, &#39;English&#39;, &#39;Communications and Journalism&#39;, &#39;Art and Performance&#39;, &#39;Social Sciences and History&#39;] other_cats = [&#39;Health Professions&#39;, &#39;Public Administration&#39;, &#39;Education&#39;, &#39;Agriculture&#39;, &#39;Business&#39;, &#39;Architecture&#39;] def which_type(subject): if subject in stem_cats: return &quot;STEM&quot; elif subject in lib_arts_cats: return &quot;Liberal Arts&quot; else : return &quot;Others&quot; tidy_data[&quot;Type&quot;] = tidy_data[&quot;Discipline&quot;].apply(which_type) tidy_data.head() . Year Discipline Female proportion Type . 0 1970 | Agriculture | 4.229798 | Others | . 1 1971 | Agriculture | 5.452797 | Others | . 2 1972 | Agriculture | 7.420710 | Others | . 3 1973 | Agriculture | 9.653602 | Others | . 4 1974 | Agriculture | 14.074623 | Others | . Plotting . We start visualizing the data in this section. Every graphs in this section are interactive. You can point to a data point in the graphs to see the detailed information of the data point. Further, you can click a label in the legend on the right to observe the trend of certain type or discipline. . Trends of groups . import altair as alt selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=tidy_data).mark_point(filled=True).encode( x=&quot;Year:N&quot;, y=&quot;median(Female proportion)&quot;, color=&quot;Type&quot;, tooltip=[&quot;Type&quot;, &quot;Year&quot;, &quot;median(Female proportion)&quot;] ).properties( width=700, height=300 ) line = point.mark_line() (point + line).encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . You can see an upward trend in STEM and Others disciplines from the 70s to the mid 80s and the percentage stayed unchanged after that period. In Liberal Arts, the female percentage has been arounnd 60% since 1970. Next, we delve into each disciplines in these groups. . STEM . stem = tidy_data[tidy_data.Type == &quot;STEM&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=stem).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in STEM major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . Most female percentages went upward from 1970 to 2011 except Computer Science and Math and Statistics. The female percentage of Engineer was extremely low (0.8%) in 1970s and reached about 18% in the 2000s. Only 45% of students of Psychology were female in the 1970s. This percentage grew by 30% to 76% in 2011. Throught these years, the percentage in Math and Statistics has remained stable (around 45%). In Computer Science, the trend went upward first and went downward latter. There is a peak of 35% in the 80s and in other period the precentage was around 18% to 25%. . Liberal Arts . liberal = tidy_data[tidy_data.Type == &quot;Liberal Arts&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=liberal).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in Liberal Arts major&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . In this group, most precebtages of female students are higher than the ones in STEM. Most trend are stable in this group. However, unlike most disciplines, the female percentage of Communication and Journalism increased significantly from 35% to 62% and the one of Social Science and History increased about 10%. . Others . other = tidy_data[tidy_data.Type == &quot;Others&quot;] selection = alt.selection_multi(encodings=[&quot;color&quot;], bind=&#39;legend&#39;) point = alt.Chart(data=other).mark_point(filled=True).encode( alt.X(&quot;Year&quot;, type=&quot;nominal&quot;), alt.Y(&quot;Female proportion&quot;, title=&quot;Female Proportion(%)&quot;), color=&quot;Discipline&quot;, tooltip=[&quot;Discipline&quot;, &quot;Year&quot;, &quot;Female proportion&quot;], ).properties( width=800, height=500, title=&quot;Female proportions in other majors&quot; ) line = point.mark_line() chart = point + line chart.encode( opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), ).add_selection( selection ) . . There are two types of trends in this group. Education, Health Professions and Publc Administration have much higher percentage of female stduent (Education:80%, Health Professions:85%, Publc Administration:75%). The percentage of the other three disciplines are all very low in the 70s and they all drastically increased to about 50% throuth these years. . In summary, until 2011, much more girls got into the disciplines that is normally for boys in the 1970s. For instance, only 4.2% of Agriculture students are female whereas the percentage reached 50% in 2011. Also, we can observe that some diciplines have imbalanced gender proportions. For example, although the precentage of female student in Engineer grew significantly, only 17% of students in Engineer is female. In contrast, 85% of Health Professions strudents are female. .",
            "url": "https://peiyihung.github.io/mywebsite/project/data%20visualization/python/2020/12/07/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "relUrl": "/project/data%20visualization/python/2020/12/07/Interactive-Visualization-Female-College-Majors-with-Altair.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "What's the difference between a metric and a loss?",
            "content": "In machine learning, we usually use two values to evaluate our model: a metric and a loss. For instance, if we are doing a binary classification task, our metric may be the accuracy and our loss would be the cross-entroy. They both show how good our model performs. However, why do we need both rather than just use one of them? Also, what&#39;s the difference between them? . The short answer is that the metric is for human while the loss is for your model. . Based on the metric, machine learning practitioners such as data scientists and researchers assess a machine learning model. On the assessment, ML practitioners make decisions to address their problems or achieve their business goals. For example , say a data scientist aims to build a spam classifier to distinguish normal email from spam with 95% accuracy. First, the data scientist build a model with 90% accuracy. Apparently, this result doesn&#39;t meet his business goal, so he tries to build a better one. After implementing some techniques, he might get a classifier with 97% accuracy, which goes beyond his goal. Since the goal is met, the data scientist decides to integrate this model into his data product. ML partitioners use the metric to tell whether their model is good enough. . On the other hand, a loss indicates in what direction your model should improve. The difference between machine learning and traditional programming is how they get the ability to solve a problem. Traditional programs solve problems by following exact instructions given by programmers. In contrast, machine learning models learn how to solve a problem by taking into some examples (data) and discovering the underlying patterns of the problem. How does a machine learning model learn? Most ML models learn using a gradient-based method. Here&#39;s how a gradient-based method (be specifically, a gradient descent method in supervised learning context) works: . A model takes into data and makes predictions. | Compute the loss based on the predictions and the true data. | Compute the gradients of the loss with respect to parameters of the model. | Updating these parameters based on these gradients. | The gradient of the loss helps our model to get better and better. The reason why we need a loss is that a loss is sensitive enough to small changes so our model can improve based on it. More precisely, the gradient of the loss should vary if our parameters change slightly. In our spam classification example, accuracy is obviously not suitable for being a loss since it only changes when some examples are classified differently. The cross-entrpy is relatively smoother and so it is a good candidate for a loss. However, a metric do not have to be different from a loss. A metric can be a loss as long as it is sensitive enough. For instance, in a regression setting, MSE (mean squared error) can be both a metric and a loss. . In summary, a metric helps ML partitioners to evaluate their models and a loss facilitates the learning process of a ML model. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "relUrl": "/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "A Spam Classifier",
            "content": "Introduction . In this project, I built a spam classifer by implementing machine learning models. Models were trained by the datasets from Apache SpamAssassin website. . Get the data . Download emails and load them into my program . import os import urllib import tarfile import urllib.request download_root = &quot;https://spamassassin.apache.org/old/publiccorpus/&quot; file_names = [&quot;20030228_easy_ham.tar.bz2&quot;, &quot;20030228_easy_ham_2.tar.bz2&quot;, &quot;20030228_hard_ham.tar.bz2&quot;, &quot;20030228_spam.tar.bz2&quot;, &quot;20030228_spam_2.tar.bz2&quot;] store_path = os.path.join(&quot;data&quot;) def fetch_data(root_url=download_root, file_names=file_names, store_path=store_path): # make directory storing emails os.makedirs(store_path, exist_ok=True) # download files for file in file_names: file_url = os.path.join(download_root, file) path = os.path.join(store_path, file) urllib.request.urlretrieve(file_url, path) # extract emails for file in file_names: path = os.path.join(store_path, file) with tarfile.open(path, &#39;r&#39;) as f: f.extractall(path=store_path) #fetch_data() # get file names of emails email_folders = [&quot;hard_ham&quot;, &quot;easy_ham&quot;, &quot;easy_ham_2&quot;, &quot;spam&quot;, &quot;spam_2&quot;] ham_names = {} for ham in email_folders[:3]: ham_path = os.path.join(store_path, ham) names = [name for name in sorted(os.listdir(ham_path)) if len(name) &gt; 20] ham_names[ham] = names spam_names = {} for spam in email_folders[3:]: spam_path = os.path.join(store_path, spam) names = [name for name in sorted(os.listdir(spam_path)) if len(name) &gt; 20] spam_names[spam] = names # parse emails import email import email.policy def load_email(directory, filename, spam_path=store_path): path = os.path.join(spam_path, directory) with open(os.path.join(path, filename), &quot;rb&quot;) as f: return email.parser.BytesParser(policy=email.policy.default).parse(f) hams = [] for ham in email_folders[:3]: emails = [load_email(ham, filename=name) for name in ham_names[ham]] hams.extend(emails) spams = [] for spam in email_folders[3:]: emails = [load_email(spam, filename=name) for name in spam_names[spam]] spams.extend(emails) . . explain how to download the emails and load them in my notebook . len(hams), len(spams), len(spams) / (len(hams) + len(spams)) . (4150, 1897, 0.31370927732760046) . Accuracy of random guess is 70%, so we must do better than that. . Take a look at the emails . headers . hams[1].items() . [(&#39;Return-Path&#39;, &#39;&lt;malcolm-sweeps@mrichi.com&gt;&#39;), (&#39;Delivered-To&#39;, &#39;rod@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16821 invoked by uid 505); 7 May 2002 14:37:01 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 500 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.260914 secs); 07 May 2002 14:37:01 -0000&#39;), (&#39;Delivered-To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16811 invoked by uid 505); 7 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 502 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.250416 secs); 07 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from bocelli.siteprotect.com (64.41.120.21) by h0090272a42db.ne.client2.attbi.com with SMTP; 7 May 2002 14:36:59 -0000&#39;), (&#39;Received&#39;, &#39;from mail.mrichi.com ([208.33.95.187]) tby bocelli.siteprotect.com (8.9.3/8.9.3) with SMTP id JAA14328; tTue, 7 May 2002 09:37:01 -0500&#39;), (&#39;From&#39;, &#39;malcolm-sweeps@mrichi.com&#39;), (&#39;Message-Id&#39;, &#39;&lt;200205071437.JAA14328@bocelli.siteprotect.com&gt;&#39;), (&#39;To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Subject&#39;, &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39;), (&#39;Date&#39;, &#39;Tue, 07 May 2002 09:38:27 -0600&#39;), (&#39;X-Mailer&#39;, &#39;sendEmail-v1.33&#39;)] . hams[1][&quot;Subject&quot;] . &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39; . Contents . print(hams[1].get_content()[:600]) . May 7, 2002 Dear rod-3ds@arsecandle.org: Congratulations! On behalf of Frito-Lay, Inc., we are pleased to advise you that you&#39;ve won Fourth Prize in the 3D&#39;s(R) Malcolm in the Middle(TM) Sweepstakes. Fourth Prize consists of 1 manufacturer&#39;s coupon redeemable at participating retailers for 1 free bag of 3D&#39;s(R) brand snacks (up to 7 oz. size), with an approximate retail value of $2.59 and an expiration date of 12/31/02. Follow these instructions to claim your prize: 1. Print out this email message. 2. Complete ALL of the information requested. Print clearly and legibly. Sign . Get email structure . There are some emails that have multiple parts. . from collections import Counter def get_email_structure(email): if isinstance(email, str): return email payload = email.get_payload() if isinstance(payload, list): return &quot;multipart({})&quot;.format(&quot;, &quot;.join([ get_email_structure(sub_email) for sub_email in payload ])) else: return email.get_content_type() def structure_counter(emails): structures = [get_email_structure(email) for email in emails] return Counter(structures) . structure_counter(hams).most_common() . [(&#39;text/plain&#39;, 3832), (&#39;text/html&#39;, 120), (&#39;multipart(text/plain, application/pgp-signature)&#39;, 101), (&#39;multipart(text/plain, text/html)&#39;, 63), (&#39;multipart(text/plain, text/plain)&#39;, 5), (&#39;multipart(text/plain)&#39;, 3), (&#39;multipart(text/plain, application/x-pkcs7-signature)&#39;, 2), (&#39;multipart(text/html)&#39;, 2), (&#39;multipart(text/plain, application/ms-tnef, text/plain)&#39;, 2), (&#39;multipart(text/plain, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)&#39;, 2), (&#39;multipart(text/plain, image/bmp)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html))&#39;, 1), (&#39;multipart(text/plain, image/png, image/png)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, text/enriched)&#39;, 1), (&#39;multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(text/plain, video/mng)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))&#39;, 1), (&#39;multipart(text/plain, application/x-java-applet)&#39;, 1), (&#39;multipart(text/plain, application/x-patch)&#39;, 1), (&#39;multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, application/ms-tnef)&#39;, 1), (&#39;multipart(text/plain, text/plain, text/plain)&#39;, 1)] . structure_counter(spams).most_common() . [(&#39;text/plain&#39;, 816), (&#39;text/html&#39;, 772), (&#39;multipart(text/plain, text/html)&#39;, 159), (&#39;multipart(text/html)&#39;, 49), (&#39;multipart(text/plain)&#39;, 44), (&#39;multipart(multipart(text/html))&#39;, 23), (&#39;multipart(multipart(text/plain, text/html))&#39;, 5), (&#39;multipart(text/plain, application/octet-stream)&#39;, 3), (&#39;multipart(text/html, text/plain)&#39;, 3), (&#39;multipart(text/plain, image/jpeg)&#39;, 3), (&#39;multipart(text/plain, application/octet-stream, text/plain)&#39;, 3), (&#39;multipart(text/html, application/octet-stream)&#39;, 2), (&#39;multipart/alternative&#39;, 2), (&#39;multipart(text/html, image/jpeg)&#39;, 2), (&#39;multipart(multipart(text/plain), application/octet-stream)&#39;, 2), (&#39;multipart(multipart(text/html), application/octet-stream, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)&#39;, 1), (&#39;text/plain charset=us-ascii&#39;, 1), (&#39;multipart(multipart(text/html), image/gif)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/jpeg)&#39;, 1)] . It seems that most hams are plain text, while spams are more often html. What we need to do next? . Preprocessing emails . write helper funtions and make pipeline . Split emails into train and test set . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split X = np.array(hams+spams) y = np.array([0] * len(hams) + [1] * len(spams)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, stratify=y) X_train.shape, X_test.shape . ((4837,), (1210,)) . Email to text . Parse HTML . from bs4 import BeautifulSoup def html_to_plain_text(html): soup = BeautifulSoup(html, &quot;lxml&quot;) strings = &quot;&quot; for i in soup.find_all(): if i.string: strings += i.string + &quot; n&quot; return strings . Turn email to plain text . def email_to_text(email): html = None for part in email.walk(): ctype = part.get_content_type() if not ctype in (&quot;text/plain&quot;, &quot;text/html&quot;): continue try: content = part.get_content() except: # in case of encoding issues content = str(part.get_payload()) if ctype == &quot;text/plain&quot;: return content else: html = content if html: return html_to_plain_text(html) . example_spam = email_to_text(spams[10]) print(example_spam) . Cellular Phone Accessories All At Below Wholesale Prices! http://202.101.163.34:81/sites/merchant/sales/ Hands Free Ear Buds 1.99! Phone Holsters 1.98! Booster Antennas Only $0.99 Phone Cases 1.98! Car Chargers 1.98! Face Plates As Low As 0.99! Lithium Ion Batteries As Low As 6.94! http://202.101.163.34:81/sites/merchant/sales/ Click Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, SAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below WHOLESALE PRICES! http://202.101.163.34:81/sites/merchant/sales/ ***If You Need Assistance Please Call Us (732) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be removed from future mailings please send your remove request to: removemenow68994@btamail.net.cn Thank You and have a super day :) . Replace url with &quot;URL&quot; . import re url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; example_spam = re.sub(url_pattern, &quot;URL&quot;, example_spam) example_spam . &#39;Cellular Phone Accessories All At Below Wholesale Prices! n nURL n nHands Free Ear Buds 1.99! nPhone Holsters 1.98! nBooster Antennas Only $0.99 nPhone Cases 1.98! nCar Chargers 1.98! nFace Plates As Low As 0.99! nLithium Ion Batteries As Low As 6.94! n nURL n nClick Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, nSAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below nWHOLESALE PRICES! n nURL n n***If You Need Assistance Please Call Us (732) 751-1457*** n n n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nTo be removed from future mailings please send your remove nrequest to: removemenow68994@btamail.net.cn nThank You and have a super day :) n n&#39; . Tokenize . import nltk from nltk.tokenize import word_tokenize nltk.download(&#39;punkt&#39;) example_spam_tokenized = word_tokenize(example_spam) example_spam_tokenized[:10] . [nltk_data] Downloading package punkt to /Users/hongpeiyi/nltk_data... [nltk_data] Package punkt is already up-to-date! . [&#39;Cellular&#39;, &#39;Phone&#39;, &#39;Accessories&#39;, &#39;All&#39;, &#39;At&#39;, &#39;Below&#39;, &#39;Wholesale&#39;, &#39;Prices&#39;, &#39;!&#39;, &#39;URL&#39;] . Stemming . def stemming_email(tokenized_email): stemmer = nltk.PorterStemmer() stemmed_words = [stemmer.stem(word) for word in tokenized_email] return &quot; &quot;.join(stemmed_words) stemmed_eamil = stemming_email(example_spam_tokenized) stemmed_eamil . &#39;cellular phone accessori all At below wholesal price ! url hand free ear bud 1.99 ! phone holster 1.98 ! booster antenna onli $ 0.99 phone case 1.98 ! car charger 1.98 ! face plate As low As 0.99 ! lithium ion batteri As low As 6.94 ! url click below for accessori On all nokia , motorola LG , nextel , samsung , qualcomm , ericsson , audiovox phone At below wholesal price ! url ***if you need assist pleas call Us ( 732 ) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be remov from futur mail pleas send your remov request to : removemenow68994 @ btamail.net.cn thank you and have a super day : )&#39; . Write a sklearn estimator to transform our email . from sklearn.base import BaseEstimator, TransformerMixin class EmailToTokenizedStemmed(BaseEstimator, TransformerMixin): def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True): self.strip_headers = strip_headers self.lower_case = lower_case self.remove_punctuation = remove_punctuation self.replace_urls = replace_urls self.replace_numbers = replace_numbers self.stemming = stemming def fit(self, X, y=None): return self def transform(self, X, y=None): X_transformed = [] for email in X: text = email_to_text(email) or &quot;&quot; if self.lower_case: text = text.lower() if self.replace_urls: url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; text = re.sub(url_pattern, &quot;URL&quot;, text) if self.replace_numbers: text = re.sub(r&#39; d+(?: . d*(?:[eE] d+))?&#39;, &#39;NUMBER&#39;, text) if self.remove_punctuation: text = re.sub(r&#39;[^a-zA-Z0-9]+&#39;, &#39; &#39;, text, flags=re.M) text = word_tokenize(text) text = stemming_email(text) X_transformed.append(text) return np.array(X_transformed) . Vectorizing . from sklearn.feature_extraction.text import TfidfVectorizer . Make Pipeline . from sklearn.pipeline import Pipeline email_pipeline = Pipeline([ (&quot;Tokenizing and Stemming&quot;, EmailToTokenizedStemmed()), (&quot;tf-idf Vectorizing&quot;, TfidfVectorizer()), (&quot;passthrough&quot;, None) ]) . The processed datasets . X_train_processed = email_pipeline.fit_transform(X_train) X_test_processed = email_pipeline.transform(X_test) . . Modeling . from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV from sklearn.metrics import confusion_matrix, classification_report from sklearn.naive_bayes import MultinomialNB from sklearn.linear_model import LogisticRegressionCV from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier # plotting import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # others from scipy.stats import uniform, randint, loguniform import joblib # storing models . Functions from evaluating and comparing models . models = {} # storing trained models models_names = [] # storing models names # add models and its name to dict def add_model(name, model, models_list=models, name_list=models_names): name_list.append(name) models_list[name] = model . def get_classification_report(model, X_test=X_test_processed, y_test=y_test): y_pred = model.predict(X_test) print(classification_report(y_test, y_pred, target_names=[&quot;not spam&quot;, &quot;spam&quot;], digits=4)) . Building models and tuning them . how I trained and tuned the models? what&#39;s the process? . Naive Bayes (baseline model) . nb = MultinomialNB().fit(X_train_processed, y_train) . add_model(&quot;Naive Bayes&quot;, nb) . Logistic regression . logitCV = LogisticRegressionCV(max_iter=1000, Cs=20, cv=10, scoring=&quot;accuracy&quot;) logitCV.fit(X_train_processed, y_train) . LogisticRegressionCV(Cs=20, class_weight=None, cv=10, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=1000, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, refit=True, scoring=&#39;accuracy&#39;, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0) . add_model(&quot;Logistic regression&quot;, logitCV) . SVM . svc = SVC() svc_params = {&#39;C&#39;: loguniform(1e0, 1e3), &#39;gamma&#39;: loguniform(1e-4, 1e-3), &#39;kernel&#39;: [&#39;rbf&#39;], &#39;class_weight&#39;:[&#39;balanced&#39;, None]} svc_grid = RandomizedSearchCV(svc, svc_params, n_jobs=-1, cv=10, n_iter=15, scoring=&quot;accuracy&quot;) svc_grid.fit(X_train_processed, y_train) svc_best = svc_grid.best_estimator_ #svc = joblib.load(&quot;tmp/svc.pkl&quot;) . svc.get_params() . {&#39;C&#39;: 280.3887191550727, &#39;break_ties&#39;: False, &#39;cache_size&#39;: 200, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;coef0&#39;: 0.0, &#39;decision_function_shape&#39;: &#39;ovr&#39;, &#39;degree&#39;: 3, &#39;gamma&#39;: 0.000984422644629166, &#39;kernel&#39;: &#39;rbf&#39;, &#39;max_iter&#39;: -1, &#39;probability&#39;: False, &#39;random_state&#39;: None, &#39;shrinking&#39;: True, &#39;tol&#39;: 0.001, &#39;verbose&#39;: False} . add_model(&quot;SVM&quot;, svc) . Random Forest . max_depths = [10, 50, 100, 150] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1500, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 10, oob accuracy: 0.8594 Max Depth: 50, oob accuracy: 0.9692 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 150, oob accuracy: 0.9694 . max_depths = [90, 100, 110, 120, 130] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 90, oob accuracy: 0.9700 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 110, oob accuracy: 0.9704 Max Depth: 120, oob accuracy: 0.9708 Max Depth: 130, oob accuracy: 0.9698 . rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=100) rf.fit(X_train_processed, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=100, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1, oob_score=True, random_state=44, verbose=0, warm_start=False) . add_model(&quot;Random forest&quot;, rf) . Evaluate on test set . for name in models_names: print(name) get_classification_report(models[name]) print(&quot;--&quot;) print() . Naive Bayes precision recall f1-score support not spam 0.8494 0.9988 0.9181 830 spam 0.9957 0.6132 0.7590 380 accuracy 0.8777 1210 macro avg 0.9226 0.8060 0.8385 1210 weighted avg 0.8953 0.8777 0.8681 1210 -- Logistic regression precision recall f1-score support not spam 0.9927 0.9880 0.9903 830 spam 0.9740 0.9842 0.9791 380 accuracy 0.9868 1210 macro avg 0.9833 0.9861 0.9847 1210 weighted avg 0.9868 0.9868 0.9868 1210 -- SVM precision recall f1-score support not spam 0.9891 0.9880 0.9885 830 spam 0.9738 0.9763 0.9750 380 accuracy 0.9843 1210 macro avg 0.9814 0.9821 0.9818 1210 weighted avg 0.9843 0.9843 0.9843 1210 -- Random forest precision recall f1-score support not spam 0.9776 0.9976 0.9875 830 spam 0.9945 0.9500 0.9717 380 accuracy 0.9826 1210 macro avg 0.9860 0.9738 0.9796 1210 weighted avg 0.9829 0.9826 0.9825 1210 -- . Comparing performance of models using ROC curve and AUC . from sklearn.metrics import roc_curve, roc_auc_score def plot_roc_curve(models_names=models_names, models=models): plt.figure(dpi=120) for name in models_names: if name == &quot;SVM&quot;: y_score = models[name].decision_function(X_test_processed) fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) else: y_score = models[name].predict_proba(X_test_processed)[:,1] fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) plt.plot([0, 1], [0,1], &quot;b--&quot;) plt.xlim(-0.01, 1.02) plt.ylim(-0.01, 1.02) plt.legend(title=&quot;Model (AUC score)&quot;,loc=(1.01, 0.4)) . plot_roc_curve() . Conclusion .",
            "url": "https://peiyihung.github.io/mywebsite/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "relUrl": "/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "My name is Peiyi Hung (洪培翊). .",
          "url": "https://peiyihung.github.io/mywebsite/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Learning",
          "content": "My Learning Notes . Why do we need nonlinear activation functions in a neural network ? . Explaining why we need nonlinear activation fucntions and illutrating it with python code . Dec 16, 2020 . | What&#39;s the difference between a metric and a loss? . . Dec 6, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/learning/",
          "relUrl": "/learning/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "My Projects . Predicting prices of Machine Learning Books . Getting prices of ML books by web scraping and using ML methods to predict them . Dec 25, 2020 . | Find quality wine with machine learning algorithms . Playing with the popular wine dataset . Dec 20, 2020 . | Interactive Visualization of Change in Female College Major Percentage with Altair . Building simple interactive graphs with Altair . Dec 7, 2020 . | Analyzing Sharing Bikes Usage Trends . A exploratory data analysis project answering questions about bike sharing. . Dec 2, 2020 . | A Spam Classifier . This project builds a spam classifier using Apache SpamAssassin&#39;s public datasets. . Sep 12, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://peiyihung.github.io/mywebsite/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}