{
  
    
        "post0": {
            "title": "What's the difference between a metric and a loss?",
            "content": "In machine learning, there are usually two values used to evaluate our model: a metric and a loss. For instance, if we are doing a binary classification task, our metric may be the accuracy and our loss would be the cross-entroy. They both show how good our model performs. However, why do we need both rather than just use one of them? Furthermore, what&#39;s the difference between them? . The short answer is that the metric is for human while the loss is for your model. . Based on the metric, machine learning paritioners such as data scientis and researchers assess a machine learning model. On the assessment, ML paritioners make decisions to address their problems or achieve their business goals. For example , say a data scientist aims to build a spam classifier to distinguish normal email from spam with 95% accuracy. First, the data scientist bulid a logistic regression model with 90% accruay. Apparently, this result doesn&#39;t meet his business goal, so he tries to build another model. After training a random forest model, he get a classifier with 97% accuracy, which goes beyond his goal. Since the goal is met, the data scientist decides to integrate this model into his data product. ML partitioners use the metric to tell whether their model is good enough. . On the other hand, a loss indicates in what direction your model should improve. The difference between machine learning and traditinal programming is how they get the ability to solve a problem. Traditional programs solve problems by following exact instructions given by prgrammers. On the contrary, a machine learning models learn that by taking into some examples (data) and discovery the underlying patterns. How does a machine learning model do that? Most ML models learn from a gradient-based method. Here&#39;s how a gradient-based method (be specifically, a gradient descent method in supervised learning context) works: . A model takes into data and makes predictions. | Compute a loss based on the predictions and the true data. | Compute the gradients of the loss with respect to parameters of the model. | Updating these parameters based on these gradients. | The gradient of the loss helps our model to get better and better. The reason why we need a loss is that it is sensitive enough to small changes so our model can improve. More precisely, the gradient of the loss should vary if our parameters change slighly. In our spam classification example, accuracy is obviously not suitable for being a loss since it only changes when some examples are classified differently. The cross-entrpy is relatively smoother and so it is a good candidate for a loss. However, a metric do not have to be different from a loss. A metric can be a loss as long as it is sensitive enough. For instance, in a regression setting, MSE (mean squared error) can be both a metric and a loss. . In summary, a metric helps ML partitioners to evaluate their models and a loss faciliates the learning process of a ML model. .",
            "url": "https://peiyihung.github.io/mywebsite/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "relUrl": "/learning/machine%20learning/2020/12/06/What's-the-difference-between-a-metric-and-a-loss.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "A Spam Classifier",
            "content": "Introduction . In this project, I built a spam classifer by implementing machine learning models. Models were trained by the datasets from Apache SpamAssassin website. . Get the data . Download emails and load them into my program . import os import urllib import tarfile import urllib.request download_root = &quot;https://spamassassin.apache.org/old/publiccorpus/&quot; file_names = [&quot;20030228_easy_ham.tar.bz2&quot;, &quot;20030228_easy_ham_2.tar.bz2&quot;, &quot;20030228_hard_ham.tar.bz2&quot;, &quot;20030228_spam.tar.bz2&quot;, &quot;20030228_spam_2.tar.bz2&quot;] store_path = os.path.join(&quot;data&quot;) def fetch_data(root_url=download_root, file_names=file_names, store_path=store_path): # make directory storing emails os.makedirs(store_path, exist_ok=True) # download files for file in file_names: file_url = os.path.join(download_root, file) path = os.path.join(store_path, file) urllib.request.urlretrieve(file_url, path) # extract emails for file in file_names: path = os.path.join(store_path, file) with tarfile.open(path, &#39;r&#39;) as f: f.extractall(path=store_path) #fetch_data() # get file names of emails email_folders = [&quot;hard_ham&quot;, &quot;easy_ham&quot;, &quot;easy_ham_2&quot;, &quot;spam&quot;, &quot;spam_2&quot;] ham_names = {} for ham in email_folders[:3]: ham_path = os.path.join(store_path, ham) names = [name for name in sorted(os.listdir(ham_path)) if len(name) &gt; 20] ham_names[ham] = names spam_names = {} for spam in email_folders[3:]: spam_path = os.path.join(store_path, spam) names = [name for name in sorted(os.listdir(spam_path)) if len(name) &gt; 20] spam_names[spam] = names # parse emails import email import email.policy def load_email(directory, filename, spam_path=store_path): path = os.path.join(spam_path, directory) with open(os.path.join(path, filename), &quot;rb&quot;) as f: return email.parser.BytesParser(policy=email.policy.default).parse(f) hams = [] for ham in email_folders[:3]: emails = [load_email(ham, filename=name) for name in ham_names[ham]] hams.extend(emails) spams = [] for spam in email_folders[3:]: emails = [load_email(spam, filename=name) for name in spam_names[spam]] spams.extend(emails) . . explain how to download the emails and load them in my notebook . len(hams), len(spams), len(spams) / (len(hams) + len(spams)) . (4150, 1897, 0.31370927732760046) . Accuracy of random guess is 70%, so we must do better than that. . Take a look at the emails . headers . hams[1].items() . [(&#39;Return-Path&#39;, &#39;&lt;malcolm-sweeps@mrichi.com&gt;&#39;), (&#39;Delivered-To&#39;, &#39;rod@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16821 invoked by uid 505); 7 May 2002 14:37:01 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 500 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.260914 secs); 07 May 2002 14:37:01 -0000&#39;), (&#39;Delivered-To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Received&#39;, &#39;(qmail 16811 invoked by uid 505); 7 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from malcolm-sweeps@mrichi.com by blazing.arsecandle.org t by uid 502 with qmail-scanner-1.10 (F-PROT: 3.12. Clear:0. Processed in 0.250416 secs); 07 May 2002 14:37:00 -0000&#39;), (&#39;Received&#39;, &#39;from bocelli.siteprotect.com (64.41.120.21) by h0090272a42db.ne.client2.attbi.com with SMTP; 7 May 2002 14:36:59 -0000&#39;), (&#39;Received&#39;, &#39;from mail.mrichi.com ([208.33.95.187]) tby bocelli.siteprotect.com (8.9.3/8.9.3) with SMTP id JAA14328; tTue, 7 May 2002 09:37:01 -0500&#39;), (&#39;From&#39;, &#39;malcolm-sweeps@mrichi.com&#39;), (&#39;Message-Id&#39;, &#39;&lt;200205071437.JAA14328@bocelli.siteprotect.com&gt;&#39;), (&#39;To&#39;, &#39;rod-3ds@arsecandle.org&#39;), (&#39;Subject&#39;, &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39;), (&#39;Date&#39;, &#39;Tue, 07 May 2002 09:38:27 -0600&#39;), (&#39;X-Mailer&#39;, &#39;sendEmail-v1.33&#39;)] . hams[1][&quot;Subject&quot;] . &#39;Malcolm in the Middle Sweepstakes Prize Notification&#39; . Contents . print(hams[1].get_content()[:600]) . May 7, 2002 Dear rod-3ds@arsecandle.org: Congratulations! On behalf of Frito-Lay, Inc., we are pleased to advise you that you&#39;ve won Fourth Prize in the 3D&#39;s(R) Malcolm in the Middle(TM) Sweepstakes. Fourth Prize consists of 1 manufacturer&#39;s coupon redeemable at participating retailers for 1 free bag of 3D&#39;s(R) brand snacks (up to 7 oz. size), with an approximate retail value of $2.59 and an expiration date of 12/31/02. Follow these instructions to claim your prize: 1. Print out this email message. 2. Complete ALL of the information requested. Print clearly and legibly. Sign . Get email structure . There are some emails that have multiple parts. . from collections import Counter def get_email_structure(email): if isinstance(email, str): return email payload = email.get_payload() if isinstance(payload, list): return &quot;multipart({})&quot;.format(&quot;, &quot;.join([ get_email_structure(sub_email) for sub_email in payload ])) else: return email.get_content_type() def structure_counter(emails): structures = [get_email_structure(email) for email in emails] return Counter(structures) . structure_counter(hams).most_common() . [(&#39;text/plain&#39;, 3832), (&#39;text/html&#39;, 120), (&#39;multipart(text/plain, application/pgp-signature)&#39;, 101), (&#39;multipart(text/plain, text/html)&#39;, 63), (&#39;multipart(text/plain, text/plain)&#39;, 5), (&#39;multipart(text/plain)&#39;, 3), (&#39;multipart(text/plain, application/x-pkcs7-signature)&#39;, 2), (&#39;multipart(text/html)&#39;, 2), (&#39;multipart(text/plain, application/ms-tnef, text/plain)&#39;, 2), (&#39;multipart(text/plain, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 2), (&#39;multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)&#39;, 2), (&#39;multipart(text/plain, image/bmp)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html))&#39;, 1), (&#39;multipart(text/plain, image/png, image/png)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, text/enriched)&#39;, 1), (&#39;multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(text/plain, video/mng)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))&#39;, 1), (&#39;multipart(text/plain, application/x-java-applet)&#39;, 1), (&#39;multipart(text/plain, application/x-patch)&#39;, 1), (&#39;multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)&#39;, 1), (&#39;multipart(text/plain, application/ms-tnef)&#39;, 1), (&#39;multipart(text/plain, text/plain, text/plain)&#39;, 1)] . structure_counter(spams).most_common() . [(&#39;text/plain&#39;, 816), (&#39;text/html&#39;, 772), (&#39;multipart(text/plain, text/html)&#39;, 159), (&#39;multipart(text/html)&#39;, 49), (&#39;multipart(text/plain)&#39;, 44), (&#39;multipart(multipart(text/html))&#39;, 23), (&#39;multipart(multipart(text/plain, text/html))&#39;, 5), (&#39;multipart(text/plain, application/octet-stream)&#39;, 3), (&#39;multipart(text/html, text/plain)&#39;, 3), (&#39;multipart(text/plain, image/jpeg)&#39;, 3), (&#39;multipart(text/plain, application/octet-stream, text/plain)&#39;, 3), (&#39;multipart(text/html, application/octet-stream)&#39;, 2), (&#39;multipart/alternative&#39;, 2), (&#39;multipart(text/html, image/jpeg)&#39;, 2), (&#39;multipart(multipart(text/plain), application/octet-stream)&#39;, 2), (&#39;multipart(multipart(text/html), application/octet-stream, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)&#39;, 1), (&#39;text/plain charset=us-ascii&#39;, 1), (&#39;multipart(multipart(text/html), image/gif)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif, image/jpeg)&#39;, 1)] . It seems that most hams are plain text, while spams are more often html. What we need to do next? . Preprocessing emails . write helper funtions and make pipeline . Split emails into train and test set . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split X = np.array(hams+spams) y = np.array([0] * len(hams) + [1] * len(spams)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, stratify=y) X_train.shape, X_test.shape . ((4837,), (1210,)) . Email to text . Parse HTML . from bs4 import BeautifulSoup def html_to_plain_text(html): soup = BeautifulSoup(html, &quot;lxml&quot;) strings = &quot;&quot; for i in soup.find_all(): if i.string: strings += i.string + &quot; n&quot; return strings . Turn email to plain text . def email_to_text(email): html = None for part in email.walk(): ctype = part.get_content_type() if not ctype in (&quot;text/plain&quot;, &quot;text/html&quot;): continue try: content = part.get_content() except: # in case of encoding issues content = str(part.get_payload()) if ctype == &quot;text/plain&quot;: return content else: html = content if html: return html_to_plain_text(html) . example_spam = email_to_text(spams[10]) print(example_spam) . Cellular Phone Accessories All At Below Wholesale Prices! http://202.101.163.34:81/sites/merchant/sales/ Hands Free Ear Buds 1.99! Phone Holsters 1.98! Booster Antennas Only $0.99 Phone Cases 1.98! Car Chargers 1.98! Face Plates As Low As 0.99! Lithium Ion Batteries As Low As 6.94! http://202.101.163.34:81/sites/merchant/sales/ Click Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, SAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below WHOLESALE PRICES! http://202.101.163.34:81/sites/merchant/sales/ ***If You Need Assistance Please Call Us (732) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be removed from future mailings please send your remove request to: removemenow68994@btamail.net.cn Thank You and have a super day :) . Replace url with &quot;URL&quot; . import re url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; example_spam = re.sub(url_pattern, &quot;URL&quot;, example_spam) example_spam . &#39;Cellular Phone Accessories All At Below Wholesale Prices! n nURL n nHands Free Ear Buds 1.99! nPhone Holsters 1.98! nBooster Antennas Only $0.99 nPhone Cases 1.98! nCar Chargers 1.98! nFace Plates As Low As 0.99! nLithium Ion Batteries As Low As 6.94! n nURL n nClick Below For Accessories On All NOKIA, MOTOROLA LG, NEXTEL, nSAMSUNG, QUALCOMM, ERICSSON, AUDIOVOX PHONES At Below nWHOLESALE PRICES! n nURL n n***If You Need Assistance Please Call Us (732) 751-1457*** n n n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nTo be removed from future mailings please send your remove nrequest to: removemenow68994@btamail.net.cn nThank You and have a super day :) n n&#39; . Tokenize . import nltk from nltk.tokenize import word_tokenize nltk.download(&#39;punkt&#39;) example_spam_tokenized = word_tokenize(example_spam) example_spam_tokenized[:10] . [nltk_data] Downloading package punkt to /Users/hongpeiyi/nltk_data... [nltk_data] Package punkt is already up-to-date! . [&#39;Cellular&#39;, &#39;Phone&#39;, &#39;Accessories&#39;, &#39;All&#39;, &#39;At&#39;, &#39;Below&#39;, &#39;Wholesale&#39;, &#39;Prices&#39;, &#39;!&#39;, &#39;URL&#39;] . Stemming . def stemming_email(tokenized_email): stemmer = nltk.PorterStemmer() stemmed_words = [stemmer.stem(word) for word in tokenized_email] return &quot; &quot;.join(stemmed_words) stemmed_eamil = stemming_email(example_spam_tokenized) stemmed_eamil . &#39;cellular phone accessori all At below wholesal price ! url hand free ear bud 1.99 ! phone holster 1.98 ! booster antenna onli $ 0.99 phone case 1.98 ! car charger 1.98 ! face plate As low As 0.99 ! lithium ion batteri As low As 6.94 ! url click below for accessori On all nokia , motorola LG , nextel , samsung , qualcomm , ericsson , audiovox phone At below wholesal price ! url ***if you need assist pleas call Us ( 732 ) 751-1457*** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To be remov from futur mail pleas send your remov request to : removemenow68994 @ btamail.net.cn thank you and have a super day : )&#39; . Write a sklearn estimator to transform our email . from sklearn.base import BaseEstimator, TransformerMixin class EmailToTokenizedStemmed(BaseEstimator, TransformerMixin): def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True): self.strip_headers = strip_headers self.lower_case = lower_case self.remove_punctuation = remove_punctuation self.replace_urls = replace_urls self.replace_numbers = replace_numbers self.stemming = stemming def fit(self, X, y=None): return self def transform(self, X, y=None): X_transformed = [] for email in X: text = email_to_text(email) or &quot;&quot; if self.lower_case: text = text.lower() if self.replace_urls: url_pattern = &#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!* ( ),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39; text = re.sub(url_pattern, &quot;URL&quot;, text) if self.replace_numbers: text = re.sub(r&#39; d+(?: . d*(?:[eE] d+))?&#39;, &#39;NUMBER&#39;, text) if self.remove_punctuation: text = re.sub(r&#39;[^a-zA-Z0-9]+&#39;, &#39; &#39;, text, flags=re.M) text = word_tokenize(text) text = stemming_email(text) X_transformed.append(text) return np.array(X_transformed) . Vectorizing . from sklearn.feature_extraction.text import TfidfVectorizer . Make Pipeline . from sklearn.pipeline import Pipeline email_pipeline = Pipeline([ (&quot;Tokenizing and Stemming&quot;, EmailToTokenizedStemmed()), (&quot;tf-idf Vectorizing&quot;, TfidfVectorizer()), (&quot;passthrough&quot;, None) ]) . The processed datasets . X_train_processed = email_pipeline.fit_transform(X_train) X_test_processed = email_pipeline.transform(X_test) . . Modeling . from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV from sklearn.metrics import confusion_matrix, classification_report from sklearn.naive_bayes import MultinomialNB from sklearn.linear_model import LogisticRegressionCV from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier # plotting import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # others from scipy.stats import uniform, randint, loguniform import joblib # storing models . Functions from evaluating and comparing models . models = {} # storing trained models models_names = [] # storing models names # add models and its name to dict def add_model(name, model, models_list=models, name_list=models_names): name_list.append(name) models_list[name] = model . def get_classification_report(model, X_test=X_test_processed, y_test=y_test): y_pred = model.predict(X_test) print(classification_report(y_test, y_pred, target_names=[&quot;not spam&quot;, &quot;spam&quot;], digits=4)) . Building models and tuning them . how I trained and tuned the models? what&#39;s the process? . Naive Bayes (baseline model) . nb = MultinomialNB().fit(X_train_processed, y_train) . add_model(&quot;Naive Bayes&quot;, nb) . Logistic regression . logitCV = LogisticRegressionCV(max_iter=1000, Cs=20, cv=10, scoring=&quot;accuracy&quot;) logitCV.fit(X_train_processed, y_train) . LogisticRegressionCV(Cs=20, class_weight=None, cv=10, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=1000, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, refit=True, scoring=&#39;accuracy&#39;, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0) . add_model(&quot;Logistic regression&quot;, logitCV) . SVM . svc = SVC() svc_params = {&#39;C&#39;: loguniform(1e0, 1e3), &#39;gamma&#39;: loguniform(1e-4, 1e-3), &#39;kernel&#39;: [&#39;rbf&#39;], &#39;class_weight&#39;:[&#39;balanced&#39;, None]} svc_grid = RandomizedSearchCV(svc, svc_params, n_jobs=-1, cv=10, n_iter=15, scoring=&quot;accuracy&quot;) svc_grid.fit(X_train_processed, y_train) svc_best = svc_grid.best_estimator_ #svc = joblib.load(&quot;tmp/svc.pkl&quot;) . svc.get_params() . {&#39;C&#39;: 280.3887191550727, &#39;break_ties&#39;: False, &#39;cache_size&#39;: 200, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;coef0&#39;: 0.0, &#39;decision_function_shape&#39;: &#39;ovr&#39;, &#39;degree&#39;: 3, &#39;gamma&#39;: 0.000984422644629166, &#39;kernel&#39;: &#39;rbf&#39;, &#39;max_iter&#39;: -1, &#39;probability&#39;: False, &#39;random_state&#39;: None, &#39;shrinking&#39;: True, &#39;tol&#39;: 0.001, &#39;verbose&#39;: False} . add_model(&quot;SVM&quot;, svc) . Random Forest . max_depths = [10, 50, 100, 150] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1500, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 10, oob accuracy: 0.8594 Max Depth: 50, oob accuracy: 0.9692 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 150, oob accuracy: 0.9694 . max_depths = [90, 100, 110, 120, 130] for depth in max_depths: rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=depth) rf.fit(X_train_processed, y_train) print(f&quot;Max Depth: {depth:3}, oob accuracy: {rf.oob_score_:.4f}&quot;) . Max Depth: 90, oob accuracy: 0.9700 Max Depth: 100, oob accuracy: 0.9711 Max Depth: 110, oob accuracy: 0.9704 Max Depth: 120, oob accuracy: 0.9708 Max Depth: 130, oob accuracy: 0.9698 . rf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=1000, random_state=44, max_depth=100) rf.fit(X_train_processed, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=100, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1, oob_score=True, random_state=44, verbose=0, warm_start=False) . add_model(&quot;Random forest&quot;, rf) . Evaluate on test set . for name in models_names: print(name) get_classification_report(models[name]) print(&quot;--&quot;) print() . Naive Bayes precision recall f1-score support not spam 0.8494 0.9988 0.9181 830 spam 0.9957 0.6132 0.7590 380 accuracy 0.8777 1210 macro avg 0.9226 0.8060 0.8385 1210 weighted avg 0.8953 0.8777 0.8681 1210 -- Logistic regression precision recall f1-score support not spam 0.9927 0.9880 0.9903 830 spam 0.9740 0.9842 0.9791 380 accuracy 0.9868 1210 macro avg 0.9833 0.9861 0.9847 1210 weighted avg 0.9868 0.9868 0.9868 1210 -- SVM precision recall f1-score support not spam 0.9891 0.9880 0.9885 830 spam 0.9738 0.9763 0.9750 380 accuracy 0.9843 1210 macro avg 0.9814 0.9821 0.9818 1210 weighted avg 0.9843 0.9843 0.9843 1210 -- Random forest precision recall f1-score support not spam 0.9776 0.9976 0.9875 830 spam 0.9945 0.9500 0.9717 380 accuracy 0.9826 1210 macro avg 0.9860 0.9738 0.9796 1210 weighted avg 0.9829 0.9826 0.9825 1210 -- . Comparing performance of models using ROC curve and AUC . from sklearn.metrics import roc_curve, roc_auc_score def plot_roc_curve(models_names=models_names, models=models): plt.figure(dpi=120) for name in models_names: if name == &quot;SVM&quot;: y_score = models[name].decision_function(X_test_processed) fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) else: y_score = models[name].predict_proba(X_test_processed)[:,1] fpr, tpr, thresholds = roc_curve(y_test, y_score) auc = roc_auc_score(y_test, y_score) label = name + f&quot;({auc:.4f})&quot; plt.plot(fpr, tpr, label=label) plt.plot([0, 1], [0,1], &quot;b--&quot;) plt.xlim(-0.01, 1.02) plt.ylim(-0.01, 1.02) plt.legend(title=&quot;Model (AUC score)&quot;,loc=(1.01, 0.4)) . plot_roc_curve() . Conclusion .",
            "url": "https://peiyihung.github.io/mywebsite/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "relUrl": "/project/machine%20learning/classification/2020/09/12/Apache-Spam-Classifier.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Who am I ? . My name is Peiyi Hong (洪培翊). . Contect me . Email | Github | Linkedin | .",
          "url": "https://peiyihung.github.io/mywebsite/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Learning",
          "content": "My Learning Notes . What&#39;s the difference between a metric and a loss? . . Dec 6, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/learning/",
          "relUrl": "/learning/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "My Projects . Analyzing Sharing Bikes Usage Trends . A exploratory data analysis project answering questions about bike sharing. . Dec 2, 2020 . | A Spam Classifier . This project builds a spam classifier using Apache SpamAssassin&#39;s public datasets. . Sep 12, 2020 . | .",
          "url": "https://peiyihung.github.io/mywebsite/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://peiyihung.github.io/mywebsite/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}